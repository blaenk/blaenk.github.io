<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Machine Learning - Blaenk Denum</title>
  <meta name="author" content="Jorge Israel Peña">
  <meta name="description" content="AKA Blaenk Denum">

  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <link href="/favicon.png" rel="shortcut icon">
  <link href='http://fonts.googleapis.com/css?family=Noto+Sans:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Merriweather:900italic,900,700italic,400italic,700,400,300italic,300' rel='stylesheet' type='text/css'>
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
  <link href='/css/screen.css' rel='stylesheet' type='text/css' />
  <link rel="alternate" type="application/atom+xml" title="Atom Feed" href="/atom.xml" />
  <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML&delayStartupUntil=configured"></script>
  <script src="https://rawgit.com/ekalinin/typogr.js/master/typogr.min.js"></script>
  <script src='/js/blaenk.js' type='text/javascript'></script>
</head>
<body>
  <div class="page-wrapper">
    <header id="header">
  <div id="stamp">
    <h1 id="name">
      <a href="/">
        <span class="emboldened">Jorge</span>.Israel.<span class="emboldened">Peña</span>
      </a>
    </h1>
    <h4 id="pseudonym">
      AKA <span class="emboldened">Blaenk</span>.Denum
    </h4>
  </div>
  <nav id="main-nav">
    <ul class="main">
      <li><a href="/about/">About</a></li>
      <li><a href="/notes/">Notes</a></li>
      <li><a href="/work/">Work</a></li>
      <li><a href="/lately/">Lately</a></li>
      <li><a id="search_btn">Search</a></li>
    </ul>
  </nav>
  <nav id="mobile-nav">
    <div class="menu">
      <a class="button">Menu</a>
      <div class="container">
        <ul class="main">
          <li><a href="/about/">About</a></li>
          <li><a href="/notes/">Notes</a></li>
          <li><a href="/work/">Work</a></li>
          <li><a href="/lately/">Lately</a></li>
        </ul>
      </div>
    </div>
    <div class="search">
      <a class="button"></a>
      <div class="container">
        <form action="http://google.com/search" method="get">
          <input type="text" name="q" results="0">
          <input type="hidden" name="q" value="site:blaenkdenum.com">
        </form>
      </div>
    </div>
  </nav>
</header>
<form class="desk_search" action="http://google.com/search" method="get">
  <input id="search" type="text" name="q" results="0" placeholder="Search" autocomplete="off" spellcheck="false">
  <input type="hidden" name="q" value="site:blaenkdenum.com">
</form>

    
        <article class="post">
  <h2 class="title"><a href="/notes/machine-learning"><span>Machine Learning</span></a></h2>
  <div class="entry-content"><p>I&#39;ve been wanting to learn about the subject of machine learning for a while now. I&#39;m familiar with some basic concepts, as well as reinforcement learning. What follows are notes on my attempt to comprehend the subject. The primary learning resource I&#39;m using is Cal Tech&#39;s CS 1156 on edX, with supplementary material from Stanford&#39;s CS 229 on Coursera.</p>

<p>I pushed my code for the programming assignments for this class to <a href="https://github.com/blaenk/learning-from-data">github</a>.</p>

<nav id="toc">
<h3>Contents</h3><ol>
<li>
<a href="#learning-problem">Learning Problem</a>
<ol>
<li>
<a href="#learning-components">Learning Components</a>
</li>
<li>
<a href="#perceptrons">Perceptrons</a>
<ol>
<li>
<a href="#perceptron-model">Perceptron Model</a>
</li>
<li>
<a href="#perceptron-learning-algorithm">Perceptron Learning Algorithm</a>
</li>
</ol>
</li>
<li>
<a href="#types-of-learning">Types of Learning</a>
</li>
</ol>
</li>
<li>
<a href="#feasibility">Feasibility</a>
<ol>
<li>
<a href="#learning-notation">Learning Notation</a>
</li>
</ol>
</li>
<li>
<a href="#linear-model">Linear Model</a>
<ol>
<li>
<a href="#input-representation">Input Representation</a>
</li>
<li>
<a href="#pocket-algorithm">Pocket Algorithm</a>
</li>
<li>
<a href="#linear-regression">Linear Regression</a>
<ol>
<li>
<a href="#linear-regression-algorithm">Linear Regression Algorithm</a>
</li>
<li>
<a href="#linear-regression-for-classification">Linear Regression for Classification</a>
</li>
</ol>
</li>
<li>
<a href="#non-linear-transformations">Non-Linear Transformations</a>
</li>
</ol>
</li>
<li>
<a href="#error-and-noise">Error and Noise</a>
<ol>
<li>
<a href="#choosing-the-error-measure">Choosing the Error Measure</a>
</li>
<li>
<a href="#noise">Noise</a>
</li>
</ol>
</li>
<li>
<a href="#training-vs.-testing">Training vs. Testing</a>
<ol>
<li>
<a href="#overlapping-events">Overlapping Events</a>
</li>
<li>
<a href="#dichotomies">Dichotomies</a>
</li>
<li>
<a href="#growth-function">Growth Function</a>
<ol>
<li>
<a href="#positive-rays">Positive Rays</a>
</li>
<li>
<a href="#positive-intervals">Positive Intervals</a>
</li>
<li>
<a href="#convex-sets">Convex Sets</a>
</li>
</ol>
</li>
<li>
<a href="#break-point">Break Point</a>
</li>
</ol>
</li>
<li>
<a href="#theory-of-generalization">Theory of Generalization</a>
<ol>
<li>
<a href="#conceptualization">Conceptualization</a>
</li>
<li>
<a href="#estimating-script-typemathtexalphascript-and-script-typemathtexbetascript">Estimating <script type="math/tex">\alpha</script> and <script type="math/tex">\beta</script></a>
</li>
<li>
<a href="#estimating-script-typemathtexbetascript">Estimating <script type="math/tex">\beta</script></a>
</li>
<li>
<a href="#bound-relation">Bound Relation</a>
</li>
<li>
<a href="#computing-the-bound">Computing the Bound</a>
<ol>
<li>
<a href="#numerical-derivation">Numerical Derivation</a>
</li>
<li>
<a href="#analytical-derivation">Analytical Derivation</a>
</li>
</ol>
</li>
<li>
<a href="#vapnik-chervonenkis-inequality">Vapnik-Chervonenkis Inequality</a>
</li>
</ol>
</li>
<li>
<a href="#vc-dimension">VC Dimension</a>
<ol>
<li>
<a href="#vc-dimension-of-perceptrons">VC Dimension of Perceptrons</a>
</li>
<li>
<a href="#interpretation-of-vc-dimension">Interpretation of VC Dimension</a>
</li>
<li>
<a href="#minimum-sample-size">Minimum Sample Size</a>
</li>
<li>
<a href="#generalization-bound">Generalization Bound</a>
</li>
</ol>
</li>
<li>
<a href="#bias-variance-tradeoff">Bias-Variance Tradeoff</a>
<ol>
<li>
<a href="#bias-variance-representation">Bias-Variance Representation</a>
</li>
<li>
<a href="#tradeoff">Tradeoff</a>
</li>
<li>
<a href="#learning-curves">Learning Curves</a>
</li>
<li>
<a href="#analysis-of-linear-regression">Analysis of Linear Regression</a>
</li>
</ol>
</li>
<li>
<a href="#linear-model-ii">Linear Model II</a>
<ol>
<li>
<a href="#non-linear-transformations-ii">Non-Linear Transformations II</a>
</li>
<li>
<a href="#logistic-regression">Logistic Regression</a>
<ol>
<li>
<a href="#logistic-regression-error-measure">Logistic Regression Error Measure</a>
</li>
<li>
<a href="#logistic-regression-learning-algorithm">Logistic Regression Learning Algorithm</a>
</li>
</ol>
</li>
</ol>
</li>
<li>
<a href="#neural-networks">Neural Networks</a>
<ol>
<li>
<a href="#stochastic-gradient-descent">Stochastic Gradient Descent</a>
</li>
<li>
<a href="#neural-network-model">Neural Network Model</a>
</li>
<li>
<a href="#neural-network-backpropagation">Neural Network Backpropagation</a>
</li>
</ol>
</li>
<li>
<a href="#overfitting">Overfitting</a>
<ol>
<li>
<a href="#role-of-noise">Role of Noise</a>
</li>
<li>
<a href="#deterministic-noise">Deterministic Noise</a>
</li>
<li>
<a href="#noise-and-bias-variance">Noise and Bias-Variance</a>
</li>
<li>
<a href="#dealing-with-overfitting">Dealing with Overfitting</a>
</li>
</ol>
</li>
<li>
<a href="#regularization">Regularization</a>
<ol>
<li>
<a href="#polynomial-model">Polynomial Model</a>
</li>
<li>
<a href="#augmented-error">Augmented Error</a>
</li>
<li>
<a href="#weight-decay">Weight Decay</a>
<ol>
<li>
<a href="#variations-of-weight-decay">Variations of Weight Decay</a>
</li>
</ol>
</li>
<li>
<a href="#weight-growth">Weight Growth</a>
</li>
<li>
<a href="#generalized-regularizer">Generalized Regularizer</a>
</li>
<li>
<a href="#neural-network-regularizers">Neural Network Regularizers</a>
<ol>
<li>
<a href="#neural-network-weight-decay">Neural Network Weight Decay</a>
</li>
<li>
<a href="#neural-network-weight-elimination">Neural Network Weight Elimination</a>
</li>
<li>
<a href="#early-stopping-in-neural-networks">Early Stopping in Neural Networks</a>
</li>
</ol>
</li>
<li>
<a href="#optimal-script-typemathtexlambdascript">Optimal <script type="math/tex">\lambda</script></a>
</li>
</ol>
</li>
<li>
<a href="#validation">Validation</a>
<ol>
<li>
<a href="#purpose-of-validation">Purpose of Validation</a>
</li>
<li>
<a href="#model-selection">Model Selection</a>
</li>
<li>
<a href="#validation-bias">Validation Bias</a>
</li>
<li>
<a href="#data-contamination">Data Contamination</a>
</li>
<li>
<a href="#cross-validation">Cross-Validation</a>
</li>
<li>
<a href="#cross-validation-optimization">Cross-Validation Optimization</a>
</li>
</ol>
</li>
<li>
<a href="#support-vector-machines">Support Vector Machines</a>
<ol>
<li>
<a href="#lagrange-formulation">Lagrange Formulation</a>
</li>
<li>
<a href="#svm-non-linear-transformations">SVM Non-Linear Transformations</a>
</li>
</ol>
</li>
<li>
<a href="#kernel-methods">Kernel Methods</a>
<ol>
<li>
<a href="#kernel-trick">Kernel Trick</a>
<ol>
<li>
<a href="#polynomial-kernel">Polynomial Kernel</a>
</li>
<li>
<a href="#kernel-formulation-of-svm">Kernel Formulation of SVM</a>
</li>
<li>
<a href="#kernel-function-validity">Kernel Function Validity</a>
</li>
</ol>
</li>
<li>
<a href="#soft-margin-svm">Soft-Margin SVM</a>
<ol>
<li>
<a href="#soft-margin-svm-error-measure">Soft-Margin SVM Error Measure</a>
</li>
<li>
<a href="#soft-margin-svm-optimization">Soft-Margin SVM Optimization</a>
</li>
<li>
<a href="#types-of-support-vectors">Types of Support Vectors</a>
</li>
</ol>
</li>
</ol>
</li>
<li>
<a href="#radial-basis-functions">Radial Basis Functions</a>
<ol>
<li>
<a href="#effect-of-script-typemathtexgammascript">Effect of <script type="math/tex">\gamma</script></a>
</li>
<li>
<a href="#rbf-classification">RBF Classification</a>
<ol>
<li>
<a href="#k-centers-rbf">K-Centers RBF</a>
</li>
<li>
<a href="#k-means-clustering">K-Means Clustering</a>
</li>
<li>
<a href="#lloyd39s-algorithm">LLoyd&#39;s Algorithm</a>
</li>
<li>
<a href="#rbf-calculating-weights">RBF Calculating Weights</a>
</li>
</ol>
</li>
<li>
<a href="#rbf-network">RBF Network</a>
<ol>
<li>
<a href="#rbf-vs.-neural-networks">RBF vs. Neural Networks</a>
</li>
</ol>
</li>
<li>
<a href="#choosing-script-typemathtexgammascript">Choosing <script type="math/tex">\gamma</script></a>
</li>
<li>
<a href="#rbf-vs.-svm-kernel">RBF vs. SVM Kernel</a>
</li>
<li>
<a href="#rbf-regularization">RBF Regularization</a>
</li>
</ol>
</li>
<li>
<a href="#three-learning-principles">Three Learning Principles</a>
<ol>
<li>
<a href="#occam39s-razor">Occam&#39;s Razor</a>
<ol>
<li>
<a href="#puzzle-1-football-oracle">Puzzle 1: Football Oracle</a>
</li>
<li>
<a href="#why-is-simpler-better">Why is Simpler Better?</a>
</li>
<li>
<a href="#meaningless-fit">Meaningless Fit</a>
</li>
</ol>
</li>
<li>
<a href="#sampling-bias">Sampling Bias</a>
<ol>
<li>
<a href="#puzzle-2-presidential-election">Puzzle 2: Presidential Election</a>
</li>
<li>
<a href="#matching-the-distributions">Matching the Distributions</a>
</li>
<li>
<a href="#puzzle-3-credit-approval">Puzzle 3: Credit Approval</a>
</li>
</ol>
</li>
<li>
<a href="#data-snooping">Data Snooping</a>
<ol>
<li>
<a href="#looking-at-the-data">Looking at the Data</a>
</li>
<li>
<a href="#puzzle-4-financial-forecasting">Puzzle 4: Financial Forecasting</a>
</li>
<li>
<a href="#data-set-reuse">Data Set Reuse</a>
</li>
<li>
<a href="#data-snooping-remedies">Data Snooping Remedies</a>
</li>
<li>
<a href="#puzzle-5-bias-via-snooping">Puzzle 5: Bias via Snooping</a>
</li>
</ol>
</li>
</ol>
</li>
<li>
<a href="#epilogue">Epilogue</a>
<ol>
<li>
<a href="#map-of-machine-learning">Map of Machine Learning</a>
</li>
<li>
<a href="#bayesian-learning">Bayesian Learning</a>
</li>
<li>
<a href="#aggregation-methods">Aggregation Methods</a>
<ol>
<li>
<a href="#before-the-fact">Before the Fact</a>
</li>
<li>
<a href="#after-the-fact">After the Fact</a>
</li>
</ol>
</li>
</ol>
</li>
<li>
<a href="#resources">Resources</a>
</li>
</ol>
</nav>
<h1 id="learning-problem">
<span class="hash">#</span>
<a href="#learning-problem" class="header-link">Learning Problem</a>
</h1>
<p>The essence of machine learning:</p>

<ol>
<li>pattern exists</li>
<li>cannot pin it down mathematically</li>
<li>have data on it to learn from</li>
</ol>

<p>A movie recommender system might be modeled such that there are a set of factors of varying degrees of likability for the viewer, and varying degrees of presence in a given movie. These two sets of factors combine to produce a projected viewer rating for that given movie.</p>

<p>The machine learning aspect would take an actual user rating, and given two contributing-factor vectors for the movie and viewer full of random factors, it would modify each until a rating similar to the actual user rating is produced.</p>
<h2 id="learning-components">
<span class="hash">#</span>
<a href="#learning-components" class="header-link">Learning Components</a>
</h2>
<ul>
<li><strong>Input</strong>: <script type="math/tex">x = (x_1, x_2, \dots, x_n)</script>

<ul>
<li>feature vector</li>
</ul></li>
<li><strong>Output</strong>: <script type="math/tex">y</script>

<ul>
<li>result given the input</li>
</ul></li>
<li><strong>Target Function</strong>: <script type="math/tex">f\colon \cal {X} \to \cal {Y}</script>

<ul>
<li>The ideal function for determining the result, unknown, reason for learning</li>
</ul></li>
<li><strong>Data</strong>: <script type="math/tex">(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)</script>

<ul>
<li>Historical records of actual inputs and their results</li>
</ul></li>
<li><strong>Hypothesis</strong>: <script type="math/tex">g\colon \cal {X} \to \cal {Y}</script>

<ul>
<li>Result of learning, <script type="math/tex">g \approx f</script></li>
</ul></li>
<li><strong>Learning Algorithm</strong>: <script type="math/tex">\cal A</script>

<ul>
<li>The machine learning algorithm</li>
</ul></li>
<li><strong>Hypothesis Set</strong>: <script type="math/tex">\cal H</script>

<ul>
<li>The set of candidate hypotheses where <script type="math/tex">g \in \cal H</script></li>
</ul></li>
</ul>

<p>Together, <script type="math/tex">\cal A</script> and <script type="math/tex">\cal H</script> are known as the <em>learning model</em>.</p>
<h2 id="perceptrons">
<span class="hash">#</span>
<a href="#perceptrons" class="header-link">Perceptrons</a>
</h2><h3 id="perceptron-model">
<span class="hash">#</span>
<a href="#perceptron-model" class="header-link">Perceptron Model</a>
</h3>
<p>Given input <script type="math/tex">\def \feature {\mathbf x} \feature = (x_1, x_2, \dots, x_n)</script>:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\text {approve} &\colon \sum_{i=1}^{d} w_i x_i > \text {threshold} \\
\text {deny}    &\colon \sum_{i=1}^{d} w_i x_i < \text {threshold}
\end{align}
</script></p>

<p>This can be expressed as a linear formula <script type="math/tex">h \in \cal H</script>:</p>

<p><script type="math/tex; mode=display">
\newcommand{sign}{\operatorname{sign}}
h(x) = \sign \left( \left( \sum_{i=1}^{d} w_i x_i \right) - \text {threshold} \right)
</script></p>

<p>The factors that are varied to determine the hypothesis function are the weights and threshold.</p>

<p>The threshold can be represented as a weight <script type="math/tex">w_0</script> if an artificial constant <script type="math/tex">x_0</script> is added to the feature vector where <script type="math/tex">x_0 = 1</script>. This simplifies the formula to:</p>

<p><script type="math/tex; mode=display"> h(x) = \sign \left( \sum_{i=0}^{d} w_i x_i \right) </script></p>

<p>This operation is the same as the dot product of the two vectors:</p>

<p><script type="math/tex; mode=display">
\def \weight {\mathbf w}
\def \weightT {\weight^\intercal}
h(x) = \sign(\weight \bullet \feature)
</script></p>
<h3 id="perceptron-learning-algorithm">
<span class="hash">#</span>
<a href="#perceptron-learning-algorithm" class="header-link">Perceptron Learning Algorithm</a>
</h3>
<p><strong>Given the perceptron</strong>:</p>

<p><script type="math/tex; mode=display"> h(x) = \sign(\weight \bullet \feature) </script></p>

<p><strong>Given the training set</strong>:</p>

<p><script type="math/tex; mode=display">\feature = (x_1, x_2, \dots, x_n)</script></p>

<p><strong>Pick a misclassified point</strong>. A point is misclassified if the perceptron&#39;s output doesn&#39;t match the recorded data&#39;s output given an input vector:</p>

<p><script type="math/tex; mode=display"> \sign(w \bullet x_n) \not= y_n </script></p>

<p><strong>Update the weight vector</strong>: The algorithm must correct for this by updating the weight vector.</p>

<p><script type="math/tex; mode=display"> \weight' \gets \weight + y_n x_n </script></p>

<p>Visualizing <script type="math/tex">\vec w</script> and <script type="math/tex">\vec x</script>, it&#39;s apparent that the perceptron is equivalent to the dot product, which is equivalent to <script type="math/tex">\cos \theta</script> where <script type="math/tex">\theta</script> is the angle between <script type="math/tex">\vec w</script> and <script type="math/tex">\vec x</script>. Given this, <script type="math/tex">\vec w</script> is updated depending on what the intended result <script type="math/tex">y_n</script> is.</p>

<p>For example, if the perceptron modeled the result to be <script type="math/tex">-1</script>, then <script type="math/tex">\theta > 90^\circ</script>. However, if the intended result <script type="math/tex">y_n = +1</script>, then there is a mismatch, so the weight vector is &quot;nudged&quot; in the correct direction so that <script type="math/tex">\theta < 90^\circ</script> by adding it to <script type="math/tex">x_n</script>.</p>
<h2 id="types-of-learning">
<span class="hash">#</span>
<a href="#types-of-learning" class="header-link">Types of Learning</a>
</h2>
<ul>
<li><strong>Supervised learning</strong>: when the input/output data is provided</li>
<li><strong>Unsupervised learning</strong>: only the input is provided; &quot;unlabeled data&quot;</li>
<li><strong>Reinforcement learning</strong>: input and <em>some</em> output is provided</li>
</ul>
<h1 id="feasibility">
<span class="hash">#</span>
<a href="#feasibility" class="header-link">Feasibility</a>
</h1>
<p>It seems as if it isn&#39;t feasible to learn an unknown function because the function can assume any value outside of the data available to us.</p>

<p>To understand why it is indeed possible, consider a probabilistic example. Given a bin full of marbles that are either <strong>red</strong> or <strong>green</strong>:</p>

<p><script type="math/tex; mode=display">
\begin{align}
&P \left( \text {picking a red marble} \right) = \mu \\
\\
&P \left( \text {picking a green marble} \right) = 1 - \mu \\
\end{align}
</script></p>

<p>If the value of <script type="math/tex">\mu</script> is unknown, and we pick <script type="math/tex">N</script> marbles independently, then:</p>

<p><script type="math/tex; mode=display"> \text {frequency of red marbles in sample} = \nu </script></p>

<p>The question is: does <script type="math/tex">\nu</script> say anything about <script type="math/tex">\mu</script>? It might appear that it <strong>doesn&#39;t</strong>. For example, if there are 100 marbles in the bin and only 10 of them are red, just because we happen to take out all 10 (<script type="math/tex">\nu  = 1</script>) doesn&#39;t mean that that sample is representative of the distribution in the bin (<script type="math/tex">\mu = 1</script>).</p>

<p>However, in a <strong>big sample</strong>, it&#39;s more probable that <script type="math/tex">\nu</script> is close to <script type="math/tex">\mu</script>, that is, they are within <script type="math/tex">\epsilon</script> of each other. This can be formally expressed as <em>Hoeffding&#39;s Inequality</em>:</p>

<p><script type="math/tex; mode=display">
P \left( \left| \nu - \mu \right| \gt \epsilon \right) \leq 2e^{- 2\epsilon^2 N}
</script></p>

<p><script type="math/tex">\nu</script> is generally varied, and <script type="math/tex">\mu</script> is a constant.</p>

<p>As is apparent from the inequality, if we choose a very small <script type="math/tex">\epsilon</script> value, it has the effect of setting the right-hand side to near 1, thus rendering the effort pointless since we already knew that the probability would be <script type="math/tex">\leq 1</script>. <strong>Therefore</strong>, if we want a smaller <script type="math/tex">\epsilon</script>, we will have to increase the input size <script type="math/tex">N</script> to compensate.</p>

<p>The appeal of Hoeffding&#39;s Inequality is that it is valid for any <script type="math/tex">N \in \mathbb {Z}^+</script> and any <script type="math/tex">\epsilon > 0</script>. What we&#39;re trying to say with the inequality is that <script type="math/tex">\nu \approx \mu</script> which means that <script type="math/tex">\mu \approx \nu</script>.</p>

<p>This relates to learning in the following way. In the bin, <script type="math/tex">\mu</script> is unknown, but in learning the unknown is the target function <script type="math/tex">f \colon \mathcal {X} \to \mathcal {Y}</script>. Now think of the bin as the input space, where every marble is a point <script type="math/tex">x \in \mathcal {X}</script> such as a credit application. As a result, the bin is really <script type="math/tex">\mathcal {X}</script>, where the marbles are of different colors such as green and gray, where:</p>

<ul>
<li><strong>green</strong> represents that the hypothesis got it right, that is <script type="math/tex">h(x) = f(x)</script></li>
<li><strong>red</strong> represents that the hypothesis got it wrong, that is <script type="math/tex">h(x) \not= f(x)</script></li>
</ul>

<p>However, in creating this analogy from the bin example to the learning model, the bin example has a probability component in picking a marble from the bin. This must be mapped to the learning model as well, in the form of introducing a probability distribution <script type="math/tex">P</script>, where <script type="math/tex">P</script> is not restricted over <script type="math/tex">\mathcal {X}</script>, and <script type="math/tex">P</script> doesn&#39;t have to be known. It is then assumed that the probability is used to generate the input data points.</p>

<p>The problem so far is that the hypothesis <script type="math/tex">h</script> is fixed, and for a given <script type="math/tex">h</script>, <script type="math/tex">\nu</script> generalizes to <script type="math/tex">\mu</script>, which ends up being a <em>verification</em> of <script type="math/tex">h</script>, not learning.</p>

<p>Instead, to make it a learning process, then there needs to be no guarantee that <script type="math/tex">\nu</script> will be small, and we need to choose from multiple <script type="math/tex">h</script>&#39;s. To generalize the bin model to more than one hypothesis, we can use multiple bins. Out of the many bins that were created, the hypothesis responsible for the bin with the smallest <script type="math/tex">\mu</script>---the fraction of red marbles in the bin---is chosen.</p>
<h2 id="learning-notation">
<span class="hash">#</span>
<a href="#learning-notation" class="header-link">Learning Notation</a>
</h2>
<p>Both <script type="math/tex">\mu</script> and <script type="math/tex">\nu</script> depend on which hypothesis <script type="math/tex">h</script>:</p>

<p><script type="math/tex">\nu</script> is the error <em>in sample</em>, which is denoted by <script type="math/tex">\def \insample {E_{\text {in}}} \insample(h)</script></p>

<p><script type="math/tex">\mu</script> is the error <em>out of sample</em>, which is denoted by <script type="math/tex">\def \outsample {E_{\text {out}}} \outsample(h)</script></p>

<p>For clarification, if something performs well &quot;out of sample&quot; then it&#39;s likely that learning actually took place. This notation can be used to modify Hoeffding&#39;s Inequality:</p>

<p><script type="math/tex; mode=display">
P \left( \left|\insample(h) - \outsample(h)\right| \gt \epsilon \right) \leq 2e^{- 2\epsilon^2 N}
</script></p>

<p>The problem now is that Hoeffding&#39;s Inequality doesn&#39;t apply to multiple bins. To account for multiple bins, the inequality can be modified to be:</p>

<p><script type="math/tex; mode=display">
\begin{align*}
P \left( \left|\insample(g) - \outsample(g)\right| \gt \epsilon \right) &\leq
P \begin{aligned}[t]
              \left( \vphantom {\epsilon} \right. &\hphantom {\text {or}}\
                \left|\insample(h_1) - \outsample(h_1)\right| \gt \epsilon \\
              &\text {or}\ \left|\insample(h_2) - \outsample(h_2)\right| \gt \epsilon \\
              &\dots \\
              &\left. \vphantom{\insample(h_1)} \text {or}\ \left|\insample(h_M) - \outsample(h_M)\right| \gt \epsilon \right)
            \end{aligned} \\
&\leq \sum_{m=1}^M P \left( \left| \insample(h_m) - \outsample(h_m) \right| > \epsilon \right) \\
&\leq \sum_{m=1}^M 2e^{- 2\epsilon^2 N} \\
P \left( \left|\insample(g) - \outsample(g)\right| \gt \epsilon \right) &\leq 2Me^{- 2\epsilon^2 N}
\end{align*}
</script></p>
<h1 id="linear-model">
<span class="hash">#</span>
<a href="#linear-model" class="header-link">Linear Model</a>
</h1><h2 id="input-representation">
<span class="hash">#</span>
<a href="#input-representation" class="header-link">Input Representation</a>
</h2>
<p>If we want to develop a system that can detect hand-written numbers, the input can be different examples of hand-written digits. However, if an example digit is a 16x16 bitmap, then that corresponds to an array of 256 real numbers, which becomes 257 when <script type="math/tex">x_0</script> is added for the threshold. This means the problem is operating in 257 dimensions.</p>

<p>Instead, the input can be represented in just three dimensions: <script type="math/tex">x_0</script>, intensity, and symmetry. The intensity corresponds to how many black pixels exist in the example, and the symmetry is a measure of how symmetric the digit is along the x and y axes. See slide 5 to see a plot of the data using these features.</p>
<h2 id="pocket-algorithm">
<span class="hash">#</span>
<a href="#pocket-algorithm" class="header-link">Pocket Algorithm</a>
</h2>
<p>The PLA would never converge on non-linearly separable data, so it&#39;s common practice to forcefully terminate it after a certain number of iterations. However, this has the consequence that the hypothesis function ends up being whatever the result of the last iteration was. This is a problem because it could be that a better hypothesis function with lower in-sample error <script type="math/tex">\insample</script> was discovered in a previous iteration.</p>

<p>The Pocket algorithm is a simple modification to the PLA which simply keeps track of the hypothesis function with the least in-sample error <script type="math/tex">\insample</script>. When this is combined with forceful termination after a certain number of iterations, PLA becomes usable with non-linearly separable data.</p>
<h2 id="linear-regression">
<span class="hash">#</span>
<a href="#linear-regression" class="header-link">Linear Regression</a>
</h2>
<p>The word <em>regression</em> simply means real-valued output. For example, in the scenario of credit approval, a classification problem would consist of determining whether or not to grant a credit line to an applicant. However, a regression problem would be determining the dollar amount for a particular credit line.</p>

<p>The linear regression output is defined as:</p>

<p><script type="math/tex; mode=display"> h(x)  = \sum_{i = 0}^d w_i x_i = \mathbf {w^{\mathrm {T}}x} = \mathbf {w \bullet x} </script></p>

<p>For example, the input may look something like this:</p>

<p><script type="math/tex; mode=display"> (x_1, y_1), (x_2, y_2), \dots, (x_n, y_n) </script></p>

<p>Here <script type="math/tex">y_n \in \mathbb {R}</script> is the credit line for customer <script type="math/tex">x_n</script>.</p>

<p>The measure of how well <script type="math/tex">h</script> approximates <script type="math/tex">f</script> is referred to as the error. With linear regression, the standard error function used is the <em>squared error</em>, defined as:</p>

<p><script type="math/tex; mode=display"> \text {squared error} = (h(x) - f(x))^2 </script></p>

<p>This means that the in-sample error <script type="math/tex">\insample</script> is defined as:</p>

<p><script type="math/tex; mode=display"> \insample(h) = \frac {1} {N} \sum_{n = 1}^N (h(\mathbf {x}_n) - y_n)^2 </script></p>

<p>This can be written in terms of <script type="math/tex">\vec {w}</script>:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\insample(\weight) &= \frac {1} {N} \sum_{n = 1}^N (\weightT \mathbf {x}_n - y_n)^2 \\
                  &= \frac {1} {N} \sum_{n = 1}^N (\weight \bullet \mathbf {x}_n - y_n)^2
\end{align}
</script></p>

<p>This can be written in vector form as:</p>

<p><script type="math/tex; mode=display">
\begin{align}
&\insample(w) = \frac {1} {N} \| \mathrm {X} \mathbf {w} - \mathbf {y} \|^2 \\[10pt]
&\text {where}\ \mathrm {X} = \begin{bmatrix}
                              —\ x_1^{\mathrm {T}}\ — \\
                              —\ x_2^{\mathrm {T}}\ — \\
                              \vdots \\
                              —\ x_n^{\mathrm {T}}\ —
                            \end{bmatrix},\ 
              \mathrm {y} = \begin{bmatrix}
                              y_1 \\
                              y_2 \\
                              \vdots \\
                              y_n
                            \end{bmatrix}
\end{align}
</script></p>

<p>Since the goal is to minimize the in-sample error <script type="math/tex">\insample</script>, and <script type="math/tex">\mathrm {X}</script> and <script type="math/tex">\mathrm {y}</script> are constant since they were provided as input data, <script type="math/tex">\insample</script> can be minimized by varying <script type="math/tex">\vec {w}</script>:</p>

<p><script type="math/tex; mode=display">
\begin{align}
&\phantom {\nabla} \insample(w) = \frac {1} {N} \| \mathrm {X} \mathbf {w} - \mathbf {y} \|^2 \\
&\nabla \insample(w) = \frac {2} {N} \mathrm {X}^{\mathrm {T}} \left( \mathrm {X} \mathbf {w} - \mathbf {y} \right) = 0 \\
\end{align}
</script></p>

<p>Knowing this, an equation for the weight vector can be found by distributing the <script type="math/tex">\mathrm {X}^{\mathrm {T}}</script> factor. The <script type="math/tex">\mathrm {w}</script> factor can then be isolated by multiplying both sides by the inverse of <script type="math/tex">\mathrm {X}^{\mathrm {T}} \mathrm {X}</script>. The resulting factor <script type="math/tex">X^\dagger</script> on the right side is known as the <em>pseudo-inverse</em> of <script type="math/tex">\mathrm {X}</script>.</p>

<p><script type="math/tex; mode=display">
\begin{align}
\mathrm {X}^{\mathrm {T}} \mathrm {Xw} &= \mathrm {X}^{\mathrm {T}} \mathrm {y} \\
\text {if}\ \mathrm {X}^\dagger &= \left( \mathrm {X}^{\mathrm {T}} \mathrm {X} \right)^{-1} \mathrm {X}^{\mathrm {T}} \\
\text {then}\ \mathrm {w} &= \mathrm {X}^\dagger \mathrm {y}
\end{align}
</script></p>

<p>The dimension of <script type="math/tex">\mathrm {X}^\mathrm {T}</script> is <script type="math/tex">(d + 1) \times N</script>, so the dimension of <script type="math/tex">\mathrm X</script> is <script type="math/tex">N \times (d + 1)</script>. This means that even if <script type="math/tex">N</script> is some large number, their product results in a small square matrix of dimensions <script type="math/tex">(d + 1) \times (d + 1)</script>. This means that the dimensions of <script type="math/tex">\mathrm {X}^\dagger</script> will be <script type="math/tex">(d + 1) \times N</script>.</p>
<h3 id="linear-regression-algorithm">
<span class="hash">#</span>
<a href="#linear-regression-algorithm" class="header-link">Linear Regression Algorithm</a>
</h3>
<p>The algorithm for linear regression is therefore:</p>

<ol>
<li>construct input data matrix <script type="math/tex">\mathrm {X}</script> and target vector <script type="math/tex">\mathrm {y}</script> from the data set <script type="math/tex">(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)</script></li>
</ol>

<p><script type="math/tex; mode=display">
\mathrm {X} = \begin{bmatrix}
                —\ x_1^{\mathrm {T}}\ — \\
                —\ x_2^{\mathrm {T}}\ — \\
                \vdots \\
                —\ x_n^{\mathrm {T}}\ —
              \end{bmatrix},\
\mathrm {y} = \begin{bmatrix}
                y_1 \\
                y_2 \\
                \vdots \\
                y_n
              \end{bmatrix}
</script></p>

<ol>
<li>compute the pseudo inverse <script type="math/tex">\mathrm {X}^\dagger = \left( \mathrm {X}^{\mathrm {T}} \mathrm {X} \right)^{-1} \mathrm {X}^{\mathrm {T}}</script></li>
<li>return the weight vector <script type="math/tex">\mathrm {w} = \mathrm {X}^\dagger \mathrm {y}</script></li>
</ol>
<h3 id="linear-regression-for-classification">
<span class="hash">#</span>
<a href="#linear-regression-for-classification" class="header-link">Linear Regression for Classification</a>
</h3>
<p>Linear regression learns a real-valued function <script type="math/tex">y = f(x) \in \mathbb {R}</script>. However, binary-valued functions are also real-valued: <script type="math/tex">\pm 1 \in \mathbb {R}</script>. Therefore, we can use linear regression to find <script type="math/tex">\mathrm w</script> where:</p>

<p><script type="math/tex; mode=display"> \mathrm {w}^{\mathrm {T}} \mathrm {x}_n \approx y_n = \pm 1 </script></p>

<p>This way, <script type="math/tex">\text {sign} (\mathrm {w}^{\mathrm {T}} \mathrm {x}_n)</script> is likely to agree with <script type="math/tex">y_n = \pm 1</script>. This provides good initial weights for classification.</p>
<h2 id="non-linear-transformations">
<span class="hash">#</span>
<a href="#non-linear-transformations" class="header-link">Non-Linear Transformations</a>
</h2>
<p>Not all data is linearly separable. In fact, certain data features aren&#39;t linear. For example, a credit line is affected by &quot;years in residence,&quot; but it doesn&#39;t affect it in a linear way where someone with 10 years in residence will get much more benefit than someone in 5. Instead, the feature can be defined as affecting it in a non-linear manner given the following conditions:</p>

<p><script type="math/tex; mode=display"> [\![ x_i < 1 ]\!] \text { and } [\![ x_i > 5 ]\!] </script></p>

<p>Linear regression and classification work because they are linear in the weights. For this reason, data can be transformed non-linearly.</p>

<p>For example, if a given data set has positive data points around the center of a region, a transformation could be applied to each point which simply measures the distance from the center of the region to a given point:</p>

<p><script type="math/tex; mode=display"> (x_1, x_2) \xrightarrow{\Phi} (x_1^2, x_2^2) </script></p>

<p>This newly transformed data set---which is now linearly separable---is used as the new data set.</p>

<p>To recap, non-linear transformations can be used to transform data such that it becomes linearly separable:</p>

<ol>
<li>given original data

<ul>
<li><script type="math/tex">\mathrm {x}_n \in \mathcal X</script></li>
</ul></li>
<li>transform the data

<ul>
<li><script type="math/tex">\mathrm {z}_n = \Phi (\mathrm {x}_n) \in \mathcal Z</script></li>
</ul></li>
<li>separate data in the <script type="math/tex">\mathcal Z</script>-space

<ul>
<li><script type="math/tex">\tilde {g} (\mathrm {z}) = \text {sign} (\tilde {w}^{\mathrm {T}} \mathrm {z})</script></li>
</ul></li>
<li>classify in <script type="math/tex">\mathcal X</script>-space

<ul>
<li><script type="math/tex">g(\mathrm {x}) = \tilde {g} (\Phi (\mathrm {x})) = \text {sign} (\tilde {w}^{\mathrm {T}} \Phi (\mathrm {x}))</script></li>
</ul></li>
</ol>

<p>A <script type="math/tex">\Phi</script>-transform transforms input data into the <script type="math/tex">\mathcal Z</script>-coordinate space:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\mathrm {x} = (x_0, x_1, \dots, x_d)\ &\xrightarrow{\Phi}\ \mathrm {z} = (z_0, z_1, \dots \dots, z_{\tilde d}) \\
\mathrm { x_1, x_2, \dots, x_n}\      &\xrightarrow{\Phi}\ \mathrm {z_1, z_2, \dots, z_n } \\
\mathrm { y_1, y_2, \dots, y_n}\      &\xrightarrow{\Phi}\ \mathrm {y_1, y_2, \dots, y_n } \\
\text {no weights in}\ \mathcal {X}\  &\phantom {\xrightarrow{\Phi}\ } \mathrm {\tilde {w}} = (w_0, w_1, \dots \dots, w_{\tilde {d}}) \\
g(\mathrm {x}) &= \text {sign} (\mathrm {\tilde {w}^T z}) = \text {sign} (\mathrm {\tilde {w}^T \Phi (x)})
\end{align}
</script></p>
<h1 id="error-and-noise">
<span class="hash">#</span>
<a href="#error-and-noise" class="header-link">Error and Noise</a>
</h1>
<p>Error measures explain what it means for <script type="math/tex">h \approx f</script>. It is defined as <script type="math/tex">E(h, f)</script>. It almost always has a pointwise definition <script type="math/tex">e(h(\mathbf x), f(\mathbf x))</script> which takes as input the value of the same point for both the hypothesis and target functions. An example of this is the squared error and binary error:</p>

<p><script type="math/tex; mode=display">
\def \xpoint {\mathbf x}
\text {squared error}\colon \quad e(h(\xpoint), f(\xpoint)) = (h(\xpoint) - f(\xpoint))^2
</script></p>

<p><script type="math/tex; mode=display"> \text {binary error}\colon \quad e(h(\xpoint), f(\xpoint)) = [\![ h(\xpoint) \not= f(\xpoint)]\!] </script></p>

<p>To go from a pointwise error measure to overall, we take the average of pointwise errors.</p>

<p><script type="math/tex; mode=display">
\begin{align}
\insample(h)  &= \frac 1 N \sum_{n = 1}^N e(h(\xpoint_n), f(\xpoint_n)) \\ \\
\outsample(h) &= \mathbb {E}_{\xpoint} \left[ e(h(\xpoint), f(\xpoint)) \right] \\
              &\phantom {=}\ \text {where } \mathbb E \text { is the expected value}
\end{align}
</script></p>

<p>It can therefore be said that <script type="math/tex">g \approx f</script> if it is tested with the values from the same distribution that it was trained on and yields an acceptable pointwise error measure.</p>
<h2 id="choosing-the-error-measure">
<span class="hash">#</span>
<a href="#choosing-the-error-measure" class="header-link">Choosing the Error Measure</a>
</h2>
<p>An example in which the choice of error measure is important is fingerprint verification, which returns <script type="math/tex">+1</script> if it thinks it&#39;s you, and <script type="math/tex">-1</script> if it thinks it&#39;s an intruder.</p>

<p>There are two types of error:</p>

<ul>
<li><strong>false accept</strong>: something that shouldn&#39;t have been accepted was accepted (<em>false positive</em>)</li>
<li><strong>false reject</strong>: something that shouldn&#39;t have been rejected was rejected (<em>false negative</em>)</li>
</ul>

<p>The question is, how much should each type of error be penalized?</p>

<table style="background-color: transparent !important; border-collapse: collapse; border: none; empty-cells: hide;">
  <tr>
    <td colspan="2" style="background-color: transparent;"></td>
    <td colspan="2" style="text-align: center;">$f$</td>
  </tr>
  <tr style="background-color: transparent;">
    <td colspan="2"></td>
    <td style="text-align: center; border-bottom: 2px solid black;">$+1$</td>
    <td style="text-align: center; border-bottom: 2px solid black;">$-1$</td>
  </tr>
  <tr>
    <td rowspan="2">$h$</td>
    <td style="border-right: 2px solid black;">$+1$</td>
    <td style="text-align: center">no error</td>
    <td style="text-align: center">**false accept**</td>
  </tr>
  <tr style="background-color: transparent;">
    <td style="border-right: 2px solid black;">$-1$</td>
    <td style="text-align: center">**false reject**</td>
    <td style="text-align: center">no error</td>
  </tr>
</table>

<p>The choice of error measure is application specific. For example, in the scenario that fingerprint recognition is used to determine which customers in a particular supermarket can get discounts, the cost of false rejections or accepts are:</p>

<ul>
<li><strong>false reject</strong>: costly; customer gets annoyed, possibly switches to competitor</li>
<li><strong>false accept</strong>: minor; give away discount to someone that didn&#39;t qualify for it, but the intruder has left their fingerprint behind (to later train the system to fix that mistake?)</li>
</ul>

<p>Given this particular application domain, we may want to penalize a candidate hypothesis function if it has a high rate of false rejects/negatives, in this case by a factor of <script type="math/tex">10</script>. False accepts/positives on the other hand are only penalized by a factor of <script type="math/tex">1</script>.</p>

<table style="background-color: transparent !important; border-collapse: collapse; border: none; empty-cells: hide;">
  <tr>
    <td colspan="2" style="background-color: transparent;"></td>
    <td colspan="2" style="text-align: center;">$f$</td>
  </tr>
  <tr style="background-color: transparent;">
    <td colspan="2"></td>
    <td style="text-align: center; border-bottom: 2px solid black;">$+1$</td>
    <td style="text-align: center; border-bottom: 2px solid black;">$-1$</td>
  </tr>
  <tr>
    <td rowspan="2">$h$</td>
    <td style="border-right: 2px solid black;">$+1$</td>
    <td style="text-align: center">$0$</td>
    <td style="text-align: center">$1$</td>
  </tr>
  <tr style="background-color: transparent;">
    <td style="border-right: 2px solid black;">$-1$</td>
    <td style="text-align: center">$10$</td>
    <td style="text-align: center">$0$</td>
  </tr>
</table>

<p>In an alternative scenario, the fingerprint identification system is used by the CIA for security.</p>

<ul>
<li><strong>false accept</strong>: disaster</li>
<li><strong>false reject</strong>: can be tolerated, you&#39;re an employee</li>
</ul>

<table style="background-color: transparent !important; border-collapse: collapse; border: none; empty-cells: hide;">
  <tr>
    <td colspan="2" style="background-color: transparent;"></td>
    <td colspan="2" style="text-align: center;">$f$</td>
  </tr>
  <tr style="background-color: transparent;">
    <td colspan="2"></td>
    <td style="text-align: center; border-bottom: 2px solid black;">$+1$</td>
    <td style="text-align: center; border-bottom: 2px solid black;">$-1$</td>
  </tr>
  <tr>
    <td rowspan="2">$h$</td>
    <td style="border-right: 2px solid black;">$+1$</td>
    <td style="text-align: center">$0$</td>
    <td style="text-align: center">$1000$</td>
  </tr>
  <tr style="background-color: transparent;">
    <td style="border-right: 2px solid black;">$-1$</td>
    <td style="text-align: center">$1$</td>
    <td style="text-align: center">$0$</td>
  </tr>
</table>

<p>This reinforces the fact that the error measure is application specific, and thus specified by the user. A question that should be asked to determine the error measure is how much does it cost the application to have a false positive or negative.</p>

<p>However, it isn&#39;t always possible to determine a correct error measure. There are alternatives to this:</p>

<ul>
<li><strong>plausible measures</strong>: approximate e.g. squared error <script type="math/tex">\equiv</script> Gaussian noise</li>
<li><strong>friendly measures</strong>: closed-form solution e.g. linear regression, convex optimization</li>
</ul>

<p>In the learning diagram, the error measure goes into the learning algorithm so that it knows what value to minimize among the hypothesis set, and to determine the final hypothesis, to know if the hypothesis candidate approximates the target function.</p>
<h2 id="noise">
<span class="hash">#</span>
<a href="#noise" class="header-link">Noise</a>
</h2>
<p>The &quot;target function&quot; is not always a <em>function</em>. In the mathematical sense, a function must return a unique value for any given input. However, going back to the credit-card approval system, it is entirely possible that two given applicants had the exact same feature set (e.g. age, annual salary, years in residence) but ended up having different outputs: one ended up having good credit and the other bad. This is possible because the feature set doesn&#39;t capture every possible factor.</p>

<p>This is mitigated by having a target distribution. Instead of having a function <script type="math/tex">f</script> that yields an exact answer <script type="math/tex">y</script> for any given <script type="math/tex">\xpoint</script>:</p>

<p><script type="math/tex; mode=display"> y = f(\xpoint) </script></p>

<p>We will use a target <em>distribution</em>, which still encodes <script type="math/tex">y</script>&#39;s dependence on <script type="math/tex">\xpoint</script>, but in a probabilistic manner:</p>

<p><script type="math/tex; mode=display"> y \sim P(y \mid \xpoint) </script></p>

<p>Now <script type="math/tex">(\xpoint, y)</script> will be generated by the joint distribution:</p>

<p><script type="math/tex; mode=display"> P(\xpoint) P(y \mid \xpoint) </script></p>

<p>This means that we will get a noisy target, which is equivalent to a deterministic target <script type="math/tex">f(\xpoint) = \mathbb {E} (y \mid \xpoint)</script> <strong>plus</strong> noise <script type="math/tex">y - f(\xpoint)</script>.</p>

<p>In fact, a deterministic target is a special case of a noisy target where:</p>

<p><script type="math/tex; mode=display">
P(y \mid \xpoint) = \begin{cases}
                      1 & y = f(\xpoint) \\ \\
                      0 & y \not= f(\xpoint)
                    \end{cases}
</script></p>

<p>What this means is that in the learning diagram, we replace the &quot;unknown target function <script type="math/tex">f\colon \mathcal {X \to Y}</script>&quot; box with &quot;unknown target distribution <script type="math/tex">P(y \mid \xpoint)</script> target function: <script type="math/tex">f\colon \mathcal {X \to Y}</script> plus noise&quot;.</p>

<p>It&#39;s important to notice the distinction between <script type="math/tex">P(y \mid \xpoint)</script> and <script type="math/tex">P(\xpoint)</script>. <script type="math/tex">P(\xpoint)</script> is the probability that was introduced to satisfy Hoeffding&#39;s Inequality, derived from the unknown input distribution. <script type="math/tex">P(y \mid \xpoint)</script> is the probability that was introduced to reflect the fact that real-world target functions are noisy. Both probabilities are derived from <em>unknown</em> probability distributions, they&#39;re unknown because they don&#39;t need to be known.</p>

<p>Both probabilities convey the probabilistic aspects of <script type="math/tex">\xpoint</script> and <script type="math/tex">y</script>. The target distribution <script type="math/tex">P(y \mid \xpoint)</script> is what we are trying to learn. The input distribution <script type="math/tex">P(\xpoint)</script> is simply quantifying the relative example of <script type="math/tex">\xpoint</script>; we&#39;re not trying to learn it.</p>

<p>The input distribution is the distribution of the feature set in the general population. For example, if the feature set simply consists of salary, then the input distribution says how many people make <script type="math/tex">70k, </script>100k, etc.</p>

<p>For this reason, the probabilities can be merged as:</p>

<p><script type="math/tex; mode=display"> P(\xpoint) P(y \mid \xpoint) \equiv P(\xpoint \cap y) </script> </p>

<p>This merging mixes two concepts. <script type="math/tex">P(\xpoint \cap y)</script> is not a target distribution for supervised learning. The target distribution that we are actually trying to learn is <script type="math/tex">P(y \mid \xpoint)</script>.</p>
<h1 id="training-vs.-testing">
<span class="hash">#</span>
<a href="#training-vs.-testing" class="header-link">Training vs. Testing</a>
</h1>
<p>Learning is feasible because it is likely that:</p>

<p><script type="math/tex; mode=display"> \outsample(g) \approx \insample(g) </script></p>

<p>However, is this <em>really</em> learning?</p>

<p>We&#39;ve defined learning as being <script type="math/tex">g \approx f</script>, which effectively means that the out-of-sample error for <script type="math/tex">g</script> should be close to zero:</p>

<p><script type="math/tex; mode=display"> \outsample(g) \approx 0 </script></p>

<p>The above means that &quot;we learned well,&quot; and it&#39;s achieved through:</p>

<p><script type="math/tex; mode=display"> \outsample(g) \approx \insample(g) \quad \text {and} \quad \insample(g) \approx 0 </script></p>

<p>Learning thus reduces to two questions:</p>

<ol>
<li>Can we make sure that <script type="math/tex">\outsample(g)</script> is close enough to <script type="math/tex">\insample(g)</script>?</li>
<li>Can we make <script type="math/tex">\insample(g)</script> small enough?</li>
</ol>

<p>There is a difference between training and testing. It&#39;s analogous to when one takes a practice exam compared to when one takes a final exam.</p>

<p>In the testing scenario, <script type="math/tex">\insample</script> is how well one did in the final exam, and <script type="math/tex">\outsample</script> is how well one understands the material in general. In this case, the probability that <script type="math/tex">\insample</script> tracks <script type="math/tex">\outsample</script>---that is, that doing well on the final exam means that they understood the material---increases as the number of questions in the final exam <script type="math/tex">N</script> increases:</p>

<p><script type="math/tex; mode=display"> P(|\insample - \outsample| \gt \epsilon) \leq 2\phantom {M} e^{-2 \epsilon^2 N} </script></p>

<p>In the training scenario, <script type="math/tex">\insample</script> is how well one did on the practice problems. The more practice problems <script type="math/tex">M</script> that were done, the more of a penalty that is incurred because the practice problems are pretty much memorized, and so the topics aren&#39;t learned in a more general manner:</p>

<p><script type="math/tex; mode=display"> P(|\insample - \outsample| \gt \epsilon) \leq 2Me^{-2 \epsilon^2 N} </script></p>
<h2 id="overlapping-events">
<span class="hash">#</span>
<a href="#overlapping-events" class="header-link">Overlapping Events</a>
</h2>
<p>The goal is to replace the <script type="math/tex">M</script> term with something more manageable since otherwise the above &quot;guarantee&quot; isn&#39;t much of a guarantee, since something as simple as the perceptron learning algorithm leads to <script type="math/tex">M = \infty</script>.</p>

<p>The bad event <script type="math/tex">\mathcal B_m</script> is:</p>

<p><script type="math/tex; mode=display"> |\insample - \outsample| \gt \epsilon </script></p>

<p>Bad events can be visualized as a Venn Diagram with varying degrees of overlap. However, the union bound is made regardless of the correlations between the bad events in order to be more general:</p>

<p><script type="math/tex; mode=display"> P(\mathcal B_1\ \mathbf {or}\ \mathcal B_2\ \mathbf {or}\ \dots\ \mathbf {or}\ \mathcal B_M) </script></p>

<p>As a result, it treated as if the bad events were disjoint, in which case the total area of the sum of all bad events is considered:</p>

<p><script type="math/tex; mode=display"> \leq \underbrace {P(\mathcal B_1) + P(\mathcal B_2) + \dots + P(\mathcal B_M)}_{\text {no overlaps: M terms}} </script></p>

<p>This is a poor bound because in reality the bad events could have significant amounts of overlapping. This is why <script type="math/tex">M = \infty</script> in many learning algorithms when using this very loose bound.</p>

<p>Given the following perceptron:</p>

<div style="text-align: center; margin-top: 10px">
  <img src="/images/notes/machine-learning/training-versus-testing/e-in.png">
  <img src="/images/notes/machine-learning/training-versus-testing/e-out.png">
</div>

<p>When the perceptron (the hypothesis) is changed, there is a huge amount of overlap:</p>

<p><img src="/images/notes/machine-learning/training-versus-testing/perceptron-overlap.png" class="center"></p>

<p>Only the area in yellow is what has changed between the two perceptrons; everything else overlaps.</p>

<p><script type="math/tex; mode=display">
\begin{align}
&\Delta \outsample : \text {change in +1 and -1 areas} \\
&\Delta \rlap {\insample} \phantom {\outsample} : \text {change in labels of data points}
\end{align}
</script></p>

<p>Given this observation---that different hypotheses have significant amounts of overlapping with others---we would like to make the statement that one hypothesis exceeds epsilon often when another hypothesis exceeds epsilon.</p>

<p><script type="math/tex; mode=display"> |\insample(h_1) - \outsample(h_1)| \approx |\insample(h_2) - \outsample(h_2)| </script></p>
<h2 id="dichotomies">
<span class="hash">#</span>
<a href="#dichotomies" class="header-link">Dichotomies</a>
</h2>
<p>When counting the number of hypotheses, the entire input space is taken into consideration. In the case of a perceptron, each perceptron differs from another if they differ in at least one input point, and since the input is continuous, there are an infinite number of different perceptrons.</p>

<p>Instead of counting the number of hypotheses in the entire input space, we are going to restrict the count only to the sample: a finite set of input points. Then, simply count the number of the possible <em>dichotomies</em>. A dichotomy is like a mini-hypothesis, it&#39;s a configuration of labels on the sample&#39;s input points.</p>

<p>A hypothesis is a function that maps an input from the entire <em>input space</em> to a result:</p>

<p><script type="math/tex; mode=display"> h\colon \mathcal X \to \{-1, +1\} </script></p>

<p>The number of hypotheses <script type="math/tex">|\mathcal H|</script> can be infinite.</p>

<p>A dichotomy is a hypothesis that maps from an input from the <em>sample size</em> to a result:</p>

<p><script type="math/tex; mode=display"> h\colon \{\mathbf {x_1, x_2, \dots, x_N}\} \to \{-1, +1\} </script></p>

<p>The number of dichotomies <script type="math/tex">|\mathcal H(\mathbf {x_1, x_2, \dots, x_N})|</script> is at most <script type="math/tex">2^N</script>, where <script type="math/tex">N</script> is the sample size. This makes it a candidate for replacing <script type="math/tex">M</script>.</p>
<h2 id="growth-function">
<span class="hash">#</span>
<a href="#growth-function" class="header-link">Growth Function</a>
</h2>
<p>The <em>growth function</em> counts the <em>most</em> dichotomies on any <script type="math/tex">N</script> points.</p>

<p><script type="math/tex; mode=display">
\def \growthfunc {m_{\mathcal H}}
\growthfunc(N) = \max_{\mathbf {x_1, \dots, x_N} \in \mathcal X} |\mathcal H(\mathbf {x_1, \dots, x_N})|
</script></p>

<p>This translates to choosing any <script type="math/tex">N</script> points and laying them out in any fashion in the input space. Determining <script type="math/tex">m</script> is equivalent to looking for such a layout of the <script type="math/tex">N</script> points that yields the <em>most</em> dichotomies.</p>

<p>The growth function satisfies:</p>

<p><script type="math/tex; mode=display"> \growthfunc(N) \leq 2^N </script></p>

<p>This can be applied to the perceptron. For example, when <script type="math/tex">N = 4</script>, we can lay out the points so that they are easily separated. However, given a layout, we must then consider all possible configurations of labels on the points, one of which is the following:</p>

<p><img src="/images/notes/machine-learning/training-versus-testing/breaking-point.png" class="center"></p>

<p>This is where the perceptron breaks down because it <em>cannot</em> separate that configuration, and so <script type="math/tex">\growthfunc(4) = 14</script> because two configurations---this one and the one in which the left/right points are blue and top/bottom are red---cannot be represented.</p>

<p>For this reason, we have to expect that that for perceptrons, <script type="math/tex">m</script> can&#39;t be the maximum possible because it would imply that perceptrons are as strong as can possibly be.</p>

<p>We will try to come up with a hypothesis function that can easily determine the value of <script type="math/tex">m</script>.</p>
<h3 id="positive-rays">
<span class="hash">#</span>
<a href="#positive-rays" class="header-link">Positive Rays</a>
</h3>
<p>Positive rays are defined on <script type="math/tex">\mathbb R</script> and define a point <script type="math/tex">a</script> such that:</p>

<p><script type="math/tex; mode=display">
h(x) = \begin{cases}
         +1 & \text {if } x > a \\ \\
         -1 & \text {otherwise}
       \end{cases}
</script></p>

<p><img src="/images/notes/machine-learning/training-versus-testing/positive-rays.png" class="center"></p>

<p><script type="math/tex">\mathcal H</script> is the set from the reals to a label <script type="math/tex">-1</script> or <script type="math/tex">+1</script>, so <script type="math/tex">h\colon \mathbb R \to \{-1, +1\}</script>. Put more simply:</p>

<p><script type="math/tex; mode=display"> h(x) = \mathrm {sign} (x - a) </script></p>

<p>The dichotomy is determined by noticing between which two points in the input set <script type="math/tex">a</script> falls into, these are the line segments between the input points on the line, of which there are <script type="math/tex">N + 1</script>, so:</p>

<p><script type="math/tex; mode=display"> \growthfunc(N) = N + 1 </script></p>
<h3 id="positive-intervals">
<span class="hash">#</span>
<a href="#positive-intervals" class="header-link">Positive Intervals</a>
</h3>
<p>Instead of a ray, we define an interval <script type="math/tex">[l, r]</script> where any point that falls within it is defined as <script type="math/tex">+1</script> and everything outside it is <script type="math/tex">-1</script>:</p>

<p><script type="math/tex; mode=display">
h(x) = \begin{cases}
         +1 & \text {if } l \leq x \leq r \\ \\
         -1 & \text {otherwise}
       \end{cases}
</script></p>

<p><img src="/images/notes/machine-learning/training-versus-testing/positive-intervals.png" class="center"></p>

<p>The way to vary the different dichotomies is by choosing two line segments---of which there are again <script type="math/tex">N + 1</script>---at which to place the interval ends. For this reason, the growth function is:</p>

<p><script type="math/tex; mode=display"> \growthfunc(N) = {N + 1 \choose 2} </script></p>

<p>However, this doesn&#39;t count the configuration in which the interval ends lie on the same line segment, making all points <script type="math/tex">-1</script>, so we add that:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\growthfunc(N) &= {N + 1 \choose 2} + 1 \\
&= \frac 1 2 N^2 + \frac 1 2 N + 1
\end{align}
</script></p>
<h3 id="convex-sets">
<span class="hash">#</span>
<a href="#convex-sets" class="header-link">Convex Sets</a>
</h3>
<p>In this case, we take a plane and not a line so that <script type="math/tex">\mathcal H</script> is the set of <script type="math/tex">h\colon \mathbb R^2 \to \{-1, +1\}</script>. In this model, if a point is found within a convex region then it results in <script type="math/tex">+1</script>, and <script type="math/tex">-1</script> otherwise.</p>

<p><script type="math/tex; mode=display">
h(x) = \begin{cases}
         +1 & \text {if inside region} \\ \\
         -1 & \text {otherwise}
       \end{cases}
</script></p>

<p>A convex region is a region where for any two points picked within a region, the entirety of the line segment connecting them lies within the region. For example, the left image is a convex region, but the one on the right isn&#39;t because the line segment connecting the chosen two points doesn&#39;t fall entirely within the region:</p>

<div style="text-align: center; margin-top: 10px">
  <img src="/images/notes/machine-learning/training-versus-testing/convex-region.png">
  <img src="/images/notes/machine-learning/training-versus-testing/non-convex-region.png">
</div>

<p>Convex regions can therefore be used to model dichotomies. The best layout for the points in the input set is to place them on the perimeter of a circle, in which case any configuration of the labels can be satisfied with a convex region as shown below:</p>

<p><img src="/images/notes/machine-learning/training-versus-testing/convex-region-dichotomy.png" class="center"></p>

<p>When a hypothesis set is able to reach every possible dichotomy, it is said that the hypothesis set shattered the points. This means that the hypothesis set is very good at fitting the data, but this is a trade off with generalization.</p>
<h2 id="break-point">
<span class="hash">#</span>
<a href="#break-point" class="header-link">Break Point</a>
</h2>
<p>What happens when <script type="math/tex">\growthfunc(N)</script> replaces <script type="math/tex">M</script>? If the growth function is polynomial then everything will be fine. So we need to prove that <script type="math/tex">\growthfunc(N)</script> is polynomial.</p>

<p>If no data set of size <script type="math/tex">k</script> can be shattered by <script type="math/tex">\mathcal H</script>, then <script type="math/tex">k</script> is a <em>break point</em> for <script type="math/tex">\mathcal H</script>. By extension, this means that a bigger data set cannot be shattered either. In other words, given a hypothesis set, a break point is the point at which we fail to achieve all possible dichotomies.</p>

<p>We <a href="#growth-function">already saw</a> that for perceptrons, <script type="math/tex">k = 4</script>.</p>

<p>For positive rays where <script type="math/tex">\growthfunc(N) = N + 1</script>, the break point is <script type="math/tex">k = 2</script> because <script type="math/tex">2 + 1 = 3</script> and this is not the same as <script type="math/tex">2^2 = 4</script>. This is evidenced by the fact that the below configuration cannot be represented by a positive ray:</p>

<p><img src="/images/notes/machine-learning/training-versus-testing/positive-ray-break-point.png" class="center"></p>

<p>Similarly, for positive intervals where <script type="math/tex">\growthfunc(N) = \frac 1 2 N^2 + \frac 1 2 N + 1</script>, the break point is <script type="math/tex">k = 3</script> because <script type="math/tex">\growthfunc(3) = 7 \not= 2^3 = 8</script>. The following configuration is an example configuration that cannot be represented by a positive interval:</p>

<p><img src="/images/notes/machine-learning/training-versus-testing/positive-interval-break-point.png" class="center"></p>

<p>Convex sets don&#39;t have a break point, so <script type="math/tex">k = \infty</script>. Having no break point means that <script type="math/tex">\growthfunc(N) = 2^N</script>.</p>

<p>On the other hand, if there <em>is</em> a break point, then <script type="math/tex">\growthfunc(n)</script> is guaranteed to be polynomial (growth) on <script type="math/tex">N</script>. <strong>This means that it is possible to learn with the given hypothesis set.</strong></p>

<p>For example, consider a puzzle where there are three binary points <script type="math/tex">\mathbf x_1</script>, <script type="math/tex">\mathbf x_2</script>, and <script type="math/tex">\mathbf x_3</script>. The constraint is that the breaking point is <script type="math/tex">k = 2</script>, this effectively means that no combination of 2 points can be shattered, which in this context means that no two points can hold every possible combination: 00, 01, 10, and 11. A possible solution to the puzzle is:</p>

<table>
<thead>
<tr>
<th style="text-align: center"><script type="math/tex">\mathbf x_1</script></th>
<th style="text-align: center"><script type="math/tex">\mathbf x_2</script></th>
<th style="text-align: center"><script type="math/tex">\mathbf x_3</script></th>
</tr>
</thead>

<tbody>
<tr>
<td style="text-align: center">○</td>
<td style="text-align: center">○</td>
<td style="text-align: center">○</td>
</tr>
<tr>
<td style="text-align: center">○</td>
<td style="text-align: center">○</td>
<td style="text-align: center">●</td>
</tr>
<tr>
<td style="text-align: center">○</td>
<td style="text-align: center">●</td>
<td style="text-align: center">○</td>
</tr>
<tr>
<td style="text-align: center">●</td>
<td style="text-align: center">○</td>
<td style="text-align: center">○</td>
</tr>
</tbody>
</table>
<h1 id="theory-of-generalization">
<span class="hash">#</span>
<a href="#theory-of-generalization" class="header-link">Theory of Generalization</a>
</h1>
<p>We want to prove that <script type="math/tex">\growthfunc(N)</script> is indeed polynomial, and also that it is therefore possible to use it to replace <script type="math/tex">M</script>.</p>

<p>First, to show that <script type="math/tex">\growthfunc(N)</script> is polynomial, we will show that it is bound by a polynomial:</p>

<p><script type="math/tex; mode=display"> \growthfunc(N) \leq \dots \leq \dots \leq \text {a polynomial} </script></p>

<p>We will use the quantity <script type="math/tex">B(N, k)</script> which is the maximum number of dichotomies on <script type="math/tex">N</script> points with break point <script type="math/tex">k</script>. In other words, the maximum number of rows we can get on <script type="math/tex">N</script> points such that no <script type="math/tex">k</script> columns have all possible patterns</p>
<h2 id="conceptualization">
<span class="hash">#</span>
<a href="#conceptualization" class="header-link">Conceptualization</a>
</h2>
<p>To understand how <script type="math/tex">B(N, k)</script> is defined, we should first determine each combination that is possible given a break point <script type="math/tex">k</script> in a table with a column for every feature <script type="math/tex">\mathbf x_i</script>. We will then group these rows into two major groups.</p>

<p><script type="math/tex">S_1</script> is the group of <script type="math/tex">\alpha</script> rows that are entirely unique on <script type="math/tex">\mathbf x_1</script> to <script type="math/tex">\mathbf x_N</script>. If the last column <script type="math/tex">\mathbf x_N</script> were removed, there would be no other row in the set with the same <script type="math/tex">\mathbf x_1</script> to <script type="math/tex">\mathbf x_{N - 1}</script>.</p>

<p>Since there are <script type="math/tex">\alpha</script> rows in <script type="math/tex">S_1</script>, we will say that <script type="math/tex">B(N, k)</script> is defined in terms of it as:</p>

<p><script type="math/tex; mode=display"> B(N, k) = \alpha\ +\ ? </script></p>

<p><script type="math/tex">S_2</script> is a group comprised of two subgroups: <script type="math/tex">S_2^+</script> and <script type="math/tex">S_2^-</script>. The rows that belong here are those where, if the last row <script type="math/tex">\mathbf x_N</script> were removed, there would be exactly one other row with the same <script type="math/tex">\mathbf x_1</script> to <script type="math/tex">\mathbf x_{N - 1}</script>. If the last column is <script type="math/tex">-1</script> then it goes into <script type="math/tex">S_2^-</script> and vice versa. There are <script type="math/tex">\beta</script> rows in <script type="math/tex">S_2^+</script> and <script type="math/tex">\beta</script> rows in <script type="math/tex">S_2^-</script> for a total of <script type="math/tex">2\beta</script> rows in <script type="math/tex">S_2</script>.</p>

<p>We can now add the extra term to <script type="math/tex">B(N, k)</script>:</p>

<p><script type="math/tex; mode=display"> B(N, k) = \alpha + 2\beta </script></p>
<h2 id="estimating-script-typemathtexalphascript-and-script-typemathtexbetascript">
<span class="hash">#</span>
<a href="#estimating-script-typemathtexalphascript-and-script-typemathtexbetascript" class="header-link">Estimating <script type="math/tex">\alpha</script> and <script type="math/tex">\beta</script></a>
</h2>
<p>We will now look at estimating <script type="math/tex">\alpha</script> and <script type="math/tex">\beta</script> by relating it to smaller numbers of <script type="math/tex">N</script> and <script type="math/tex">k</script> in order to achieve a recursive definition.</p>

<p>If we only focus on the columns <script type="math/tex">\mathbf x_1</script> to <script type="math/tex">\mathbf x_{N - 1}</script>, then we can observe that the only unique rows are those in <script type="math/tex">S_1</script> and those in <em>one</em> of the subgroups of <script type="math/tex">S_2</script>. This is relevant to <script type="math/tex">B(N, k)</script> because it only concerns those rows that are <em>different</em> such that a condition occurs, the condition being:</p>

<p><script type="math/tex; mode=display"> \alpha + \beta \leq B(N - 1, k) </script></p>

<p>We can make this relation because we are now considering <script type="math/tex">N - 1</script> columns instead of all <script type="math/tex">N</script>, and we are still using a break point of <script type="math/tex">k</script>. We say &quot;<script type="math/tex">\leq</script>&quot; and not &quot;<script type="math/tex">=</script>&quot; as before because before, we made it equal by construction. However, we arrived at the mini-matrix of <script type="math/tex">\alpha + \beta</script> rows in a manner where we can&#39;t be positive that it maximizes the number of rows. However, we can be sure that it&#39;s at <em>most</em> <script type="math/tex">B(N, k)</script> because that is the maximum number that is possible.</p>
<h2 id="estimating-script-typemathtexbetascript">
<span class="hash">#</span>
<a href="#estimating-script-typemathtexbetascript" class="header-link">Estimating <script type="math/tex">\beta</script></a>
</h2>
<p>We now want to estimate <script type="math/tex">\beta</script> by itself. To do this, we&#39;ll focus only on the <script type="math/tex">S_2</script> group, comprised of <script type="math/tex">S_2^+ \cap S_2^-</script> rows. Specifically, we&#39;ll focus on a single mini-matrix of <script type="math/tex">\beta</script> rows, such as <script type="math/tex">S_2^+</script>, <em>without</em> the last column <script type="math/tex">\mathbf x_n</script>. We will make the assertion that the break point for this mini-matrix is <script type="math/tex">k - 1</script>.</p>

<p>We can say this is true because, if it weren&#39;t, then it would mean that it would indeed be <em>possible</em> to shatter <script type="math/tex">k - 1</script> points within that mini-matrix. This would mean that we could attain <script type="math/tex">2^{k - 1}</script> different rows within the mini-matrix; one for each different combination.</p>

<p>However, we must remember that we&#39;re only focusing on one sub-matrix within the overall matrix. Further, we know that by definition there is another sub-matrix <script type="math/tex">S_2^-</script> with the same amount of rows.</p>

<p>Therefore, if we then were to add back in the <script type="math/tex">\mathbf x_n</script> column and also consider the rows from <script type="math/tex">S_2^-</script>, we would have <script type="math/tex">2^k</script> rows with every possible combination (i.e. shattering <script type="math/tex">k</script> points), which would <em>contradict</em> the fact that <script type="math/tex">k</script> is the break point for the larger matrix. Therefore, we can be certain that the break point for the mini-matrix is at most <script type="math/tex">k - 1</script>. It could be lower, but since we are trying to achieve an upper bound, this is irrelevant.</p>

<p>With this in mind, we can define a relation for the maximum number of dichotomies within this mini-matrix which is composed of <script type="math/tex">N - 1</script> points and for which we have proved that <script type="math/tex">k - 1</script> is the break point:</p>

<p><script type="math/tex; mode=display"> \beta \leq B(N - 1, k - 1) </script></p>
<h2 id="bound-relation">
<span class="hash">#</span>
<a href="#bound-relation" class="header-link">Bound Relation</a>
</h2>
<p>We want to arrive at an upper bound without having to know <script type="math/tex">\alpha</script> or <script type="math/tex">\beta</script>.</p>

<p>If we combine the two derived relations into a system of inequalities:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\alpha + \beta &\leq B(N - 1, k) \\
\beta &\leq B(N - 1, k - 1)
\end{align}
</script></p>

<p>Adding them together yields:</p>

<p><script type="math/tex; mode=display"> \alpha + 2\beta \leq B(N - 1, k) + B(N - 1, k - 1) </script></p>

<p>And since we know that:</p>

<p><script type="math/tex; mode=display"> B(N, k) = \alpha + 2\beta </script></p>

<p>We can say that:</p>

<p><script type="math/tex; mode=display"> B(N, k) \leq B(N - 1, k) + B(N - 1, k - 1) </script></p>
<h2 id="computing-the-bound">
<span class="hash">#</span>
<a href="#computing-the-bound" class="header-link">Computing the Bound</a>
</h2>
<p>To define this recursive bound, we will have to compute the values of the bound for smaller values of <script type="math/tex">N</script> and <script type="math/tex">k</script>.</p>
<h3 id="numerical-derivation">
<span class="hash">#</span>
<a href="#numerical-derivation" class="header-link">Numerical Derivation</a>
</h3>
<p>We can start doing this by filling out a table with the left hand side being <script type="math/tex">N</script> and the top being <script type="math/tex">k</script>, where any element in the table specifies the maximum number of dichotomies for that combination <script type="math/tex">B(N, k)</script>. We begin doing this by filling out the bounds for when <script type="math/tex">k = 1</script> and when <script type="math/tex">N = 1</script>.</p>

<p>When <script type="math/tex">k = 1</script>, we will only ever have one dichotomy because we&#39;re using a binary function, since having more than one row would mean that one of the columns is fully exhausted.</p>

<p>When <script type="math/tex">N = 1</script> and <script type="math/tex">k \geq 2</script>, given that we only have one point, the maximum number of dichotomies we can represent with a binary function is two.</p>

<p>With these bounds, we can then define every other point in the table:</p>

<p><img src="/images/notes/machine-learning/theory-of-generalization/dichotomy-table.png" class="center"></p>

<p>Remembering that:</p>

<p><script type="math/tex; mode=display"> B(N, k) \leq B(N - 1, k) + B(N - 1, k - 1) </script></p>

<p>We can take an example such as <script type="math/tex">B(3, 3)</script>. To get this value, we add <script type="math/tex">B(N - 1, k - 1) = B(2, 2) = 3</script>, with <script type="math/tex">B(N - 1, k) = B(2, 3) = 4</script>, resulting in <script type="math/tex">B(3, 3) = 7</script>.</p>
<h3 id="analytical-derivation">
<span class="hash">#</span>
<a href="#analytical-derivation" class="header-link">Analytical Derivation</a>
</h3>
<p>A more analytic solution for computing the bound is given by the theorem:</p>

<p><script type="math/tex; mode=display">
\def \upperbound {\sum_{i = 0}^{k - 1} {N \choose i}}
B(N, k) \leq \upperbound
</script></p>

<p>We can easily verify that this is true for the boundary conditions that we derived in the table above.</p>

<p>For every other condition we must consider the induction step. Given the above theorem, it is implied that:</p>

<p><script type="math/tex; mode=display"> \upperbound = \sum_{i = 0}^{k - 1} {N - 1 \choose i} + \sum_{i = 0}^{k - 2} {N - 1 \choose i} </script></p>

<p>So it&#39;s recursively implied that:</p>

<p><script type="math/tex; mode=display">
\begin{align}
B(N - 1, k) &= \sum_{i = 0}^{k - 1} {N - 1 \choose i} \\
B(N - 1, k - 1) &= \sum_{i = 0}^{k - 2} {N - 1 \choose i}
\end{align}
</script></p>

<p>We can accomplish this by reducing the right hand side to resemble the left hand side:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\sum_{i = 0}^{k - 1} {N \choose i} &= \sum_{i = 0}^{k - 1} {N - 1 \choose i} + \sum_{i = 0}^{k - 2} {N - 1 \choose i} \\
&= 1 + \sum_{i = 1}^{k - 1} {N - 1 \choose i} + \sum_{i = 1}^{k - 1} {N - 1 \choose i - 1} \\
&= 1 + \sum_{i = 1}^{k - 1} \left[ {N - 1 \choose i} + {N - 1 \choose i - 1} \right]
\end{align}
</script></p>

<p>We can reduce the final expression further by considering the example. You are in a room full of <script type="math/tex">N</script> people, and want to determine how to choose <script type="math/tex">10</script> people in the room: <script type="math/tex">\smash {N \choose 10}</script>. However, this can also be represented as the number of ways we can count <script type="math/tex">10</script> people <em>excluding</em> you and <em>including</em> you; both cases are disjoint and together cover all combinations.</p>

<p>The number ways to choose <script type="math/tex">10</script> people <em>excluding</em> you is <script type="math/tex">\smash {N - 1 \choose 10}</script>. It&#39;s <script type="math/tex">N - 1</script> because you are being excluded from the total number of people. The number of ways to choose <script type="math/tex">10</script> people <em>including</em> you is <script type="math/tex">\smash {N - 1 \choose 9}</script>. It&#39;s <script type="math/tex">9</script> because you&#39;re already being included, so you just need <script type="math/tex">9</script> others. Adding these both together should therefore be the same as <script type="math/tex">\smash {N \choose 10}</script>.</p>

<p><script type="math/tex; mode=display">
\begin{align}
\sum_{i = 0}^{k - 1} {N \choose i} &= 1 + \sum_{i = 1}^{k - 1} \left[ {N - 1 \choose i} + {N - 1 \choose i - 1} \right] \\
&= 1 + \sum_{i = 1}^{k - 1} {N \choose i} \\
&= \upperbound \\ \\
\therefore\ B(N, k) &\leq \upperbound \quad \text {q.e.d}
\end{align}
</script></p>

<p>So the above proves that the growth function <script type="math/tex">\growthfunc(N)</script> is bounded by a polynomial <script type="math/tex">B(N, k)</script>. It&#39;s polynomial because for a given <script type="math/tex">\mathcal H</script>, the break point <script type="math/tex">k</script> is fixed.</p>

<p><script type="math/tex; mode=display"> \growthfunc(N) \leq \underbrace {\upperbound}_{\text {maximum power is } N^{k - 1}} </script></p>

<p>To show that this holds, we can use it to determine the growth function <script type="math/tex">\growthfunc</script> for:</p>

<ul>
<li><script type="math/tex">\mathcal H</script> is <strong>positive rays</strong> (break point <script type="math/tex">k = 2</script>):

<ul>
<li><script type="math/tex">N + 1 \leq N + 1</script></li>
</ul></li>
<li><script type="math/tex">\mathcal H</script> is <strong>positive intervals</strong> (break point <script type="math/tex">k = 3</script>):

<ul>
<li><script type="math/tex">\frac 1 2 N^2 + \frac 1 2 N + 1 \leq \frac 1 2 N^2 + \frac 1 2 N + 1</script></li>
</ul></li>
<li><script type="math/tex">\mathcal H</script> is <strong>2D perceptrons</strong> (break point <script type="math/tex">k = 4</script>):

<ul>
<li><script type="math/tex">\text {?} \leq \frac 1 6 N^3 + \frac 5 6 N + 1</script></li>
</ul></li>
</ul>
<h2 id="vapnik-chervonenkis-inequality">
<span class="hash">#</span>
<a href="#vapnik-chervonenkis-inequality" class="header-link">Vapnik-Chervonenkis Inequality</a>
</h2>
<p>We would like to replace the <script type="math/tex">M</script> factor in Hoeffding&#39;s Inequality:</p>

<p><script type="math/tex; mode=display"> P(|\insample(g) - \outsample(g)| \gt \epsilon) \leq 2Me^{-2 \epsilon^2 N} </script></p>

<p>The following straightforward substitution would not work:</p>

<p><script type="math/tex; mode=display"> P(|\insample(g) - \outsample(g)| \gt \epsilon) \leq 2 \growthfunc(N) e^{-2 \epsilon^2 N} </script></p>

<p>This is because we need to consider the following questions:</p>

<p>How does <script type="math/tex">\growthfunc(N)</script> relate to overlaps? If we use the VC bound, it takes into consideration the various overlaps of the error.</p>

<p>What do we do about <script type="math/tex">\outsample</script>? The problem is that to determine the overlap we need to determine what a &quot;bad point&quot; is: a bad point is one that deviates from <script type="math/tex">\outsample</script>, so we need to know <script type="math/tex">\outsample</script>. Going back to the sheet with the holes analogy where we could only see the color of the points but not the separating line: to determine the error overlap between two separating lines, we would have to remove the covering sheet (the one with the holes).</p>

<p>We could alleviate this by getting rid of <script type="math/tex">\outsample</script>. To do this, we pick <strong>two</strong> samples instead of just one. We already know that <script type="math/tex">\outsample</script> and <script type="math/tex">\insample</script> track each other because <script type="math/tex">\insample</script> is generated from the same distribution as <script type="math/tex">\outsample</script>. We give the two samples different names: <script type="math/tex">\insample</script> and <script type="math/tex">\insample'</script>. Does <script type="math/tex">\insample</script> track <script type="math/tex">\insample'</script>? It is obvious that each of them track <script type="math/tex">\outsample</script>, since they were generated from the same distribution, so consequently they <strong>do track each other</strong>, albeit perhaps more loosely.</p>

<p>The advantage of this fact is that, when considering multiple bins, the same situation applies. That is, with multiple bins, the tie between <script type="math/tex">\outsample</script> and <script type="math/tex">\insample</script> became looser and looser. Likewise, when considering two samples, they do track each other but it becomes looser and looser. The advantage of using two samples is that we are in the realm of dichotomies. We are in the realm of dichotomies because we now only care about what happens in the sample, not the input space, even though the sample is larger now at <script type="math/tex">2N</script> not <script type="math/tex">N</script>.</p>

<p>With all of this in mind, the <em>Vapnik-Chervonenkis Inequality</em> is defined as:</p>

<p><script type="math/tex; mode=display"> P(|\insample(g) - \outsample(g)| \gt \epsilon) \leq 4 \growthfunc(2N) e^{-{\frac 1 8} \epsilon^2 N} </script></p>

<p>The growth function is parameterized with <script type="math/tex">2N</script> because we are now considering two samples. This inequality more or less says that we are making a statement that is probably (RHS), approximately (LHS epsilon), correct.</p>
<h1 id="vc-dimension">
<span class="hash">#</span>
<a href="#vc-dimension" class="header-link">VC Dimension</a>
</h1>
<p>The <em>VC Dimension</em> is a quantity defined for a hypothesis set <script type="math/tex">\mathcal H</script> denoted by <script type="math/tex">\def \vc {d_{\text {VC}}} \vc(\mathcal H)</script> and is defined as the most points that <script type="math/tex">\mathcal H</script> can shatter; the largest value of <script type="math/tex">N</script> for which <script type="math/tex">\growthfunc(N) = 2^N</script>.</p>

<p><script type="math/tex; mode=display">
\begin{align}
N &\leq \vc(\mathcal H)\ \Longrightarrow\ \mathcal H \text { can shatter } N \text { points } \\
N &\gt \vc(\mathcal H)\ \Longrightarrow\ N \text { is a break point for } H
\end{align}
</script></p>

<p>Therefore, the growth function can be defined in terms of a break point <script type="math/tex">k</script>:</p>

<p><script type="math/tex; mode=display"> \growthfunc(N) \leq \upperbound </script></p>

<p>It can also be defined in terms of the VC dimension <script type="math/tex">\vc</script>:</p>

<p><script type="math/tex; mode=display"> \growthfunc(N) \leq \underbrace {\sum_{i = 0}^{\vc} {N \choose i}}_{\text {maximum power is } N^{\vc}} </script></p>

<p>With respect to learning, the effect of the VC dimension is that if the VC dimension is finite, then the hypothesis will generalize:</p>

<p><script type="math/tex; mode=display"> \vc(\mathcal H)\ \Longrightarrow\ g \in \mathcal H \text { will generalize } </script></p>

<p>The key observation here is that this statement is <strong>independent of</strong>:</p>

<ul>
<li>the learning algorithm</li>
<li>the input distribution</li>
<li>the target function</li>
</ul>

<p>The only things that factor into this are the training examples, the hypothesis set, and the final hypothesis.</p>
<h2 id="vc-dimension-of-perceptrons">
<span class="hash">#</span>
<a href="#vc-dimension-of-perceptrons" class="header-link">VC Dimension of Perceptrons</a>
</h2>
<p>We already know that <script type="math/tex">\vc = 3</script> for <script type="math/tex">d = 2</script> (2D). However, we would like to generalize this for any dimension. We will argue that <script type="math/tex">\vc = d + 1</script>. We prove this by showing that <script type="math/tex">\vc \leq d + 1</script> and <script type="math/tex">\vc \geq d + 1</script>.</p>

<p>To do this we will first construct a set of <script type="math/tex">N = d + 1</script> points in <script type="math/tex">\mathbb R^d</script> in such a way that they can be shattered by the perceptron. We begin this by first setting the first element of every row vector to <script type="math/tex">1</script> since it corresponds to <script type="math/tex">x_0</script>: the threshold weight which we have already established is always set to <script type="math/tex">1</script>. The rest of the rows are set so that they form an identity matrix which is easily invertible, which is a property that we will show to mean that every point can be shattered:</p>

<p><script type="math/tex; mode=display">
\mathrm {X} = \begin{bmatrix}
                —\ x_1^{\mathrm {T}}\ — \\
                —\ x_2^{\mathrm {T}}\ — \\
                \vdots \\
                —\ x_{d + 1}^{\mathrm {T}}\ —
              \end{bmatrix} = 
              \begin{bmatrix}
                1 & 0 & 0 & \cdots & 0 \\
                1 & 1 & 0 & \cdots & 0 \\
                1 & 0 & 1 & \cdots & 0 \\
                & \vdots & & \ddots & 0 \\
                1 & 0 & \cdots & 0 & 1
              \end{bmatrix}
</script></p>

<p>The key observation is that <script type="math/tex">\mathrm X</script> is invertible.</p>

<p><script type="math/tex; mode=display">
\text {For any } \mathrm y = \begin{bmatrix}
                               y_1 \\
                               y_2 \\
                               \vdots \\
                               y_n
                             \end{bmatrix} = 
                             \begin{bmatrix}
                               \pm 1 \\
                               \pm 1 \\
                               \vdots \\
                               \pm 1 \\
                             \end{bmatrix}
</script></p>

<p>The question is, can we find a vector <script type="math/tex">\mathbf w</script> satisfying:</p>

<p><script type="math/tex; mode=display"> \text {sign}(\mathbf {Xw}) = \mathbf y </script></p>

<p>We can do this by changing the requirement to be:</p>

<p><script type="math/tex; mode=display"> \mathbf {Xw} = \mathbf y </script></p>

<p>This is valid because we&#39;re using a binary function.</p>

<p>From here it&#39;s a straightforward process to finding the vector <script type="math/tex">\mathbf w</script> by isolating it, made possible by multiplying both sides by the inverse of the feature matrix <script type="math/tex">\mathrm X</script>, which we know is invertible because we specifically constructed it to be:</p>

<p><script type="math/tex; mode=display"> \mathbf w = \mathbf {X^{-1}y} </script></p>

<p>Since we were able to come up with <script type="math/tex">\mathbf w</script>, it means that we were able to shatter <script type="math/tex">d + 1</script> points. More specifically, it means that we know for a fact that we can shatter <em>at least</em> <script type="math/tex">d + 1</script> points, but it&#39;s entirely possible that we can shatter more than that:</p>

<p><script type="math/tex; mode=display"> \vc \geq d + 1 </script></p>

<p>Therefore, to prove that <script type="math/tex">\vc = d + 1</script>, we must now show that <script type="math/tex">\vc \leq d + 1</script>, since the combination of both of these proofs would prove that <script type="math/tex">\vc = d + 1</script>. To prove that <script type="math/tex">\vc \leq d + 1</script>, we must show that we <strong>cannot</strong> shatter <em>any</em> set of <script type="math/tex">d + 2</script> points.</p>

<p>For any <script type="math/tex">d + 2</script> points <script type="math/tex">\mathbf {x}_1, \dots, \mathbf {x}_{d + 1}, \mathbf {x}_{d + 2}</script>, it is obvious that there are more points than dimensions. This is because each feature vector <script type="math/tex">\mathbf x_i</script> is a <script type="math/tex">d + 1</script> vector. Since there are more vectors than dimensions, we know that the vectors are linearly dependent; one of the vectors <script type="math/tex">\mathbf x_j</script> can be represented in terms of the other <script type="math/tex">\mathbf x_i</script> vectors:</p>

<p><script type="math/tex; mode=display"> \mathbf x_j = \sum_{i \neq j} a_i \mathbf x_i </script></p>

<p>It can be said that not all of the <script type="math/tex">a_i</script> scalars are zeros, this follows from the fact that <script type="math/tex">x_0</script> is always <script type="math/tex">1</script>.</p>

<p>Now consider the specific dichotomy where the vectors in <script type="math/tex">\mathbf x_i</script> with a non-zero <script type="math/tex">a_i</script> get the value <script type="math/tex">y_i = \text {sign}(a_i)</script>, the vectors with zero-valued <script type="math/tex">a_i</script> get the value <script type="math/tex">\pm 1</script>, and the vector <script type="math/tex">\mathbf x_j</script> gets the value <script type="math/tex">y_i = -1</script>:</p>

<p><script type="math/tex; mode=display">
\begin{align}
y_i &= \text {sign}(\mathbf w^{\mathrm T} \mathbf x_i) =
       \begin{cases}
         \text {sign}(a_i) & \text {if } a_i \text { is non-zero} \\ \\
         \pm 1 & \text {otherwise}
       \end{cases} \\ \\
y_j &= \text {sign}(\mathbf w^{\mathrm T} \mathbf x_j) = -1
\end{align}
</script></p>

<p>We are going to argue that no perceptron can implement this dichotomy. Remember that <script type="math/tex">\mathbf x_j</script> is the linear sum of the rest of the vectors <script type="math/tex">\mathbf x_i</script>, each scaled by a factor <script type="math/tex">a_i</script>. We now multiply them by a weight vector, in order to represent a perceptron:</p>

<p><script type="math/tex; mode=display">
\mathbf x_j = \sum_{i \neq j} a_i \mathbf x_i\ \Longrightarrow\ \mathbf w^{\mathrm T} \mathbf x_j = \sum_{i \neq j} a_i \mathbf w^{\mathrm T} \mathbf x_i
</script></p>

<p>An observation to make is that if by definition of the dichotomy, <script type="math/tex">\text {sign}(\mathbf w^{\mathrm T} \mathbf x_i) = \text {sign}(a_i)</script>, then it means that the signs of both are the same. By extension, this means that their product will always be greater than zero:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\text {if } &y_i = \text {sign}(\mathbf w^{\mathrm T} \mathbf x_i) = \text {sign}(a_i) \\
\text {then } &a_i \mathbf w^{\mathrm T} \mathbf x_i \gt 0
\end{align}
</script></p>

<p>This by extension means that the sum of the <script type="math/tex">\mathbf x_i</script>&#39;s will also greater than zero:</p>

<p><script type="math/tex; mode=display">
\mathbf w^{\mathrm T} \mathbf x_j = \sum_{i \neq j} a_i \mathbf w^{\mathrm T} \mathbf x_i > 0
</script></p>

<p>Therefore, since we defined <script type="math/tex">y_j = \text {sign}(\mathbf w^{\mathrm T} \mathbf x_j)</script> in our dichotomy, the value of <script type="math/tex">y_j</script> will always be:</p>

<p><script type="math/tex; mode=display"> y_j = \text {sign}(\mathbf w^{\mathrm T} \mathbf x_j) = +1 </script></p>

<p>However, in our dichotomy we defined <script type="math/tex">y_j</script> as being equal to <script type="math/tex">-1</script>:</p>

<p><script type="math/tex; mode=display"> y_j = \text {sign}(\mathbf w^{\mathrm T} \mathbf x_j) = -1 </script></p>

<p>Therefore we cannot appropriately represent our dichotomy.</p>

<p>Now we have proved that <script type="math/tex">\vc \leq d + 1</script> and <script type="math/tex">\vc \geq d + 1</script>. Therefore:</p>

<p><script type="math/tex; mode=display"> \vc = d = 1 </script></p>

<p>What is <script type="math/tex">d + 1</script> in the perceptron? It is the number of parameters in the perceptron model: <script type="math/tex">w_0, w_1, \dots, w_d</script>. In essence, this means that when we have a higher number of parameters, we will have a higher <script type="math/tex">\vc</script>.</p>
<h2 id="interpretation-of-vc-dimension">
<span class="hash">#</span>
<a href="#interpretation-of-vc-dimension" class="header-link">Interpretation of VC Dimension</a>
</h2>
<p>The parameters in the weight vector correspond to degrees of freedom that allow us to create a specific hypothesis. The number of parameters correspond to <em>analog</em> degrees of freedom: varying any single parameter---which is itself continuous---yields an entirely new perceptron. The VC dimension translated these into <em>binary</em> degrees of freedom, since we&#39;re only trying to get a different dichotomy.</p>

<p>This is important because it allows us to ascertain how expressive a model may be; how many different outputs we can actually get.</p>

<p>There is a distinction between parameters and degrees of freedom. This is because parameters may not contribute degrees of freedom.</p>

<p>For example, consider a 1D perceptron. This consists of a weight parameter and a threshold parameter, resulting in two degrees of freedom. This results in <script type="math/tex">\vc = d + 1 = 2</script>.</p>

<p>Now consider the situation where the output of this model is fed as input into another perceptron, which is fed to another perceptron and so on, for a total of four linked perceptrons. This corresponds to <script type="math/tex">8</script> parameters since each perceptron contains <script type="math/tex">2</script>. <strong>However</strong>, there are still only <script type="math/tex">2</script> degrees of freedom because every perceptron after the first simply returns the input; they are redundant.</p>

<p>For this reason, we can think of <script type="math/tex">\vc</script> as measuring the <em>effective</em> number of parameters.</p>
<h2 id="minimum-sample-size">
<span class="hash">#</span>
<a href="#minimum-sample-size" class="header-link">Minimum Sample Size</a>
</h2>
<p>Knowing all this, we would like to find a way to determine the number of training points needed to achieve a certain level of performance. The first realization is that merely having a finite VC dimension means it&#39;s even possible to learn. We should also remember that the VC inequality has two quantities that we would like to minimize: <script type="math/tex">\epsilon</script> and <script type="math/tex">\delta</script>:</p>

<p><script type="math/tex; mode=display"> P(|\insample(g) - \outsample(g)| > \epsilon) \leq \underbrace {4 \growthfunc(2N) e^{- \frac 1 8 \epsilon^2 N}}_{\delta} </script></p>

<p>What we would like to do is say that we want a particular <script type="math/tex">\epsilon</script> and <script type="math/tex">\delta</script>. For example, we would like to be at most 10% away from <script type="math/tex">\outsample</script> (<script type="math/tex">\epsilon = 10\% = 0.1</script>) and we want that statement to be correct at least 95% of the time (<script type="math/tex">\delta = 5\% = 0.05</script>). How many examples do we need to satisfy these constraints?</p>

<p>The question is, how does <script type="math/tex">N</script> depend on <script type="math/tex">\vc</script>? Consider a simplification of the RHS:</p>

<p><script type="math/tex; mode=display"> N^d e^{-N} </script></p>

<p>An observation made from plotting the above for increasing values of <script type="math/tex">d</script> with <script type="math/tex">N</script> vs the logarithm of the probability shows that <script type="math/tex">N</script> is proportional to <script type="math/tex">\vc</script>. A practical observation made by machine learning academics and practitioners is that the actual quantity we are trying to bound follows the same monotonicity as the actual bound, e.g. a bigger VC dimension yields bigger quantities, if not close to proportional. The higher the VC dimension, the more examples that are required.</p>

<p>The <em>rule of thumb</em> is that for a large range of reasonable <script type="math/tex">\epsilon</script> and <script type="math/tex">\delta</script>, and for a large range of practical applications, you need at least 10 times the VC dimension:</p>

<p><script type="math/tex; mode=display"> N \geq 10\ \vc </script></p>
<h2 id="generalization-bound">
<span class="hash">#</span>
<a href="#generalization-bound" class="header-link">Generalization Bound</a>
</h2>
<p>We will now simplify the VC inequality. We&#39;ll begin by denoting the RHS as <script type="math/tex">\delta</script>:</p>

<p><script type="math/tex; mode=display"> P(|\insample(g) - \outsample(g)| > \epsilon) \leq \underbrace {4 \growthfunc(2N) e^{- \frac 1 8 \epsilon^2 N}}_{\delta} </script></p>

<p>We want to get <script type="math/tex">\epsilon</script> in terms of <script type="math/tex">\delta</script>, to allow us to state the reliability <script type="math/tex">\delta</script> of the statement we would like to achieve, and have the equation output the tolerance <script type="math/tex">\epsilon</script> that the statement can guarantee with that reliability constraint.</p>

<p><script type="math/tex; mode=display">
\delta = 4 \growthfunc(2N) e^{- \frac 1 8 \epsilon^2 N}\ \Longrightarrow\ \epsilon = \underbrace {\sqrt {\frac 8 N \ln {\frac {4 \growthfunc(2N)} \delta}}}_{\Omega}
</script></p>

<p>This means that the probability that <script type="math/tex">\insample</script> tracks <script type="math/tex">\outsample</script> within <script type="math/tex">\Omega</script> is at least <script type="math/tex">1 - \delta</script>:</p>

<p><script type="math/tex; mode=display"> P(|\outsample - \insample| \leq \Omega(N, \mathcal H, \delta)) \geq 1 - \delta </script></p>

<p>The absolute value can be removed because <script type="math/tex">\insample</script> is usually much smaller than <script type="math/tex">\outsample</script>, since <script type="math/tex">\insample</script> is the value we minimize deliberately. The difference between <script type="math/tex">\outsample</script> and <script type="math/tex">\insample</script> is known as the <em>generalization error</em>:</p>

<p><script type="math/tex; mode=display"> P(\outsample - \insample \leq \Omega) \geq 1 - \delta </script></p>

<p>This can then be rearranged and simplified further into the <em>generalization bound</em>:</p>

<p><script type="math/tex; mode=display"> P(\outsample \leq \insample + \Omega) \geq 1 - \delta </script></p>

<p>The effect of the generalization bound is that it bounds the unknown value <script type="math/tex">\outsample</script> by values we do know, namely <script type="math/tex">\insample</script> and <script type="math/tex">\Omega</script>.</p>
<h1 id="bias-variance-tradeoff">
<span class="hash">#</span>
<a href="#bias-variance-tradeoff" class="header-link">Bias-Variance Tradeoff</a>
</h1>
<p>We have noticed that there is a trade off between approximation and generalization. In other words, we want to minimize <script type="math/tex">\outsample</script>, such that we attain a good approximation of <script type="math/tex">f</script> out of sample. The more complex the hypothesis set <script type="math/tex">\mathcal H</script>, the better chance we have of <em>approximating</em> <script type="math/tex">f</script>. However, the less complex of a hypothesis set <script type="math/tex">\mathcal H</script>, the better the chance of <em>generalizing</em> out of sample.</p>

<p><script type="math/tex; mode=display">
\begin{align}
\text {more complex } \mathcal H\ &\Longrightarrow\ \text {better chance of } \textbf {approximating }  f \\
\text {less complex } \mathcal H\ &\Longrightarrow\ \text {better chance of } \textbf {generalizing } \text {out of sample}
\end{align}
</script></p>

<p>We have already learned that VC analysis is one approach of decomposing <script type="math/tex">\outsample</script>:</p>

<p><script type="math/tex; mode=display"> \outsample \leq \insample + \Omega </script></p>

<p>Bias-Variance analysis is another approach to decomposing <script type="math/tex">\outsample</script>, which does so into two components:</p>

<ul>
<li>how well <script type="math/tex">\mathcal H</script> can approximate <script type="math/tex">f</script></li>
<li>how well we can zoom in on a good <script type="math/tex">h \in \mathcal H</script></li>
</ul>

<p>Bias-Variance applies to <em>real-valued targets</em> and uses <em>squared error</em>.</p>

<p><script type="math/tex; mode=display"> \outsample(g^{(\mathcal D)}) = \mathbb E_{\mathrm x} \left[ (g^{(\mathcal D)}(\mathbf x) - f(\mathbf x))^2 \right] </script></p>

<p><script type="math/tex">g^{(\mathcal D)}</script> refers to the fact that the hypothesis comes from the dataset <script type="math/tex">\mathcal D</script>.</p>

<p>Given a budget of <script type="math/tex">N</script> training examples, we want to generalize the above for any data set of size <script type="math/tex">N</script>:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\mathbb E_{\mathcal D} \left[ \outsample(g^{(\mathcal D)}) \right] &= \mathbb E_{\mathcal D} \left[ \mathbb E_{\mathrm x} \left[ (g^{(\mathcal D)}(\mathbf x) - f(\mathbf x))^2 \right] \right] \\
&= \mathbb E_{\mathrm x} \left[ \mathbb E_{\mathcal D} \left[ (g^{(\mathcal D)}(\mathbf x) - f(\mathbf x))^2 \right] \right]
\end{align}
</script></p>

<p>We&#39;re only going to focus on the inner component of the RHS. We&#39;re going to consider the concept of an average hypothesis <script type="math/tex">\def \avghypo {\bar g} \avghypo</script> so that:</p>

<p><script type="math/tex; mode=display"> \avghypo(\mathbf x) = \mathbb E_{\mathcal D} \left[ g^{(\mathcal D)}(\mathbf x) \right] </script></p>

<p>The average hypothesis for a particular point <script type="math/tex">\mathbf x</script>, <script type="math/tex">\avghypo(\mathbf x)</script>, is equivalent to finding the hypothesis found from many different data sets, then averaging the result of each of those hypothesis on the point <script type="math/tex">\mathbf x</script>:</p>

<p><script type="math/tex; mode=display">
\avghypo(\mathbf x) \approx \frac 1 K \sum_{k = 1}^K g^{(\mathcal D_k)}(\mathbf x)
</script></p>
<h2 id="bias-variance-representation">
<span class="hash">#</span>
<a href="#bias-variance-representation" class="header-link">Bias-Variance Representation</a>
</h2>
<p>We will now decompose the expected error into two components, bias and variance:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\mathbb E_{\mathcal D} \Big[ (g^{(\mathcal D)}(\mathbf x) - f(\mathbf x))^2 \Big]
&= \mathbb E_{\mathcal D} \left[ (g^{(\mathcal D)}(\mathbf x) - \bar g(\mathbf x) + \bar g(\mathbf x) - f(\mathbf x))^2 \right] \\
&= \mathbb E_{\mathcal D} \Big[ (g^{(\mathcal D)}(\mathbf x) - \bar g(\mathbf x))^2 + (\bar g(\mathbf x) - f(\mathbf x))^2 \\
&\phantom {= \mathbb E_{\mathcal D}} + 2\ (g^{(\mathcal D)}(\mathbf x) - \bar g(\mathbf x))\ (\bar g(\mathbf x) - f(\mathbf x)) \Big] \\
&= \mathbb E_{\mathcal D} \left[ (g^{(\mathcal D)}(\mathbf x) - \bar g(\mathbf x))^2 \right] + (\bar g(\mathbf x) - f(\mathbf x))^2
\end{align}
</script></p>

<p>The result is an equation that says that the expected error <script type="math/tex">\mathbb E_{\mathcal D}</script> for a particular data set <script type="math/tex">\mathcal D</script> given the hypothesis <script type="math/tex">g^{(\mathcal D)}(\mathbf x)</script> resulting from that given data set is measured against the actual target function <script type="math/tex">f(\mathbf x)</script>, and that error measure <strong>is equivalent</strong> to the <em>variance</em>---the expected error of the hypothesis <script type="math/tex">g^{(\mathcal D)}(\mathbf x)</script> measured against the average hypothesis <script type="math/tex">\bar g(\mathbf x)</script>, <strong>plus</strong> the <em>bias</em>---the error measure of the average hypothesis <script type="math/tex">\bar g(\mathbf x)</script> against the target function <script type="math/tex">f(\mathbf x)</script>:</p>

<p><script type="math/tex; mode=display">
\def \bias {\textbf {bias} (\mathbf x)}
\def \var {\textbf {var} (\mathbf x)}
\mathbb E_{\mathcal D} \Big[ (g^{(\mathcal D)}(\mathbf x) - f(\mathbf x))^2 \Big] = \underbrace {\mathbb E_{\mathcal D} \left[ (g^{(\mathcal D)}(\mathbf x) - \bar g(\mathbf x))^2 \right]}_{\var} + \underbrace {\vphantom {\Big[} (\bar g(\mathbf x) - f(\mathbf x))^2}_{\bias}
</script></p>

<p>Both the bias and the variance in the above equation are as measured from a particular point <script type="math/tex">\mathbf x</script>. The bias essentially represents the bias of the hypothesis set <script type="math/tex">\mathcal H</script> away from the target function. The variance is essentially measuring the effect of the finite data set, where each finite data set will vary in its result. Therefore:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\mathbb E_{\mathcal D} \left[ \outsample(g^{(\mathcal D)}) \right] &= \mathbb E_{\mathrm x} \left[ \mathbb E_{\mathcal D} \left[ (g^{(\mathcal D)}(\mathbf x) - f(\mathbf x))^2 \right] \right] \\
&= \mathbb E_{\mathrm x} \Big[ \bias + \var \Big] \\
&= \textbf {bias} + \textbf {var}
\end{align}
</script></p>
<h2 id="tradeoff">
<span class="hash">#</span>
<a href="#tradeoff" class="header-link">Tradeoff</a>
</h2>
<p>There is a trade off between the bias and the variance:</p>

<p><script type="math/tex; mode=display">
\textbf {bias} = \mathbb E_{\mathrm x} \Big[ (\bar g(\mathbf x) - f(\mathbf x))^2 \Big] \qquad \textbf {var} = \mathbb E_{\mathrm x} \Big[ \mathbb E_{\mathcal D} \Big[ (g^{(\mathcal D)}(\mathbf x) - \bar g(\mathbf x))^2 \Big] \Big]
</script></p>

<p>If we go from a small hypothesis to a bigger one, the bias goes down but the variance goes up:</p>

<p><script type="math/tex; mode=display"> \mathcal H \uparrow\colon \qquad \textbf {bias} \downarrow \qquad \textbf {variance} \uparrow </script></p>

<p>We can now formulate the relations between the hypothesis for a given dataset <script type="math/tex">\mathcal D</script> known as <script type="math/tex">g^{(\mathcal D)}(\mathbf x)</script>, the average hypothesis <script type="math/tex">\bar g(\mathbf x)</script>, and the target function <script type="math/tex">f(\mathbf x)</script>:</p>

<p><script type="math/tex; mode=display">
g^{(\mathcal D)}(\mathbf x)\ \xrightarrow[\text {variance}]{}\ \bar g(\mathbf x)\ \xrightarrow[\text {bias}]{}\ f(\mathbf x)
</script></p>

<p>We will now use an example to prove that this is true. Given a target function that is a sinusoid:</p>

<p><script type="math/tex; mode=display"> f\colon [-1, 1] \to \mathbb R \qquad f(x) = \sin(\pi x) </script></p>

<p>We are only given two examples <script type="math/tex">N = 2</script>. There are two hypothesis sets, one is a constant model and the other one is linear, and we would like to see which one is better:</p>

<p><script type="math/tex; mode=display">
\begin{align}
&\mathcal H_0 \colon \quad h(x) = b \\
&\mathcal H_1 \colon \quad h(x) = ax + b
\end{align}
</script></p>

<p>We will now see which one fares better by approximating what we think would be the best hypothesis from each set. In the case of the constant model, we would choose the line at <script type="math/tex">y = 0</script> to minimize the MSE. The linear model would use a line that also tries to minimize the MSE. The following is the <script type="math/tex">\outsample</script> for both:</p>

<div style="text-align: center; margin-top: 10px">
  <img src="/images/notes/machine-learning/bias-variance-tradeoff/constant-model.png">
  <img src="/images/notes/machine-learning/bias-variance-tradeoff/linear-model.png">
</div>

<p>It&#39;s clear that from approximation, <script type="math/tex">\mathcal H_1</script> seems to be better. We will now see which one fares better through machine learning. For example, for these particular two points we get the following result:</p>

<div style="text-align: center; margin-top: 10px">
  <img src="/images/notes/machine-learning/bias-variance-tradeoff/learning-constant-model.png">
  <img src="/images/notes/machine-learning/bias-variance-tradeoff/learning-linear-model.png">
</div>

<p>The problem is that these results depend on the two points that we were given, so it complicates the task of comparing the two hypothesis sets. This is why we need bias-variance analysis, it gives us the expected error <em>with respect to</em> the choice of the data set.</p>

<p>If we were derive a hypothesis from any two points, for a large number of different two points, we would come up with something like the left image, where every line represents a derived hypothesis. It therefore stands to reason that the average hypothesis would fall somewhere near <script type="math/tex">y = 0</script>---the midpoint of the range of possible hypotheses. The error measure of the average hypothesis against the target function is the <em>bias</em>, and the <em>variance</em> is represented by the gray region which corresponds to the standard deviation of the possible hypotheses. It&#39;s apparent that this model has a <em>high bias</em> (<script type="math/tex">0.5</script>) and a <em>low variance</em> (<script type="math/tex">0.25</script>):</p>

<div style="text-align: center; margin-top: 10px">
  <img src="/images/notes/machine-learning/bias-variance-tradeoff/multiple-hypotheses.png">
  <img src="/images/notes/machine-learning/bias-variance-tradeoff/average-hypothesis.png">
</div>

<p>The same is slightly more complicated with the second hypothesis <script type="math/tex">\mathcal H_1</script> because of its linear model, which yields very different hypotheses, that is, <em>high variance</em> (<script type="math/tex">1.69</script>). There is <em>low bias</em> (<script type="math/tex">0.21</script>) however, because it has many different hypotheses to average from:</p>

<div style="text-align: center; margin-top: 10px">
  <img src="/images/notes/machine-learning/bias-variance-tradeoff/linear-multiple-hypotheses.png">
  <img src="/images/notes/machine-learning/bias-variance-tradeoff/linear-average-hypothesis.png">
</div>

<p>It&#39;s clear that when the two components are summed for both models, the expected error of the first hypothesis set <script type="math/tex">\mathcal H_0</script> is much lower than <script type="math/tex">\mathcal H_1</script>&#39;s.</p>

<p>The <strong>conclusion</strong> from this example is that we are matching the <em>model complexity</em> to the <em>data resources</em>, not to the <em>target complexity</em>.</p>
<h2 id="learning-curves">
<span class="hash">#</span>
<a href="#learning-curves" class="header-link">Learning Curves</a>
</h2>
<p>A learning curve plots the expected value of <script type="math/tex">\outsample</script> and <script type="math/tex">\insample</script> as a function of <script type="math/tex">N</script>. For a data set of size <script type="math/tex">N</script>, how does the expected <script type="math/tex">\outsample</script> and expected <script type="math/tex">\insample</script> vary with <script type="math/tex">N</script>?</p>

<p>The following images are learning curves for a simple and complex model. The simple model shows that <script type="math/tex">\outsample</script> decreases with the <script type="math/tex">N</script>, but so does <script type="math/tex">\insample</script>, which can be attributed to exceeding the degrees of freedom available in the hypothesis set. The second model has so many degrees of freedom that it can fit the training set perfectly until the part where the blue curve appears on the left side, however, <script type="math/tex">\outsample</script> is very high before that point, which corresponds to not learning anything; just memorizing the examples. Therefore if there are very few examples, then it&#39;s clear that the simple model would fare better. This is why we want to match the model&#39;s complexity to the data resources that we have.</p>

<div style="text-align: center; margin-top: 10px">
  <img src="/images/notes/machine-learning/bias-variance-tradeoff/simple-learning-curve.png">
  <img src="/images/notes/machine-learning/bias-variance-tradeoff/complex-learning-curve.png">
</div>

<p>The following is a comparison of the learning curves for a given model using VC analysis and bias-variance analysis. In the VC analysis curve on the left, the blue region is <script type="math/tex">\insample</script> and the red region is <script type="math/tex">\Omega</script>---what happens within the generalization bound. In the Bias-Variance curve, the black bar is the approximation. Everything below the approximation is the bias, so everything else under the <script type="math/tex">\outsample</script> curve must be the variance. Both curves are talking about approximations. The Bias-Variance curve is concerning over-all approximation, whereas the VC analysis curve is concerning in-sample approximation.</p>

<div style="text-align: center; margin-top: 10px">
  <img src="/images/notes/machine-learning/bias-variance-tradeoff/vc-learning-curve.png">
  <img src="/images/notes/machine-learning/bias-variance-tradeoff/bias-variance-learning-curve.png">
</div>
<h2 id="analysis-of-linear-regression">
<span class="hash">#</span>
<a href="#analysis-of-linear-regression" class="header-link">Analysis of Linear Regression</a>
</h2>
<p>Given a data set <script type="math/tex">\mathcal D = \{ (\mathbf x_1, y_1), \dots, (\mathbf x_n, y_n)\}</script> and a noisy target function:</p>

<p><script type="math/tex; mode=display">y = \mathbf w^{* \mathrm T} \mathbf x + \text {noise}</script></p>

<p>The linear regression solution is:</p>

<p><script type="math/tex; mode=display"> \mathbf w = \left( \mathrm {X}^{\mathrm {T}} \mathrm {X} \right)^{-1} \mathrm {X}^{\mathrm {T}} \mathbf y </script></p>

<p>The in-sample error vector would be:</p>

<p><script type="math/tex; mode=display"> \mathrm X \mathbf w - \mathbf y </script></p>

<p>The &#39;out-of-sample&#39; error vector would be calculated by generating a new set of points from the same inputs but with different noise and would therefore be:</p>

<p><script type="math/tex; mode=display"> \mathrm X \mathbf w - \mathbf y' </script></p>

<p>This yields the following analysis learning curve. Up until <script type="math/tex">d + 1</script> data points, the data was being fit perfectly. The best approximation error, the variance of the noise, is denoted by <script type="math/tex">\sigma^2</script>.</p>

<p><img src="/images/notes/machine-learning/bias-variance-tradeoff/linear-regression-learning-curve.png" class="center"></p>

<p>We can observe the following characteristics:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\text {best approximation error} &= \sigma^2 \vphantom {\left(1 - \frac {d + 1} N \right)} \\
\text {expected in-sample error} &= \sigma^2 \left(1 - \frac {d + 1} N \right) \\
\text {expected out-of-sample error} &= \sigma^2 \left(1 + \frac {d + 1} N\right) \\
\text {expected generalization error} &= 2 \sigma^2 \left(\frac {d + 1} N\right)
\end{align}
</script></p>
<h1 id="linear-model-ii">
<span class="hash">#</span>
<a href="#linear-model-ii" class="header-link">Linear Model II</a>
</h1><h2 id="non-linear-transformations-ii">
<span class="hash">#</span>
<a href="#non-linear-transformations-ii" class="header-link">Non-Linear Transformations II</a>
</h2>
<p>To recap, a transformation <script type="math/tex">\Phi</script> transforms the input vector <script type="math/tex">\mathbf x</script> to the feature space, resulting in a feature vector <script type="math/tex">\mathbf z</script>, where every element <script type="math/tex">z_i</script> is the result of performing a non-linear transformation <script type="math/tex">\phi_i</script> on the <em>entire</em> input vector. Therefore, <script type="math/tex">\mathbf z</script> can be of different size, often longer, than the input vector <script type="math/tex">\mathbf x</script>:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\mathrm {x} = (x_0, x_1, \dots, x_d)\ \xrightarrow{\Phi}\ &\mathrm {z} = (z_0, z_1, \dots \dots, z_{\tilde d}) \\
&\text {each } z_i = \phi_i(\mathbf x) \\
&\mathbf z = \Phi(\mathbf x) \\
\text {e.g. } &\mathbf z = (1, x_1, x_2, x_1 x_2, x_1^2, x_2^2)
\end{align}
</script></p>

<p>It&#39;s important to remember that the final hypothesis <script type="math/tex">g(\mathbf x)</script> will always be in the <script type="math/tex">\mathcal X</script>-space; the <script type="math/tex">\mathcal Z</script>-space is transparent to the user.</p>

<p><script type="math/tex; mode=display"> g(\mathbf x) = \text {sign}(\tilde w^{\mathrm T} \Phi(\mathbf x)) \qquad \text {or} \qquad \tilde w^{\mathrm T} \Phi(\mathbf x) </script></p>

<p>What is the price paid in using a non-linear transformation? Well it has been established that the VC dimension of a <script type="math/tex">d + 1</script> sized input vector is <script type="math/tex">d + 1</script>, therefore the feature vector <script type="math/tex">\mathbf z</script>, whose size is <script type="math/tex">\tilde d + 1</script>, will have a VC dimension of at most <script type="math/tex">\tilde d + 1</script>, where <script type="math/tex">\tilde d</script> is generally larger than <script type="math/tex">d</script>. It&#39;s &quot;at most&quot; because the VC dimension is always measured in the <script type="math/tex">\mathcal X</script>-space, and this is the <script type="math/tex">\mathcal Z</script>-space. While this means that we will be able to better fit the data, we won&#39;t have much of a chance of generalization.</p>

<p>For example considering the following <strong>case 1</strong> where there are two outliers in the data set. If we want to really fit the data, we could use a 4th-order surface in order to completely classify the data in the set. However, this increase in complexity would mean that it&#39;d be very difficult to generalize. Sometimes it&#39;s best to accept that there will be an <script type="math/tex">\insample > 0</script>.</p>

<div style="text-align: center; margin-top: 10px">
  <img src="/images/notes/machine-learning/linear-model-ii/non-linear-case-1.png">
  <img src="/images/notes/machine-learning/linear-model-ii/non-linear-case-1-transformed.png">
</div>

<p>Now consider <strong>case 2</strong>, where we don&#39;t stand a chance using a linear model to fit the data, which clearly falls inside a circular region:</p>

<p><img src="/images/notes/machine-learning/linear-model-ii/non-linear-case-2.png" class="center"></p>

<p>In this case, we can use a non-linear transformation to map the data to a general 2nd-order surface as follows:</p>

<p><script type="math/tex; mode=display"> \mathbf x = (1, x_1, x_2)\ \xrightarrow{\Phi}\ \mathbf z = (1, x_1, x_2, x_1 x_2, x_1^2, x_2^2) </script></p>

<p>Note that with <script type="math/tex">\mathbf x</script> we were only using three weights, however in <script type="math/tex">\mathbf z</script> we are using six weights. This effectively translates to now requiring twice as many examples to achieve the same level of performance. So a natural conclusion is to try to find a way to avoid paying this increase in cost.</p>

<p>First let&#39;s consider the following alternative model, since it seems we only need <script type="math/tex">x_1^2</script> and <script type="math/tex">x_2^2</script> to represent the circle. Now we only have three weights again, the same as with the linear model represented by <script type="math/tex">\mathbf x</script>:</p>

<p><script type="math/tex; mode=display"> \mathbf z = (1, x_1^2, x_2^2) </script></p>

<p>We reduce the cost further by adopting the following model, which does away with representing the independence of <script type="math/tex">x_1^2</script> and <script type="math/tex">x_2^2</script>, as they simply represent the radius:</p>

<p><script type="math/tex; mode=display"> \mathbf z = (1, x_1^2 + x_2^2) </script></p>

<p>Now consider the extreme case, where we completely reduce to a single parameter, completely doing away with the threshold weight:</p>

<p><script type="math/tex; mode=display"> \mathbf z = (x_1^2 + x_2^2 - 0.6) </script></p>

<p>The problem with these reductions is that the guarantee of the VC inequality is forfeited if we look at the data. We can consider the concept of us simplifying the model for the machine as performing a hierarchical learning process, where we the human do some learning, and then pass the result for the machine to finish the learning process. From this perspective, it&#39;s apparent that the VC dimension resulting from this process is that of the entire hypothesis space that we as the human considered during the simplification process.</p>

<p>The bottom line is that <strong>looking at the data <em>before</em> choosing the model can be hazardous to your</strong> <script type="math/tex">\outsample</script>. The act of looking at the data before choosing the model is one of the many mistakes that fall under <em>data snooping</em>.</p>
<h2 id="logistic-regression">
<span class="hash">#</span>
<a href="#logistic-regression" class="header-link">Logistic Regression</a>
</h2>
<p>We&#39;re already familiar with two linear models. The first is <em>linear classification</em> which classifies data points based on what side of a hyperplane they&#39;re on. The second is <em>linear regression</em>, which makes it straightforward to predict a regression, and is computed by skipping the last step of linear classification, the part where it determines the sign of the result.</p>

<p>A third model called <em>logistic regression</em> can predict the probability of an event occurring, and is defined as:</p>

<p><script type="math/tex; mode=display"> h(\mathbf x) = \theta(s) </script></p>

<p>Where <script type="math/tex">s</script> is simply the dot product of the weight and feature vectors as is done in the other two linear models, and <script type="math/tex">\theta</script> is a non-linear function that is often called a <em>sigmoid</em>, and looks something like this:</p>

<p><img src="/images/notes/machine-learning/linear-model-ii/sigmoid.png" class="center"></p>

<p>In our particular case, the formula we&#39;re going to use is:</p>

<p><script type="math/tex; mode=display"> \theta(s) = \frac {e^s} {1 + e^s} </script></p>

<p>This kind of function is also called a &quot;soft threshold,&quot; since it&#39;s a gradual increase rather than a hard increase as in linear classification. It is in this way that this kind of function expresses uncertainty.</p>

<p>Given this functions&#39; bounds <script type="math/tex">(0, 1)</script> and the fact that it&#39;s a &quot;soft threshold,&quot; it&#39;s clear that it can be interpreted as a probability. For example, when predicting heart attacks, we want to predict whether there&#39;s a small or big risk in having a heart attack anytime soon. If we used a binary function as in linear classification, it would be too hard of a threshold and thus wouldn&#39;t really convey as much meaningful information, since it isn&#39;t clear exactly which things definitely cause heart attacks.</p>

<p>On the other hand, with a logistic regression model, we could predict the probability of having a heart attack in the next 12 months. In this context, the dot product between the weight and feature vector can be considered a &quot;risk score,&quot; which can give one a general idea of the result but which must still be passed through the logistic function in order to determine the probability.</p>

<p>A key point is that this probability is considered as a genuine probability. That is, not only does the logistic function have bounds of <script type="math/tex">(0, 1)</script>, but that the examples also have a probabilistic interpretation. For example, the input data consists of feature vectors and binary results. It&#39;s clear that there is some sort of probability embedded in yielding the binary results, but it is unknown to us. Therefore it can be said that the binary result is generated by a noisy target:</p>

<p><script type="math/tex; mode=display">
P(y \mid \mathbf x) = \begin{cases}
                        f(\mathbf x) & \text {for } y = +1; \\ \\
                        1 - f(\mathbf x) & \text {for } y = -1.
                      \end{cases}
</script></p>

<p>So the target <script type="math/tex">f</script> is:</p>

<p><script type="math/tex; mode=display"> f\colon \mathbb R^d \to [0, 1] \text { is the probability} </script></p>

<p>And we want to learn <script type="math/tex">g</script>:</p>

<p><script type="math/tex; mode=display"> g(\mathbf x) = \theta(\mathbf w^{\mathrm T} \mathbf x) \approx f(\mathbf x) </script></p>
<h3 id="logistic-regression-error-measure">
<span class="hash">#</span>
<a href="#logistic-regression-error-measure" class="header-link">Logistic Regression Error Measure</a>
</h3>
<p>We have established that for each data point <script type="math/tex">(\mathbf x, y)</script>, <script type="math/tex">y</script> is generated by the probability <script type="math/tex">f(\mathbf x)</script>. The plausible error measure is based on <em>likelihood</em>, that is, we are going to grade different hypotheses according to the likelihood that they are actually the target that generated the data. In other words, we are going to assume that a given hypothesis is indeed the target function, and then we will determine how likely it is to get a given result from its corresponding feature vector. Expressed mathematically:</p>

<p><script type="math/tex; mode=display">
P(y \mid \mathbf x) = \begin{cases}
                        h(\mathbf x) & \text {for } y = +1 \\ \\
                        1 - h(\mathbf x) & \text {for } y = -1
                      \end{cases}
</script></p>

<p>Now substitute <script type="math/tex">h(\mathbf x)</script> with <script type="math/tex">\theta(\weightT \feature)</script>, while noting that <script type="math/tex">\theta(-s) = 1 - \theta(s)</script>, since:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\theta(-s) &= \frac {e^{-s}} {1 + e^{-s}} \\
&= \frac 1 {e^s} * \frac 1 {1 + e^{-s}} \\
&= \frac 1 {e^s + 1} \\
1 - \theta(s) &= 1 - \frac {e^s} {1 + e^s} \\
&= \frac {1 + {e^s}} {1 + {e^s}} - \frac {e^s} {1 + e^s} \\
&= \frac 1 {1 + e^s}
\end{align}
</script></p>

<p>So now we can simplify the probability to:</p>

<p><script type="math/tex; mode=display"> P(y \mid \feature) = \theta(y \weightT \feature) </script></p>

<p>Now we can determine the likelihood of the entire data set <script type="math/tex">\mathcal D = (\feature_1, y_1), \dots, (\feature_N, y_N)</script> is:</p>

<p><script type="math/tex; mode=display">
\prod_{n = 1}^N P(y_n \mid \feature_n) =
\prod_{n = 1}^N \theta(y_n \weightT \feature_n)
</script></p>

<p>It&#39;s noteworthy to observe that the same weight vector is being used for each of those products, so that if it&#39;s varied to better fit one particular data point, it might no longer fit another. Therefore, whatever maximizes this product would represent the likelihood that the weight vector is representing the underlying probability distribution.</p>

<p>So now we want to see how to maximize the likelihood with respect to <script type="math/tex">\weight</script>. This corresponds to minimizing the error:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\def \crossentropy {\mathbf e}
&\phantom {=} \frac 1 N \ln \left( \prod_{n = 1}^N \theta(y_n \weightT \feature_n) \right) \\
&= \frac 1 N \sum_{n = 1}^N \ln \left( \frac 1 {\theta(y_n \weightT \feature_n)} \right), \qquad \theta(s) = \frac {e^{-s} \left(e^s\right)} {e^{-s} \left(e^s + 1\right)} = \frac 1 {1 + e^{-s}} \\
\insample(\weight) &= \frac 1 N \sum_{n = 1}^N \underbrace {\ln \left( 1 + e^{-y_n \weightT \feature_n} \right)}_{\crossentropy(h(\feature_n), y_n) \rlap {\text {, the cross-entropy error}}}
\end{align}
</script></p>
<h3 id="logistic-regression-learning-algorithm">
<span class="hash">#</span>
<a href="#logistic-regression-learning-algorithm" class="header-link">Logistic Regression Learning Algorithm</a>
</h3>
<p>Now that we have the learning model and error measure we can define the learning algorithm. Compared to the closed-form solution from the other two models, we will have to use an iterative solution called <em>gradient descent</em>, which is a general method for non-linear optimization.</p>

<p>Gradient descent starts at a point on the error function and iteratively takes steps along the steepest slope towards the minimum. So we&#39;ll have a direction unit vector <script type="math/tex">\hat v</script> pointed in the direction of the steepest slope and a fixed step size <script type="math/tex">\eta</script> which we will define as being small since we want to approximate the surface using the first order expansion of the Taylor series---the linear approximation---which works best when the distance between the two points is small. With this in mind, the new position after a step can be expressed as:</p>

<p><script type="math/tex; mode=display"> \weight(1) = \weight(0) + \eta \hat v </script></p>

<p>To derive the direction unit vector <script type="math/tex">\hat v</script>, we first observe that the change in the value of the error is simply the difference between the error at the new point and the error at the original point:</p>

<p><script type="math/tex; mode=display">
\Delta \insample = \insample(\mathbf w(0) + \eta \hat v) - \insample(\mathbf w(0))
</script></p>

<p>Remembering that the Taylor series is defined as:</p>

<p><script type="math/tex; mode=display"> \sum_{n = 0}^\infty \frac {f^{(n)}(a)} {n!} (x - a)^n </script></p>

<p>Then the first-order approximation is defined as <script type="math/tex">n = 1</script> so that the series is:</p>

<p><script type="math/tex; mode=display"> f(x) = f(a) + \frac {f'(a)} {1!} (x - a) + R_2 </script></p>

<p>If we drop the remainder term <script type="math/tex">R_2</script>, then we are left with the linear approximation:</p>

<p><script type="math/tex; mode=display"> f(x) \approx f(a) + f'(a)(x - a) </script></p>

<p>Therefore, for <script type="math/tex">f(x + a)</script> it is defined as:</p>

<p><script type="math/tex; mode=display">
\begin{align}
f(a + x) &\approx f(a) + f'(a)x \\
f'(a)x   &\approx f(a + x) - f(a)
\end{align}
</script></p>

<p>This representation of the linear approximation is similar to our definition of the change in the value of the error between the two positions:</p>

<p><script type="math/tex; mode=display">
\Delta \insample = \insample(\mathbf w(0) + \eta \hat v) - \insample(\mathbf w(0))
</script></p>

<p>However, we don&#39;t want to use the linear approximation just yet. There&#39;s another concept known as the <a href="http://en.wikipedia.org/wiki/Gradient">gradient</a> which basically defines a vector field so that at any given point in the space, a vector is available at that point which points in the direction of the &quot;steepest ascent.&quot; The gradient is simply defined as the vector of all of the possible partial derivatives. For example, given a function:</p>

<p><script type="math/tex; mode=display"> f(x, y, z) = 2x + 3y^2 - \sin(z) </script></p>

<p>The gradient is denoted by the <script type="math/tex">\nabla</script> symbol and is defined as:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\nabla f &= \frac {\partial f} {\partial x} \hat i + \frac {\partial f} {\partial y} \hat j + \frac {\partial f} {\partial z} \hat k \\
&= 2 \hat i + 6y \hat j - \cos(z) \hat k \\
&= \begin{bmatrix} 2 & 6y & -\cos(z) \end{bmatrix}
\end{align}
</script></p>

<p>It&#39;s obvious how the gradient can be useful here. It provides us with a way to determine---at any point on the surface---in what direction to move to go deeper towards the minimum. This is possible by finding the gradient vector---which points in the direction of the &quot;steepest ascent&quot;---and negate it so that it then points in the direction of the &quot;steepest descent&quot;. This is clearly useful, and fortunately there is a similar linear approximation to a vector-taking function <a href="http://en.wikipedia.org/wiki/Gradient#Linear_approximation_to_a_function">for a gradient</a>:</p>

<p><script type="math/tex; mode=display"> f(\mathbf x) \approx f(\mathbf x_0) + \nabla f(\mathbf x_0) (\mathbf x - \mathbf x_0) </script></p>

<p>This means that for <script type="math/tex">f(\mathbf x + \mathbf x_0)</script>:</p>

<p><script type="math/tex; mode=display">
\begin{align}
f(\mathbf x + \mathbf x_0) &\approx f(\mathbf x_0) + \nabla f(\mathbf x_0) \cdot (\mathbf x - \mathbf x_0) \\
\nabla f(\mathbf x_0) \cdot \mathbf x &\approx f(\mathbf x + \mathbf x_0) - f(\mathbf x_0) \\
\nabla f(\mathbf x_0)^{\mathrm T} \mathbf x &\approx f(\mathbf x + \mathbf x_0) - f(\mathbf x_0) \\
\end{align}
</script></p>

<p>So we can replace the RHS with the dot product of the gradient and the unit vector <script type="math/tex">\hat v</script> <sup id="fnref1"><a href="#fn1" rel="footnote">1</a></sup>:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\Delta \insample &= \insample(\mathbf w(0) + \eta \hat v) - \insample(\mathbf w(0)) \\
&= \eta \nabla \insample(\mathbf w(0))^{\mathrm T} \hat v
\end{align}
</script></p>

<p>Since we know that <script type="math/tex">\hat v</script> is a unit vector, we know that regardless of the value of <script type="math/tex">\hat v</script>, the inner product between <script type="math/tex">\nabla \insample(\mathbf w(0))</script> and <script type="math/tex">\hat v</script> cannot exceed the norm <script type="math/tex">\lVert \nabla \insample(\mathbf w(0)) \rVert</script>, be it in the positive or negative direction. Therefore we can guarantee that <script type="math/tex">\Delta \insample</script> must be greater than or equal to the negative norm:</p>

<p><script type="math/tex; mode=display"> \Delta \insample \geq -\eta \lVert \nabla \insample(\mathbf w(0)) \rVert </script></p>

<p>We would therefore like to choose <script type="math/tex">\hat v</script> that is closest to this lower bound, since we want to minimize the error, and a negative change in error corresponds to a big decrease in error, and therefore descending rapidly. Since <script type="math/tex">\hat v</script> is by definition a unit vector, we can simply take the gradient itself and divide it by its norm to normalize it, then negate it to flip from ascent to descent.</p>

<p><script type="math/tex; mode=display"> \hat v = - \eta \frac {\nabla \insample(\mathbf w(0))} {\lVert \nabla \insample(\mathbf w(0)) \rVert} </script></p>

<p>The above implies a fixed step. We could instead adopt a step distance based on the slope of the current position, easily accomplished by scaling the direction vector <script type="math/tex">\hat v</script> by the norm of the gradient at the current position. This allows the algorithm to take bigger steps in steeper locations, and take smaller steps once it begins to near the minimum. In this case, <script type="math/tex">\eta</script> now refers to the &quot;learning rate&quot;:</p>

<p><script type="math/tex; mode=display">
\Delta \mathbf w = - \eta \frac {\nabla \insample(\mathbf w(0))} {\lVert \nabla \insample(\mathbf w(0)) \rVert} \lVert \nabla \insample(\mathbf w(0)) \rVert = - \eta \nabla \insample(\mathbf w(0))
</script></p>

<p>Now that we have all of the pieces we can construct the learning algorithm:</p>

<ol>
<li>initialize the weights at <script type="math/tex">t = 0</script> to <script type="math/tex">\weight(0)</script></li>
<li><p>for <script type="math/tex">t = 0, 1, 2, \dots</script> do</p>

<ol>
<li><p>compute the gradient
<script type="math/tex; mode=display"> \nabla \insample = - \frac 1 N \sum_{n = 1}^N \frac {y_n \feature_n} {1 + e^{y_n \weightT(t) \feature_n}} </script></p></li>
<li><p>update the weights: <script type="math/tex">\weight(t + 1) = \weight(t) - \eta \nabla \insample</script></p></li>
<li><p>repeat until it is time to stop</p></li>
</ol></li>
<li><p>return the final weights <script type="math/tex">\weight</script></p></li>
</ol>

<p>To recap the three linear models we&#39;ve learned so far, here is a table for their possible uses within the context of credit analysis:</p>

<table>
<thead>
<tr>
<th style="text-align: left">task</th>
<th style="text-align: left">model</th>
<th style="text-align: left">error measure</th>
<th style="text-align: left">algorithm</th>
</tr>
</thead>

<tbody>
<tr>
<td style="text-align: left">approve or deny</td>
<td style="text-align: left">perceptron</td>
<td style="text-align: left">classification error</td>
<td style="text-align: left">PLA, Pocket, ...</td>
</tr>
<tr>
<td style="text-align: left">amount of credit</td>
<td style="text-align: left">linear regression</td>
<td style="text-align: left">squared error</td>
<td style="text-align: left">pseudo-inverse</td>
</tr>
<tr>
<td style="text-align: left">probability of default</td>
<td style="text-align: left">logistic regression</td>
<td style="text-align: left">cross-entropy error</td>
<td style="text-align: left">gradient descent</td>
</tr>
</tbody>
</table>
<h1 id="neural-networks">
<span class="hash">#</span>
<a href="#neural-networks" class="header-link">Neural Networks</a>
</h1><h2 id="stochastic-gradient-descent">
<span class="hash">#</span>
<a href="#stochastic-gradient-descent" class="header-link">Stochastic Gradient Descent</a>
</h2>
<p>Gradient descent minimizes <script type="math/tex">\insample</script> by iterative steps <script type="math/tex">\Delta \weight</script> along <script type="math/tex">- \nabla \insample</script>, and each one of those steps occurs only after a full <em>epoch</em>, where an epoch marks the event of having examined all of the examples. Instead of being restricted to taking steps only after having examined all of the examples, we would like to try to adapt it so that it can perform a step after having considered a single random example. This modified gradient descent is known as <em>stochastic gradient descent</em>, whereas comparatively the &quot;original&quot; gradient descent will be referred to henceforth as <em>batch gradient descent</em> because it performed a step <script type="math/tex">\Delta \weight</script> only after having examined all of the examples.</p>

<p>This is accomplished by performing epochs until a tolerance is met, where each epoch consists of examining every single example in a random order, and for every example considered, recomputing the gradient based on that example alone and descending along its negative direction. Compare this to batch gradient descent, which had to compute the gradient based on all of the points and summing up the result.</p>

<ol>
<li>until desired tolerance is met

<ol>
<li>(epoch) for every example considered in a random order

<ol>
<li>perform a descent along the negative of the gradient which was computed for that example</li>
</ol></li>
</ol></li>
</ol>

<p>Stochastic gradient descent is a randomized version of gradient descent, where we pick <strong>one</strong> example <script type="math/tex">(\feature_n, y_n)</script> at a time and apply gradient descent based on <script type="math/tex">\crossentropy(h(\feature_n), y_n)</script>, similar to PLA. It&#39;s expected that this is similar to performing the descent by considering every single point:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\mathbb E_n \left[ - \nabla \crossentropy(h(\feature_n), y_n) \right] &=
\frac 1 N \sum_{n = 1}^N - \nabla \crossentropy(h(\feature_n), y_n) \\
&= - \nabla \insample
\end{align}
</script></p>

<p>There are a variety of benefits of SGD. One is that it is a <strong>cheaper computation</strong>, because we can make a move, or descent, after having considered one point instead of every single point. Another advantage is <strong>randomization</strong> which can help in escaping very shallow local minima or certain flat regions that precede the actual minimum. Finally, SGD is a <strong>very simple</strong> optimization, so it is widely used and a variety of &quot;rules of thumb&quot; have been formulated. For example, one rule of thumb is to start with a learning rate of <script type="math/tex">\eta = 0.1</script> and scale it from there.</p>

<p>For example, think back to the movie ratings model which can learn to suggest movies to users based on their tastes. The user <script type="math/tex">i</script>&#39;s vector <script type="math/tex">\mathbf u</script> consists of tastes <script type="math/tex">u_1, u_2, \dots, u_k</script> in different movie qualities, the movie <script type="math/tex">j</script>&#39;s vector <script type="math/tex">\mathbf v</script> corresponds to how well a movie represents each of those qualities <script type="math/tex">v_1, v_2, \dots, v_k</script> in the feature vector, and the result <script type="math/tex">r_{ij}</script> is the movie rating.</p>

<p>We can take the dot product between the user and movie vector as a measure of how well both agree with each other, in fact, this will correspond to the rating. Therefore, the error measure can be represented as the difference between the actual rating and the computed rating:</p>

<p><script type="math/tex; mode=display"> \mathbf e_{ij} = \left( r_{ij} - \sum_{k = 1}^K u_{ik} v_{jk} \right)^2 </script></p>
<h2 id="neural-network-model">
<span class="hash">#</span>
<a href="#neural-network-model" class="header-link">Neural Network Model</a>
</h2>
<p>The perceptron model has a break point of 4 because it couldn&#39;t properly model situations such as the following:</p>

<p><img src="/images/notes/machine-learning/neural-networks/impossible-perceptron.png" class="center"></p>

<p>Although the above can&#39;t be modeled with one perceptron, perhaps the combination of more than one perceptron can. For example, we could divide the space twice, independently, with two different perceptrons and then somehow combine their results to effectively model the above scenario:</p>

<div style="text-align: center; margin-top: 10px">
  <img src="/images/notes/machine-learning/neural-networks/impossible-perceptron-left.png">
  <img src="/images/notes/machine-learning/neural-networks/impossible-perceptron-right.png">
</div>

<p>The results of these two independent perceptrons can be combined to model the impossible scenario. Specifically, one way this can be accomplished is by modeling logical AND and OR operations to combine results of separate perceptrons.</p>

<p>Logical OR can be modeled as a perceptron where the threshold weight <script type="math/tex">x_0</script> is set to <script type="math/tex">1.5</script>. This way it&#39;ll only return <script type="math/tex">-1</script> if both of the other features are <script type="math/tex">-1</script>, as is expected of logical OR:</p>

<p><img src="/images/notes/machine-learning/neural-networks/or-gate.png" class="center"></p>

<p>Logical AND can be modeled as a perceptron where the threshold weight <script type="math/tex">x_0</script> is set to <script type="math/tex">-1.5</script>. This way, both of the other features have to be <script type="math/tex">+1</script> in order to overcome the threshold weight and thus return <script type="math/tex">+1</script>, just as with logical AND:</p>

<p><img src="/images/notes/machine-learning/neural-networks/and-gate.png" class="center"></p>

<p>With these models, we can model the original complicated scenario. We can take the AND result of one of the perceptrons such as <script type="math/tex">h_1</script> and the negation of the other perceptron, <script type="math/tex">h_2</script>. This essentially means that the bottom quadrant in the target model would be <script type="math/tex">h_1</script>&#39;s <code>+</code> AND the negation of <script type="math/tex">h_2</script>&#39;s <code>-</code> which results in <code>+</code> as in the target model. The left quadrant can similarly be modeled by taking <script type="math/tex">h_1</script>&#39;s <code>-</code> AND the negation of <script type="math/tex">h_2</script>&#39;s <code>-</code> which results in <code>-</code> as in the target model. We take this and OR it with the alternate scenario, where <script type="math/tex">h_1</script>&#39;s values are negated, to fully complete the model.</p>

<p><img src="/images/notes/machine-learning/neural-networks/simple-layers.png" class="center"></p>

<p>The full, multi-layer perceptron can now be constructed, with the additional layer at the beginning which yields the separate, independent perceptrons <script type="math/tex">h_1</script> and <script type="math/tex">h_2</script>. Note that there are three layers, and it is strictly &quot;feedforward,&quot; that is, we don&#39;t feed outputs to previous layers nor do we skip layers:</p>

<p><img src="/images/notes/machine-learning/neural-networks/multilayer-perceptron.png" class="center"></p>

<p>This new model seems very powerful, with a seemingly infinite degree of freedom able to model a variety of situations. For example, we can model a circle only using perceptrons instead of using a non-linear transformation as we did before. The more perceptrons we use, the better the approximation:</p>

<p><img src="/images/notes/machine-learning/neural-networks/circular-perceptron-model.png" class="center"></p>

<p>There are two costs that this seemingly powerful model can incur. The first is <em>generalization</em> because we have so many perceptrons, yielding a higher VC dimension and with it higher degrees of freedom. This is a problem, but at least being aware of it means that we can overcome it by simply using an adequate number of examples. The other, perhaps more important cost, is that of <em>optimization</em>. When the data is not separable, it becomes a combinatorial optimization problem that is very difficult to solve.</p>

<p>The problem of optimization can be solved by using soft thresholds instead of the hard thresholds found in perceptrons. This can be facilitated with gradient descent, which features soft thresholds. Once the solution is found with soft thresholds and as a result so are the weights, we can switch to hard thresholds to perform classifications.</p>

<p>A neural network has various inputs and layers. Each layer has a non-linearity <script type="math/tex">\theta</script>, which is a generic non-linearity---not specifically the logistic function in logistic regression. To be precise, the non-linearity <script type="math/tex">\theta</script> is similar to the logistic function except that it ranges from <script type="math/tex">-1</script> to <script type="math/tex">+1</script>, to better approximate the hard threshold that goes from <script type="math/tex">-1</script> to <script type="math/tex">+1</script>. Each of the non-linearities <script type="math/tex">\theta</script> can be different. A famous approach to take is to make all of the <script type="math/tex">\theta</script>&#39;s be non-linear and then make the final <script type="math/tex">\theta</script> actually be linear. The intermediate layers are referred to as <em>hidden layers</em>. The final layer is referred to as the <em>output layer</em>:</p>

<p><img src="/images/notes/machine-learning/neural-networks/neural-network.png" class="center"></p>

<p>The non-linearity <script type="math/tex">\theta</script> in the neural network is the hyperbolic <script type="math/tex">\tan</script>, the <script type="math/tex">\tanh</script> function which takes on values ranging from <script type="math/tex">(-1, +1)</script>. If the signal (sum of the weights) is small then the <script type="math/tex">\tanh</script> function acts linear, and otherwise if it is very large then it asks as a hard threshold:</p>

<p><script type="math/tex; mode=display"> \theta(s) = \tanh(s) = \frac {e^s - e^{-s}} {e^s + e^{-s}} </script></p>

<p><img id="tanh" src="/images/notes/machine-learning/neural-networks/tanh.png" class="center"></p>

<p>The parameters of the neural network are weights <script type="math/tex">w</script> indexed by three indices consisting of the layers, inputs, and outputs, and each of these indices take on the following ranges:</p>

<p><script type="math/tex; mode=display">
w^{(l)}_{ij} = \begin{cases}
                 1 \leq l \leq L & \text {layers} \\
                 0 \leq i \leq d^{(l - 1)} & \text {inputs} \\
                 l \leq j \leq d^{(l)} & \text {ouputs}
               \end{cases}
</script></p>

<p>The neural network can therefore be represented by a recursive definition:</p>

<p><script type="math/tex; mode=display">
\def \neuronweight {w_{ij}^{(l)}}
x^{(l)}_j = \theta \left(s^{(l)}_j\right) =
\theta \left( \sum_{i = 0}^{d^{(l - 1)}} \neuronweight x_i^{(l - 1)} \right)
</script></p>

<p>The neural network is then constructed by applying the feature vector <script type="math/tex">\feature</script> to the first layer in the neural network <script type="math/tex">x^{(0)}_1, \dots, x^{(0)}_{d^{(0)}}</script> such that it eventually ends up in one scalar valued output from the last layer <script type="math/tex">\smash {x^{(L)}_1}</script>, which is the value that we will say that <script type="math/tex">h(\feature)</script> produces.</p>
<h2 id="neural-network-backpropagation">
<span class="hash">#</span>
<a href="#neural-network-backpropagation" class="header-link">Neural Network Backpropagation</a>
</h2>
<p>We will now apply stochastic gradient descent to our neural network. With SGD, whenever an example passes through the neural network, we will adjust all of the weights in the network in the direction of the negative of the gradient of that single example.</p>

<p>Therefore it can be said that all of the weights <script type="math/tex">\weight = \{ w^{(l)}_{ij} \}</script> determine <script type="math/tex">h(\feature)</script>. We then get the error on the example <script type="math/tex">(\feature_n, y_n)</script> which is defined as <script type="math/tex">\crossentropy(h(\feature_n), y_n) = \crossentropy(\weight)</script>. Therefore, to implement SGD we need to obtain the gradient <script type="math/tex">\nabla \crossentropy(\weight)</script>:</p>

<p><script type="math/tex; mode=display"> \nabla \crossentropy(\weight)\colon \frac {\partial \crossentropy(\weight)} {\partial w^{(l)}_{ij}} \quad \text {for all } i, j, l </script></p>

<p>There is a trick for efficient computation of the gradient by using the chain rule to separate the partial derivative into two separate partial derivatives:</p>

<p><script type="math/tex; mode=display"> \frac {\partial \crossentropy(\weight)} {\partial w^{(l)}_{ij}} = \frac {\partial \crossentropy(\weight)} {\partial s^{(l)}_j} \times \frac {\partial s^{(l)}_j} {\partial w^{(l)}_{ij}} </script></p>

<p>This is easily observable when we look at a diagram of a neural network layer:</p>

<p><img src="/images/notes/machine-learning/neural-networks/final-delta.png" class="center"></p>

<p>We already know the second component&#39;s result:</p>

<p><script type="math/tex; mode=display">
s^{(l)}_j = w^{(l)}_{ij} x^{(l - 1)}, \quad \text {so}\ 
\frac {\partial s^{(l)}_j} {\partial w^{(l)}_{ij}} = x^{(l - 1)}
</script></p>

<p>Now we only need to determine the value of the first component, which we will refer to as <script type="math/tex">\smash {\delta^{(l)}_j}</script>. This component would be easiest to compute if we knew the error output itself, for this reason, we will first compute it at the last layer and then propagate it backwards throughout the rest of the neural network. So what we want is <script type="math/tex">\smash {\delta^{(L)}_1}</script>, but first we&#39;ll have to define <script type="math/tex">\crossentropy(\weight)</script> for the final layer:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\crossentropy(\weight) &= \crossentropy(h(\feature_n), y_n) \\
&= \crossentropy(x^{(L)}_1, y_n) \\
&= (x^{(L)}_1 - y_n)^2 \qquad \text {assuming MSE}
\end{align}
</script></p>

<p>The output of the neural network is simply the signal of the final layer passed to that layer&#39;s non-linear function <script type="math/tex">\theta</script>:</p>

<p><script type="math/tex; mode=display"> x^{(L)}_1 = \theta(s^{(L)}_1) </script></p>

<p>Due to the chain rule, we have to know the derivative of <script type="math/tex">\theta</script> in order to compute the partial partial derivative <script type="math/tex">\delta</script>:</p>

<p><script type="math/tex; mode=display"> \theta'(s) = 1 - \theta^2(s) \quad \text {for } \theta = \tanh </script></p>

<p>Now all that we have to do is back-propagate the final layer&#39;s <script type="math/tex">\delta</script> to the previous layers. This is again facilitated by using the chain rule to break the partial derivative up into multiple components:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\delta^{(l - 1)}_i &= \frac {\partial \crossentropy(\weight)} {\partial s^{(l)}_j} \\
&= \sum_{j = 1}^{d^{(l)}} \frac {\partial \crossentropy(\weight)} {\partial s^{(l)}_j} \times
                          \frac {\partial s^{(l)}_j} {\partial x^{(l - 1)}_i} \times
                          \frac {\partial x^{(l - 1)}_i} {\partial s^{(l - 1)}_i} \\
&= \sum_{j = 1}^{d^{(l)}} \delta^{(l)}_j \times w^{(l)}_{ij} \times \theta'(s^{(l - 1)}_i) \\
&= (1 - (x^{(l - 1)}_i)^2) \sum_{j = 1}^{d^{(l)}} w^{(l)}_{ij} \delta^{(l)}_j
\end{align}
</script></p>

<p>This decomposition into three components is again observable from a diagram of a neural network layer:</p>

<p><img src="/images/notes/machine-learning/neural-networks/backpropagated-delta.png" class="center"></p>

<p>When the backpropagation is complete, there will be <script type="math/tex">\delta</script> values available at every position in the network where there is an <script type="math/tex">s</script> value. The algorithm can now be formulated:</p>

<ol>
<li>initialize all weights <script type="math/tex">w^{(l)}_{ij}</script> at random</li>
<li>for <script type="math/tex">t = 0, 1, 2, \dots</script> do

<ol>
<li>pick <script type="math/tex">n \in \{ 1, 2, \dots, N\}</script></li>
<li><strong>forward</strong>: compute all <script type="math/tex">x^{(l)}_j</script></li>
<li><strong>backward</strong>: compute all <script type="math/tex">\delta^{(l)}_j</script></li>
<li>update the weights: <script type="math/tex">w^{(l)}_{ij} \gets w^{(l)}_{ij} - \eta x^{(l - 1)}_i \delta^{(l)}_j</script></li>
<li>repeat until it is time to stop</li>
</ol></li>
<li>return the final weights <script type="math/tex">w^{(l)}_{ij}</script></li>
</ol>

<p>One final intuition is that the hidden layers are performing non-linear transformations which produce higher order features. However, these are features <em>learned</em> by the learning algorithm, with the VC dimension already taken into account. This allows us to avoid looking at the data to determine a proper non-linear transformation to perform explicitly and manually.</p>
<h1 id="overfitting">
<span class="hash">#</span>
<a href="#overfitting" class="header-link">Overfitting</a>
</h1>
<p>Imagine that we are given five points along with the target function in blue. We can see that the target function doesn&#39;t exactly fit some of the points, which means that there is some noise involved. If we were then to use a 4th-order polynomial, such as the one in red, to fit the points perfectly, we would get a very large <script type="math/tex">\outsample</script> despite <script type="math/tex">\insample = 0</script>.</p>

<p><img src="/images/notes/machine-learning/overfitting/polynomial-overfit.png" class="center"></p>

<p><em>Overfitting</em> is the act of fitting the data more than is warranted. It is a comparative term used to express that a solution went past a desirable point in terms of fitting, more so than another solution, where the different solutions can be different instances within the same model or different models entirely. For example, if we had used a 3rd-order polynomial instead, we would not have achieved <script type="math/tex">\insample = 0</script> but <script type="math/tex">\outsample</script> would have been considerably less. In that case, the 4th-order solution could have been considered overfitting compared to the 3rd-order solution. In other words, there is a distinction between overfitting and just plain bad generalization.</p>

<p>Overfitting within the same model occurs when <script type="math/tex">\insample</script> is decreasing but <script type="math/tex">\outsample</script> is beginning to increase, that is, when both error measures begin to diverge:</p>

<p><script type="math/tex; mode=display"> \text {overfitting:} \quad \insample \downarrow \quad \outsample \uparrow </script></p>

<p>If we stop right before this occurs, we call it <em>early stopping</em>:</p>

<p><img id="early-stopping" src="/images/notes/machine-learning/overfitting/early-stopping.png" class="center"></p>

<p>The main culprit cause for overfitting is <em>fitting the noise</em>, which is a natural side-effect of fitting the data. Fitting the noise is harmful because the learning algorithm is forming its solution from it, trying to detect a pattern, and therefore &quot;hallucinating&quot; an out-of-sample solution extrapolated from the in-sample noise.</p>

<p>Consider two different target functions with accompanying data points. The first target on the left is a simpler 10th-order polynomial but has noise involved (noisy low-order target), whereas the one on the right is a more complicated 50th-order polynomial but is noiseless (noiseless high-order target):</p>

<div style="text-align: center; margin-top: 10px">
  <img src="/images/notes/machine-learning/overfitting/case-study-1.png">
  <img src="/images/notes/machine-learning/overfitting/case-study-2.png">
</div>

<p>If we try to model the first target function using two models, a 2nd and 10th-order polynomial, we get this result:</p>

<p><img src="/images/notes/machine-learning/overfitting/case-study-1b.png" class="center"></p>

<p>The error measures clearly show that the 10th-order fit is a case of overfitting, showing the effects of how the 10th-order fit bends itself just to fit noise:</p>

<table>
<thead>
<tr>
<th style="text-align: left">error</th>
<th style="text-align: left">2nd-order</th>
<th style="text-align: left">10th-order</th>
</tr>
</thead>

<tbody>
<tr>
<td style="text-align: left"><script type="math/tex">\insample</script></td>
<td style="text-align: left"><script type="math/tex">0.050</script></td>
<td style="text-align: left"><script type="math/tex">0.034</script></td>
</tr>
<tr>
<td style="text-align: left"><script type="math/tex">\outsample</script></td>
<td style="text-align: left"><script type="math/tex">0.127</script></td>
<td style="text-align: left"><script type="math/tex">9.00</script></td>
</tr>
</tbody>
</table>

<p>Now we can try using the same order fits to model the second target function. Remember that the target is a 50th-order polynomial:</p>

<p><img src="/images/notes/machine-learning/overfitting/case-study-2b.png" class="center"></p>

<p>In this case, the 10th-order polynomial can fit the sample data very well, but the out of sample error is even worse. This is clearly another case of overfitting, because this target function actually <em>does</em> have noise, but it isn&#39;t the usual kind of noise:</p>

<table>
<thead>
<tr>
<th style="text-align: left">error</th>
<th style="text-align: left">2nd-order</th>
<th style="text-align: left">10th-order</th>
</tr>
</thead>

<tbody>
<tr>
<td style="text-align: left"><script type="math/tex">\insample</script></td>
<td style="text-align: left"><script type="math/tex">0.029</script></td>
<td style="text-align: left"><script type="math/tex">10^{-5}</script></td>
</tr>
<tr>
<td style="text-align: left"><script type="math/tex">\outsample</script></td>
<td style="text-align: left"><script type="math/tex">0.120</script></td>
<td style="text-align: left"><script type="math/tex">7680</script></td>
</tr>
</tbody>
</table>

<p>In the case of the 10th-order target, we can think of the 2nd-order fit as learner <script type="math/tex">R</script> (for restricted) and the 10th-order fit as learner <script type="math/tex">O</script> (for overfit). It can be said that <script type="math/tex">O</script> chose <script type="math/tex">\mathcal H_{10}</script> because it knew that the target is a 10th-order polynomial. On the other hand, <script type="math/tex">R</script> chose <script type="math/tex">\mathcal H_2</script> because it considered the number of points available in the training set, <script type="math/tex">15</script>. Choosing a 2nd-order polynomial provides three parameters, such that the ratio of points to degrees of freedom is 5:1, so we&#39;re pushing our luck since we know the rule of thumb is for it to be 10:1, but we do so because we figure we can&#39;t use a simple line when we <em>know</em> that the target is a 10th-order polynomial.</p>

<p>This reinforces the guideline that we are trying to match the data resources rather than the target complexity.</p>

<p>In the case of the 50th-order polynomial, <script type="math/tex">O</script> chooses <script type="math/tex">\mathcal H_{10}</script> and <script type="math/tex">R</script> chooses <script type="math/tex">\mathcal H_2</script>. We still got bad performance out of sample with <script type="math/tex">O</script>, so is there really no noise?</p>
<h2 id="role-of-noise">
<span class="hash">#</span>
<a href="#role-of-noise" class="header-link">Role of Noise</a>
</h2>
<p>We will conduct an experiment to observe the effects of overfitting, specifically to observe the impact of <em>noise level</em> and <em>target complexity</em>. Consider the general target function with added noise:</p>

<p><script type="math/tex; mode=display"> y = f(x) + \underbrace {\epsilon(x)}_{\sigma^2} =
\underbrace {\sum_{q = 0}^{Q_f} \alpha_q x^q}_{\text {normalized}} + \epsilon(x) </script></p>

<p>The level (energy) of noise is denoted by <script type="math/tex">\sigma^2</script>. This is a higher order <script type="math/tex">Q_f</script> (target complexity) polynomial. We normalize the polynomial quantity such that the energy is always <script type="math/tex">1</script>, since we want to observe the signal-to-noise ratio (SNR), so that we can definitively say that <script type="math/tex">\sigma^2</script> is really the amount of noise.</p>

<p>This function doesn&#39;t generate very interesting polynomials, so we will instead use the coefficients of Legendre polynomials, which are simply polynomials with specific coefficients such that from one order to the next they are orthogonal to each other, similar to harmonics in a sinusoidal expansion. Once the noise is added, the polynomial formed will be more interesting.</p>

<p>Those factors that seem to affect overfitting are the noise level <script type="math/tex">\sigma^2</script>, the target complexity <script type="math/tex">Q_f</script>, and the data set size <script type="math/tex">N</script>.</p>

<p>We fit the data set <script type="math/tex">(x_1, y_n), \dots, (x_N, y_N)</script> using our two models <script type="math/tex">\mathcal H_2</script> and <script type="math/tex">\mathcal H_{10}</script>, 2nd-order and 10th-order polynomials respectively. Each model yields a hypothesis function <script type="math/tex">g_2 \in \mathcal H_2</script> and <script type="math/tex">g_{10} \in \mathcal H_{10}</script>. The overfit measure is then defined as the difference between <script type="math/tex">\outsample</script> for the more complex model&#39;s hypothesis <script type="math/tex">g_{10}</script> and <script type="math/tex">\outsample</script> for the simpler model&#39;s hypothesis <script type="math/tex">g_2</script>. This works because if the more complex model is overfitting, its out of sample error will be bigger, yielding a larger positive number. If instead the measure yields a negative number, then it shows that the complex model is not overfitting because it&#39;s performing better than the simpler model:</p>

<p><script type="math/tex; mode=display"> \text {overfit measure: } \outsample(g_{10}) - \outsample(g_2) </script></p>

<p>If we run an experiment for tens of millions of iterations (complete runs). The following plots show the impact of the noise <script type="math/tex">\sigma^2</script> and the complexity <script type="math/tex">Q_f</script> on the overfit measure based on the data set size <script type="math/tex">N</script>:</p>

<div style="text-align: center; margin-top: 10px">
  <img src="/images/notes/machine-learning/overfitting/noise-impact.png">
  <img src="/images/notes/machine-learning/overfitting/complexity-impact.png">
</div>

<p>The main takeaway from these results is that there seems to be another factor aside from &quot;conventional noise&quot; that affects overfitting. The truth is that the noise <script type="math/tex">\sigma^2</script> measured in the left image is called <em>stochastic noise</em> (the more conventional noise). Meanwhile, the effect observed in the right image which seems to be related to an increase in complexity that brings about a higher overfit measure is caused by <em>deterministic noise</em>.</p>

<p><script type="math/tex; mode=display">
\begin{align}
\text {number of data points} &\uparrow \quad \text {overfitting} \downarrow \\
\text {stochastic noise} &\uparrow \quad \text {overfitting} \uparrow \\
\text {deterministic noise} &\uparrow \quad \text {overfitting} \uparrow
\end{align}
</script></p>
<h2 id="deterministic-noise">
<span class="hash">#</span>
<a href="#deterministic-noise" class="header-link">Deterministic Noise</a>
</h2>
<p><em>Deterministic noise</em> is the part of <script type="math/tex">f</script> that the best hypothesis function <script type="math/tex">h^*</script> from <script type="math/tex">\mathcal H</script> cannot capture:</p>

<p><script type="math/tex; mode=display"> \text {deterministic noise:}\ f(\feature) - h^*(\feature) </script></p>

<p><img src="/images/notes/machine-learning/overfitting/deterministic-noise.png" class="center"></p>

<p>An example of why we call this noise is the following. Imagine a younger sibling that has just learned fractions in school and comes to you to ask you to tell them more about numbers. You begin to teach them about negative numbers and real numbers, in a basic sense, but you probably would be better off not telling them about complex numbers. The reason is that this would be considered completely noise from their perspective, their &quot;hypothesis set&quot; would be so small that if you told them about complex numbers, they would create a pattern that doesn&#39;t actually exist (fitting the noise). You&#39;d be better off not telling them about complex numbers in order to avoid misleading them (just as noise would in learning).</p>

<p>This is why if we have a hypothesis set and part of the target function that we can&#39;t capture, there&#39;s no point in trying to capture it, because then we would be detecting a false pattern which we cannot extrapolate given the limitations of the hypothesis set.</p>

<p>Deterministic noise differs from stochastic noise in that:</p>

<ol>
<li>it depends on hypothesis set <script type="math/tex">\mathcal H</script>; for the same target function, using a more sophisticated hypothesis set will decrease the deterministic noise</li>
<li>it&#39;s fixed for a given <script type="math/tex">\feature</script></li>
</ol>

<p>When we have a finite <script type="math/tex">N</script> set, we gain the unfortunate ability to fit the noise, be it stochastic or deterministic, which wouldn&#39;t be possible if we had an infinite sized set.</p>
<h2 id="noise-and-bias-variance">
<span class="hash">#</span>
<a href="#noise-and-bias-variance" class="header-link">Noise and Bias-Variance</a>
</h2>
<p>Remember that the <a href="#bias-variance-representation">bias-variance decomposition</a> concerned a noiseless target <script type="math/tex">f</script>. So how would the decomposition work if <script type="math/tex">f</script> were a noisy target?</p>

<p><script type="math/tex; mode=display"> y = f(\feature) + \epsilon(\feature) \qquad \mathbb E [\epsilon(\feature)] = 0 </script></p>

<p>We can decompose it into bias-variance by adding a noise term:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\mathbb E_{\mathcal D} \Big[ (g^{(\mathcal D)}(\mathbf x) - f(\mathbf x))^2 \Big] &= \\
\mathbb E_{\mathcal D, \epsilon} \Big[ (g^{(\mathcal D)}(\mathbf x) - y)^2 \Big] &=
\mathbb E_{\mathcal D, \epsilon} \Big[ (g^{(\mathcal D)}(\mathbf x) - f(\feature) - \epsilon(\feature))^2 \Big] \\
&= \mathbb E_{\mathcal D, \epsilon} \Big[ (g^{(\mathcal D)}(\mathbf x) - \bar g(\feature) + \bar g(\feature) - f(\feature) - \epsilon(\feature))^2 \Big] \\
&= \mathbb E_{\mathcal D, \epsilon} \Big[ (g^{(\mathcal D)}(\mathbf x) - \bar g(\feature))^2 + (\bar g(\feature) - f(\feature))^2 - (\epsilon(\feature))^2 \\
& \phantom {= \mathbb E_\mathcal {D, \epsilon} \Big[} + \text {cross terms}\ \Big]
\end{align}
</script></p>

<p>Form this, we can derive the two noise terms. These can be described as moving from your hypothesis to the best possible hypothesis (variance), from the best possible hypothesis to the actual target (bias), and finally from the target to the actual output (with noise added):</p>

<p><script type="math/tex; mode=display">
\underbrace {\mathbb E_{\mathcal D, \epsilon} \Big[ (g^{(\mathcal D)}(\mathbf x) - \bar g(\feature))^2 \Big]}_{\textbf {var}} +
\underbrace {\mathbb E_{\mathrm x} \Big[ (\bar g(\feature) - f(\feature))^2 \Big]}_{\substack {\textbf {bias} \\[5 pt] \uparrow \\[5 pt] \text {deterministic noise}}} +
\underbrace {\mathbb E_{\epsilon, \mathrm x} \Big[ (\epsilon(\feature))^2 \Big]}_{\substack {\sigma^2 \\[5 pt] \uparrow \\[5 pt] \text {stochastic noise}}}
</script></p>

<p>Notice that the bias consists of the deterministic noise. This is because the average hypothesis <script type="math/tex">\bar g(\feature)</script> is supposed to be about the same as the best hypothesis, so the bias is a measure of how well the best hypothesis can approximate <script type="math/tex">f</script>, which is effectively a measure of the energy of deterministic noise.</p>
<h2 id="dealing-with-overfitting">
<span class="hash">#</span>
<a href="#dealing-with-overfitting" class="header-link">Dealing with Overfitting</a>
</h2>
<p>There are two cures for overfitting. <em>Regularization</em> can be described as hitting the breaks to avoid going into the point of overfitting. <em>Validation</em> on the other hand involves checking the bottom line and making sure it doesn&#39;t overfit.</p>
<h1 id="regularization">
<span class="hash">#</span>
<a href="#regularization" class="header-link">Regularization</a>
</h1>
<p>Regularization tends to reduce the bias at the expense of slightly increasing the variance. Regularization can be thought of as providing intermediate levels between the different fits such as constant, linear, quadratic.</p>
<h2 id="polynomial-model">
<span class="hash">#</span>
<a href="#polynomial-model" class="header-link">Polynomial Model</a>
</h2>
<p>The model <script type="math/tex">\mathcal H_Q</script> consists of the polynomials of order <script type="math/tex">Q</script>. A non-linear transformation produces <script type="math/tex">\mathbf z</script> by taking a scalar <script type="math/tex">x</script>. In effect, the elements in <script type="math/tex">\mathbf z</script> correspond to the coefficients of the Legendre polynomials. The parameterization of the hypothesis set is therefore represented as the linear combination of the weights and the Legendre polynomial coefficients:</p>

<p><script type="math/tex; mode=display">
\mathbf z = \begin{bmatrix}
              1 \\ L_1(x) \\ \vdots \\ L_Q(x)
            \end{bmatrix}, \quad
\mathcal H_Q = \left\{ \sum_{q = 0}^Q w_q L_q(x) \right\}
</script></p>

<p>The Legendre polynomials denoted by <script type="math/tex">L_q</script> look like this:</p>

<p><img src="/images/notes/machine-learning/regularization/legendre-polynomials.png" class="center"></p>

<p>Because of the summation, we&#39;re going to apply linear regression in the <script type="math/tex">\mathcal Z</script>-space.</p>

<p>We&#39;ve already placed a <em>hard</em> constraint on the weights in the previous section, in which case <script type="math/tex">\mathcal H_2</script> was the constrained version of <script type="math/tex">\mathcal H_{10}</script>. This is represented in the polynomial model by having set <script type="math/tex">w_q = 0</script> for <script type="math/tex">q > 2</script>.</p>

<p>However, we&#39;d like more of a <em>softer</em> constraint. Given a budget <script type="math/tex">C</script> for the total magnitude squared of the weights, <script type="math/tex">w_q^2</script>. Instead of the harder constraint above where we outright set some of the weights to <script type="math/tex">0</script>, we just want them to be generally small. This can be thought of as a <em>soft-order</em> constraint:</p>

<p><script type="math/tex; mode=display"> \sum_{q = 0}^Q w_q^2 \leq C </script></p>

<p>Given this model, we now want to minimize <script type="math/tex">\insample</script> while <em>being subject to</em> the constraint (shown in vector form):</p>

<p><script type="math/tex; mode=display">
\begin{align}
\text {minimize:} &\insample(\weight) = \frac 1 N (\mathbf {Zw - y})^\intercal (\mathbf {Zw - y}) \\[5 pt]
&\text {subject to:} \quad \weightT \weight \leq C
\end{align}
</script></p>

<p>The solution will be called <script type="math/tex">\def \weightreg {\weight_{\text {reg}}} \weightreg</script>, which signifies regularization, as opposed to <script type="math/tex">\def \weightlin {\weight_{\text {lin}}} \weightlin</script> which signifies linear regression.</p>

<p>The solution can be visualized by an ellipsoid. The in-sample error <script type="math/tex">\insample</script> is represented by the blue ellipsoid. The boundary of the ellipsoid consists of the same value for <script type="math/tex">\insample</script>. Anything inside the boundary is a smaller value and outside is larger. The value of <script type="math/tex">\weightT \weight</script> is represented by the red ellipsoid, so the constraint means that we have to be inside the red ellipsoid. The solution given by linear regression <script type="math/tex">\weightlin</script> is at the centroid of the blue ellipsoid, since it minimizes <script type="math/tex">\insample</script>. Since we want to choose a point within the red ellipsoid such that it minimizes <script type="math/tex">\insample</script>, it stands to reason that we&#39;ll have to go as far out as we can within the red ellipsoid. Therefore, the constraint we&#39;ll actually be using is <script type="math/tex">\weightT \weight = C</script>, since the best value of <script type="math/tex">\insample</script> will occur at the boundary of the red ellipsoid. Of course, if the red ellipsoid was large enough to contain <script type="math/tex">\weightlin</script>, then the solution would be <script type="math/tex">\weightlin</script> since that is the minimum:</p>

<p><img id="solution-visualization" src="/images/notes/machine-learning/regularization/solution-visualization.png" class="center"></p>

<p>Choose a <script type="math/tex">\weight</script> that lies on both of the ellipsoid boundaries. From here, we can visualize the gradient of <script type="math/tex">\insample</script> with respect to the chosen point as well as the orthogonal vector to the red ellipsoid. The orthogonal vector to the red ellipsoid is equivalent to <script type="math/tex">\weight</script> (from the center). From visualizing these vectors, we can tell that the chosen point below doesn&#39;t minimize <script type="math/tex">\insample</script>. If it did minimize <script type="math/tex">\insample</script>, then both vectors would be directly opposite each other:</p>

<p><img id="solution-vectors" src="/images/notes/machine-learning/regularization/solution-vectors.png" class="center"></p>

<p>A condition can therefore be expressed that the gradient of the solution <script type="math/tex">\weightreg</script> when found should be proportional to the negation of <script type="math/tex">\weightreg</script>:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\nabla \insample(\weightreg) &\propto -\weightreg \\
&= -2 \frac \lambda N \weightreg \\
\nabla \insample(\weightreg) &+ -2 \frac \lambda N \weightreg = \mathbf 0
\end{align}
</script></p>

<p>The solution of the last equation is simply the minimization of the following:</p>

<p><script type="math/tex; mode=display"> \insample(\weight) + \frac \lambda N \weightT \weight </script></p>

<p>The values of <script type="math/tex">C</script> and <script type="math/tex">\lambda</script> are related, so that if <script type="math/tex">C</script> is so big it already contains <script type="math/tex">\weightlin</script>, so <script type="math/tex">\lambda</script> can be thought of as being <script type="math/tex">0</script>, and corresponding to the simple minimization of <script type="math/tex">\insample</script>. When <script type="math/tex">C</script> is smaller, <script type="math/tex">\lambda</script> has to go up:</p>

<p><script type="math/tex; mode=display"> C \uparrow \quad \lambda \downarrow </script></p>
<h2 id="augmented-error">
<span class="hash">#</span>
<a href="#augmented-error" class="header-link">Augmented Error</a>
</h2>
<p>We&#39;re now going to consider an augmented error <script type="math/tex">\def \augerror {E_{\text {aug}}} \augerror</script> that is essentially <script type="math/tex">\insample</script> augmented with the regularization term:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\augerror(\weight) &= \insample(\weight) + \frac \lambda N \weightT \weight \\
&= \frac 1 N (\mathbf {Zw - y})^\intercal (\mathbf {Zw - y}) + \frac \lambda N
\weightT \weight
\end{align}
</script></p>

<p>The key observation is that solving the above equation is equivalent to minimizing <script type="math/tex">\insample</script> <em>subject to</em> the constraint <script type="math/tex">\weightT \weight \leq C</script>. This constraint lends itself to VC analysis.</p>

<p><script type="math/tex; mode=display">
\begin{align}
\augerror(\weight) &= \insample(\weight) + \frac \lambda N \weightT \weight \\
&= \frac 1 N \left( (\mathbf {Zw - y})^\intercal (\mathbf {Zw - y}) + \lambda \weightT \weight \right)
\end{align}
</script>
To minimize this, we take the gradient of <script type="math/tex">\augerror</script> and equate it to <script type="math/tex">\mathbf 0</script>:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\nabla \augerror(\weight) &= \mathbf 0 \\
\mathbf Z^\intercal (\mathbf {Zw - y}) + \lambda \weight &= \mathbf 0
\end{align}
</script></p>

<p>So now this results in the weight vector with regularization, <script type="math/tex">\weightreg</script>:</p>

<p><script type="math/tex; mode=display"> \weightreg = (\mathbf Z^\intercal \mathbf Z + \lambda \mathbf I)^{-1} \mathbf Z^\intercal \mathbf y </script></p>

<p>This is opposed to the weight vector <em>without</em> regularization, <script type="math/tex">\weightlin</script>, which can be achieved by simply setting <script type="math/tex">\lambda = 0</script>:</p>

<p><script type="math/tex; mode=display"> \weightlin = (\mathbf Z^\intercal \mathbf Z)^{-1} \mathbf Z^\intercal \mathbf y </script></p>

<p>We can now observe the effects of varying <script type="math/tex">\lambda</script>. In the first example, there is apparent overfitting. However, as <script type="math/tex">\lambda</script> increased, there is apparent underfitting. Clearly, the choice of <script type="math/tex">\lambda</script> is important:</p>

<div style="text-align: center; margin-top: 10px">
  <img src="/images/notes/machine-learning/regularization/lambda-variation-1.png">
  <img src="/images/notes/machine-learning/regularization/lambda-variation-2.png">
</div>
<h2 id="weight-decay">
<span class="hash">#</span>
<a href="#weight-decay" class="header-link">Weight Decay</a>
</h2>
<p>The name of the regularizer that involves minimizing the following quantity is referred to as <em>weight &#39;decay&#39;</em>:</p>

<p><script type="math/tex; mode=display"> \insample(\weight) + \frac \lambda N \weightT \weight </script></p>

<p>For example, in batch gradient descent, we take a step from <script type="math/tex">\weight(t)</script> to <script type="math/tex">\weight(t + 1)</script>:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\weight(t + 1) &= \weight(t) - \eta \nabla \insample(\weight(t)) - 2 \eta \frac \lambda N \weight(t) \\
&= \weight(t) \underbrace {(1 - 2 \eta \frac \lambda N)}_{\text {nudge factor}} - \eta \nabla \insample(\weight(t))
\end{align}
</script></p>

<p>This can be interpreted geometrically. In the usual implementation of batch gradient descent we have the previous step <script type="math/tex">\weight(t)</script> that is moved in the direction of the gradient <script type="math/tex">\nabla \insample</script>. However, this now includes a regularization term which can be interpreted as a nudge factor. That is, it takes the previous position, nudges it a bit based on <script type="math/tex">\lambda</script>, and <em>then</em> performs the gradient descent. This is why the regularizer is called weight decay, because the weights decay from one iteration to the next.</p>

<p>In neural networks, the term <script type="math/tex">\weightT \weight</script> can be computed by considering all of the weights in all of the layers, input units, and output units and squaring them and summing them up:</p>

<p><script type="math/tex; mode=display"> \weightT \weight = \sum_{l = 1}^L \sum_{i = 0}^{d^{(l - 1)}} \sum_{j = 1}^{d^{(l)}} \left( \neuronweight \right)^2 </script></p>
<h3 id="variations-of-weight-decay">
<span class="hash">#</span>
<a href="#variations-of-weight-decay" class="header-link">Variations of Weight Decay</a>
</h3>
<p>Instead of having a fixed budget <script type="math/tex">C</script> and having the sum of the squared weights being less than or equal to <script type="math/tex">C</script>, we could emphasize certain weights by using this regularizer:</p>

<p><script type="math/tex; mode=display"> \sum_{q = 0}^Q \gamma_q w_q^2 </script></p>

<p>Here, <script type="math/tex">\gamma</script> is referred to as the <em>importance factor</em>. For example, if a particular <script type="math/tex">\gamma_q</script> is small then the equivalent weight <script type="math/tex">w_q</script> is less restricted; it can be made larger while knowing it won&#39;t take up too much of the budget <script type="math/tex">C</script>. On the other hand, if a particular <script type="math/tex">\gamma_q</script> is big, then the corresponding weight doesn&#39;t have that luxury.</p>

<p>For example, imagine that <script type="math/tex">\gamma_q</script> is set to <script type="math/tex">2^q</script>. In this case, the regularizer is giving a larger emphasis on higher order terms. What this means is that the regularizer is trying to find a <em>low-order fit</em>. For example, a 10th-order polynomial would quickly kill the budget because <script type="math/tex">\gamma_{10} = 2^{10}</script>. If instead we had <script type="math/tex">\gamma_q</script> set to <script type="math/tex">2^{-q}</script>, it would look for a <em>high-order fit</em>.</p>

<p>In neural networks, the importance factor <script type="math/tex">\gamma</script> is given different values for each each layer, in other words, giving different emphasis for the weights in different layers.</p>

<p>The most general regularizer is called the <em>Tikhonov regularizer</em>, which has the form:</p>

<p><script type="math/tex; mode=display"> \weightT \mathbf \Gamma^\intercal \mathbf \Gamma \weight </script></p>

<p>This is a general quadratic in matrix form. Weight decay, low-order fits, high-order fits, and many other regularizations can be achieved given the proper choice of matrix <script type="math/tex">\mathbf \Gamma</script>.</p>
<h2 id="weight-growth">
<span class="hash">#</span>
<a href="#weight-growth" class="header-link">Weight Growth</a>
</h2>
<p>Just as big weights were constrained earlier, small weights can also be constrained. The following shows a plot of weight decay&#39;s <script type="math/tex">\outsample</script> as a function of the regularization parameter <script type="math/tex">\lambda</script>. Notice that it dips before it goes back up again, in which case it begins to underfit. This means that weight decay performs well <em>given</em> the correct choice of <script type="math/tex">\lambda</script>. Weight growth on the other hand---constraining weights to be large---is considerably worse.</p>

<p><img src="/images/notes/machine-learning/regularization/regularizers.png" class="center"></p>

<p>Stochastic noise is &quot;high-frequency&quot;. Deterministic noise is also non-smooth. Because of these common observations, the guideline is to <strong>constrain learning towards smoother hypotheses</strong>. This is because the regularizer is a cure for fitting the noise. For this reason, we want to punish the noise more than we are punishing the signal. The usual way that hypothesis sets are mathematically written as a parameterized set is by having smaller weights correspond to smoother hypotheses.</p>
<h2 id="generalized-regularizer">
<span class="hash">#</span>
<a href="#generalized-regularizer" class="header-link">Generalized Regularizer</a>
</h2>
<p>The regularizer will be referred to as <script type="math/tex">\Omega</script>:</p>

<p><script type="math/tex; mode=display"> \Omega = \Omega(h) </script></p>

<p>We will minimize the augmented error of the hypothesis:</p>

<p><script type="math/tex; mode=display"> \augerror(h) = \insample(h) + \frac \lambda N \Omega(h) </script></p>

<p>This looks similar to the VC dimension:</p>

<p><script type="math/tex; mode=display"> \outsample(h) \leq \insample(h) + \Omega(\mathcal H) </script></p>

<p>From this, we make the claim that <script type="math/tex">\augerror</script> is better than <script type="math/tex">\insample</script> as a proxy of <script type="math/tex">\outsample</script>.</p>

<p>The perfect regularizer is one that restricts in the &quot;direction&quot; of the target function. Regularization generically applies a methodology which harms the overfitting (fitting the noise) more than it harms the fitting (fitting the guideline), therefore it&#39;s a <em>heuristic</em>. For this reason, we move in the direction of <em>smoother</em> or &quot;simpler&quot; because the noise itself <strong>is not</strong> smooth. For example, in the case of movie ratings, this kind of regularization guideline tends to opt for the average rating in the absence of enough examples.</p>

<p>If we choose a bad regularizer <script type="math/tex">\Omega</script>, we still have <script type="math/tex">\lambda</script> that can save us by means of validation, which can set <script type="math/tex">\lambda</script> such that it completely ignores the bad regularizer <script type="math/tex">\Omega</script>. In any case, having a regularizer is necessary to avoid overfitting.</p>
<h2 id="neural-network-regularizers">
<span class="hash">#</span>
<a href="#neural-network-regularizers" class="header-link">Neural Network Regularizers</a>
</h2>
<p>Remember that the <a href="#tanh"><script type="math/tex">\tanh</script></a> function acts linear for small value ranges and binary for larger value ranges.</p>
<h3 id="neural-network-weight-decay">
<span class="hash">#</span>
<a href="#neural-network-weight-decay" class="header-link">Neural Network Weight Decay</a>
</h3>
<p>If we use small weights, then the signal is constrained to the range that produces linear-like values, so every neuron would essentially be computing the linear function. With multiple hidden layers in a neural network, this essentially becomes a complex neural network that is computing a very simple linear function.</p>

<p>If weights are increased to the maximum, <script type="math/tex">\tanh</script> outputs binary values, which results in a logical dependency that can be used to implement any kind of functionality.</p>

<p>Given these two extremes, it&#39;s apparent that as weights are increased, the model produces more complex models. This represents a clear correspondence between the simplicity of the function being implemented and the size of the weights, which is properly represented by <em>weight decay</em>.</p>
<h3 id="neural-network-weight-elimination">
<span class="hash">#</span>
<a href="#neural-network-weight-elimination" class="header-link">Neural Network Weight Elimination</a>
</h3>
<p><em>Weight elimination</em> works by remembering that the VC dimension of neural networks is more or less the number of weights, so it simply eliminates some of the weights by forcing them to be <script type="math/tex">0</script>, so that the number of free parameters decreases, as does the VC dimension, leading to a better chance of generalizing and less chance of overfitting.</p>

<p>Determining which weights to eliminate can is a combinatorial problem. <em>Soft weight elimination</em> is an optimization of weight elimination. For very small weights, regular weight decay is applied. For very large weights, the result is close to <script type="math/tex">1</script>. In other words, big weights are left alone and small weights are pushed towards <script type="math/tex">0</script>, or &quot;soft eliminated&quot;:</p>

<p><script type="math/tex; mode=display">
\Omega(\weight) = \sum_{i,j,l} \frac {\left( \neuronweight \right)^2} {\beta^2 + \left( \neuronweight \right)^2}
</script></p>
<h3 id="early-stopping-in-neural-networks">
<span class="hash">#</span>
<a href="#early-stopping-in-neural-networks" class="header-link">Early Stopping in Neural Networks</a>
</h3>
<p>Recall that <a href="#early-stopping">early stopping</a> consists of stopping before <script type="math/tex">\outsample</script> begins to increase. This is regularization through the optimizer. The point at which to stop is determined through <a href="#validation">validation</a>.</p>
<h2 id="optimal-script-typemathtexlambdascript">
<span class="hash">#</span>
<a href="#optimal-script-typemathtexlambdascript" class="header-link">Optimal <script type="math/tex">\lambda</script></a>
</h2>
<p>Observing the optimal value of <script type="math/tex">\lambda</script> given the different levels of noise provides some insight as to how deterministic noise is treated by the regularizer compared to how it treats stochastic noise.</p>

<p>As the level of <em>stochastic noise</em> <script type="math/tex">\sigma^2</script> increases, the level of regularization <script type="math/tex">\lambda</script> necessary to achieve the minimum <script type="math/tex">\outsample</script> increases. For example, when there is no noise, the minimum <script type="math/tex">\outsample</script> can be achieved with <script type="math/tex">\lambda = 0</script>. However, as the noise increases, regularization is <em>necessary</em> to achieve the best possible <script type="math/tex">\outsample</script>. The left side of the red and green curves represent the overfitting that occurs without regularization.</p>

<p><img src="/images/notes/machine-learning/regularization/lambda-for-stochastic-noise.png" class="center"></p>

<p>The same behavior is observed as the level of <em>deterministic noise</em> <script type="math/tex">Q_f</script> (complexity of the target function) increases.</p>

<p><img src="/images/notes/machine-learning/regularization/lambda-for-stochastic-noise.png" class="center"></p>

<p>This cements the correspondence that regularization behaves with respect to deterministic noise behaves almost exactly as if it were unknown stochastic noise.</p>
<h1 id="validation">
<span class="hash">#</span>
<a href="#validation" class="header-link">Validation</a>
</h1>
<p>Remember that regularization tried to estimate the &quot;overfit penalty;&quot; so that instead of minimizing just <script type="math/tex">\insample</script> it would minimize <script type="math/tex">\insample</script> plus the overfit penalty, a quantity known as the augmented error <script type="math/tex">\augerror</script>. <em>Validation</em> instead tries to estimate the actual <script type="math/tex">\outsample</script> and tries to minimize it directly.</p>

<p>To arrive at this estimate, we first find an out-of-sample point <script type="math/tex">(\weight, y)</script>, that is, a point that was not involved in training, otherwise known as a <em>validation point</em>. The error for this point is <script type="math/tex">\crossentropy(h(\weight), y)</script>. The error itself could be the squared error, binary error, etc., and we consider it to be an estimate of <script type="math/tex">\outsample</script>, which we can do because:</p>

<p><script type="math/tex; mode=display">\mathbb E \left[ \crossentropy(h(\weight), y) \right] = \outsample(h)</script></p>

<p>This is an unbiased estimate of <script type="math/tex">\outsample</script>. The variance is known as <script type="math/tex">\sigma^2</script>:</p>

<p><script type="math/tex; mode=display">\text {var} \left[ \crossentropy(h(\weight), y) \right] = \sigma^2</script></p>

<p>The variance of estimating <script type="math/tex">\outsample</script> from a single point would be very high, so instead of calculating this estimate from a single point, we do it from an entire <em>validation set</em> <script type="math/tex">(\weight_1, y_1), \dots, (\weight_K, y_K)</script>. Notice that the total number of points in the validation set is <script type="math/tex">K</script>, as opposed to <script type="math/tex">N</script> in the training set. The validation error derived from this validation set is known as <script type="math/tex">\def \valerror {E_{\text {val}}} \valerror</script>:</p>

<p><script type="math/tex; mode=display">\valerror(h) = \frac 1 K \sum_{k = 1}^K \crossentropy(h(\weight_k), y_k)</script></p>

<p>Therefore the expected value of <script type="math/tex">\valerror</script> is:</p>

<p><script type="math/tex; mode=display">\mathbb E \left[ \valerror(h) \right] = \frac 1 K \sum_{k = 1}^K \mathbb E \left[ \crossentropy(h(\weight_k), y_k) \right] = \outsample(h)</script></p>

<p>The variance is computed as:</p>

<p><script type="math/tex; mode=display">\text {var} \left[ \valerror(h) \right] = \frac 1 {K^2} \sum_{k = 1}^K \text {var} \left[ \crossentropy(h(\weight_k), y_k) \right] = \frac {\sigma^2} K</script></p>

<p>The validation error:</p>

<p><script type="math/tex; mode=display">\valerror(h) = \outsample(h) \pm O\left(\frac 1 {\sqrt K}\right)</script></p>

<p>Given the data set <script type="math/tex">\mathcal D = (\weight_1, y_1), \dots, (\weight_N, y_N)</script>, take <script type="math/tex">K</script> points at random and use them for validation, which leaves <script type="math/tex">N - K</script> for training. The set of <script type="math/tex">N - K</script> training points is called <script type="math/tex">\def \trainingset {\mathcal D_{\text {train}}}</script> and the set of <script type="math/tex">K</script> validation points is called <script type="math/tex">\def \validationset {\mathcal D_{\text {val}}}</script>.</p>

<p>We established that the reliability of the estimate of the validation set is of the order <script type="math/tex">\smash {\frac 1 {\sqrt K}}</script>, so the smaller the size <script type="math/tex">K</script> of the validation set, the worse off the estimate is. On the other hand, if we use a very large <script type="math/tex">K</script> we would get a reliable estimate of a worse quantity of <script type="math/tex">\outsample</script>, because the <script type="math/tex">\outsample</script> decreases with the increase in training set size, the size of which must decrease if we increase the validation set size.</p>

<p>We would instead like to estimate the error using the validation set, then once we have the estimate, return the <script type="math/tex">K</script> points to the training set. We begin by deciding that the data set <script type="math/tex">\mathcal D</script> is comprised of <script type="math/tex">\trainingset \cup \validationset</script>. Previously when we used the entire training set <script type="math/tex">\mathcal D</script> we arrived at the final hypothesis <script type="math/tex">g</script>. However, when using only <script type="math/tex">\trainingset</script>, we will call the resulting final hypothesis <script type="math/tex">g^-</script> to remind ourselves that it&#39;s not based on the full training set.</p>

<p><script type="math/tex">\trainingset</script> yields <script type="math/tex">g^-</script> which is then evaluated on <script type="math/tex">\validationset</script> in order to get the estimate <script type="math/tex">\valerror(g^-)</script>. Then the final hypothesis <script type="math/tex">g</script> is derived using the full set <script type="math/tex">\mathcal D</script>. This means that there will be a discrepancy between the estimate <script type="math/tex">\valerror(g^-)</script> and <script type="math/tex">\valerror(g)</script>, because they are based on different hypotheses, as derived from different training sets. This presents a trade off between the size of <script type="math/tex">\trainingset</script> and <script type="math/tex">\validationset</script>: a larger <script type="math/tex">\trainingset</script> means that the hypothesis more closely resembles the final hypothesis, but a subsequently smaller <script type="math/tex">\validationset</script> makes the estimate a poor one, even if the hypothesis <script type="math/tex">g^-</script> is representative of the final hypothesis <script type="math/tex">g</script>.</p>

<p><img src="/images/notes/machine-learning/validation/validation.png" class="center"></p>

<p>The <em>rule of thumb</em> for the choice of the size <script type="math/tex">K</script> of <script type="math/tex">\validationset</script> is to make it <script type="math/tex">1/4^{\text {th}}</script> the size of of the full set <script type="math/tex">\mathcal D</script>:</p>

<p><script type="math/tex; mode=display">K = \frac N 5</script></p>
<h2 id="purpose-of-validation">
<span class="hash">#</span>
<a href="#purpose-of-validation" class="header-link">Purpose of Validation</a>
</h2>
<p>Looking back at the <a href="#early-stopping">early stopping</a> curve, if we used <script type="math/tex">K</script> points to estimate <script type="math/tex">\outsample</script> then the estimate would be called <script type="math/tex">E_{\text {test}}</script> <em>if</em> we don&#39;t perform any actions based on this estimate. As soon as we decide to perform an action based on it, such as stopping, then the previous <script type="math/tex">\insample</script> is considered the <em>validation error</em>, because it is now considered to be biased. The difference is that the test set is unbiased, whereas the validation set has an optimistic bias.</p>

<p>For example, given two hypotheses <script type="math/tex">h_1</script> and <script type="math/tex">h_2</script> with <script type="math/tex">\outsample(h_1) = \outsample(h_2) = 0.5</script> and error estimates <script type="math/tex">\crossentropy_1</script> and <script type="math/tex">\crossentropy_2</script> that are uniform on <script type="math/tex">[0,1]</script>. Now pick <script type="math/tex">h \in \{h_1, h_2\}</script> based on <script type="math/tex">\crossentropy = \min(\crossentropy_1, \crossentropy_2)</script>. The value of <script type="math/tex">\mathbb E(\crossentropy) < 0.5</script>, because given two variables, the probability that one of them is <script type="math/tex">< 0.5</script> is <script type="math/tex">75\%</script>, since the probability that both them are <script type="math/tex">0.5</script> is <script type="math/tex">0.5 * 0.5 = 25\%</script>. This means that <script type="math/tex">\mathbb E(\crossentropy)</script> is an optimistic bias. This bias has a negligible effect on overall learning.</p>
<h2 id="model-selection">
<span class="hash">#</span>
<a href="#model-selection" class="header-link">Model Selection</a>
</h2>
<p>The main use of validation sets is model selection, which is accomplished by using <script type="math/tex">\validationset</script> more than once. We begin with <script type="math/tex">M</script> models <script type="math/tex">\mathcal {H_1, \dots, H_M}</script> to choose from. This could be models such as linear models, neural networks, support vector machines, etc. It could also be that we&#39;re only using polynomial models, in which case the question is whether to use 2nd, 3rd, 4th-order etc. It could also be that we&#39;re only using 5th-order polynomials, in which case the question is the value of <script type="math/tex">\lambda</script> being for example <script type="math/tex">0.01, 0.1</script> or <script type="math/tex">1.0</script>. All of this falls under model selection, based on <script type="math/tex">\outsample</script> by means of <script type="math/tex">\validationset</script>.</p>

<p>Then we will use <script type="math/tex">\trainingset</script> to learn <script type="math/tex">g_m^-</script> for each model. The hypothesis <script type="math/tex">g_m^-</script> is then evaluated using <script type="math/tex">\validationset</script> to derive <script type="math/tex">E_{\text m}</script>, which is simply <script type="math/tex">\valerror(g_m^-)</script>. The model with the smallest <script type="math/tex">E_{\text m}</script> is chosen as the optimal model <script type="math/tex">m^*</script>, which is then used to train on the entire dataset <script type="math/tex">\mathcal D</script> to derive the hypothesis <script type="math/tex">g_{m^*}</script>.</p>

<p><img src="/images/notes/machine-learning/validation/model-selection.png" class="center"></p>
<h2 id="validation-bias">
<span class="hash">#</span>
<a href="#validation-bias" class="header-link">Validation Bias</a>
</h2>
<p>The bias is introduced because we chose the model <script type="math/tex">\mathcal H_{m^*}</script> using <script type="math/tex">\validationset</script>, so <script type="math/tex">\valerror(g_{m^*}^-)</script> is a biased estimate of <script type="math/tex">\outsample(g_{m^*}^-)</script>. The following graph depicts two models to choose between, one being 2nd-order and the other 5th-order polynomials. It&#39;s not shown which model was chosen.</p>

<p><img src="/images/notes/machine-learning/validation/validation-bias.png" class="center"></p>

<p>The curve for <script type="math/tex">\valerror</script> goes up because the higher the <script type="math/tex">K</script>, the less points that are left for <script type="math/tex">\trainingset</script>. The curve for <script type="math/tex">\outsample</script> goes up because it is based on the number of points left in <script type="math/tex">\trainingset</script>, so the error increases as there are less and less points.</p>

<p>The curve for <script type="math/tex">\valerror</script> converges with <script type="math/tex">\outsample</script> because <script type="math/tex">K</script> is increasing, which means that the estimate <script type="math/tex">\valerror</script> is more and more accurate.</p>

<p>It can be considered that <script type="math/tex">\validationset</script> is actually used for &quot;training&quot; on a very special kind of hypothesis set, known as the <em>finalists model</em>, which consists of the hypotheses derived from each model:</p>

<p><script type="math/tex; mode=display">\mathcal H_{\text {val}} = \{g_1^-, g_2^-, \dots, g_M^-\}</script></p>

<p>Now when we measure the bias between <script type="math/tex">\valerror</script> and <script type="math/tex">\outsample</script>, where <script type="math/tex">\valerror</script> can be thought of as the training error of this special set <script type="math/tex">\mathcal H_{\text {val}}</script>. This means we can go back to Hoeffding and VC, to say that:</p>

<p><script type="math/tex; mode=display">
\outsample(g_{m^*}^-) \leq \valerror(g_{m^*}^-) +
O\left( \sqrt {\frac {\ln M} K} \right)
</script></p>

<p>If for example we&#39;re trying to determine the value of <script type="math/tex">\lambda</script>, it can be thought of as being able to take on an infinite number of values, however, instead it can be though of as being a single parameter, which according to the VC inequality is not a problem. This applies to regularization which is trying to determine the value for the single parameter <script type="math/tex">\lambda</script>, or early-stopping which is trying to determine the value for the single parameter <script type="math/tex">T</script> (number of epochs to perform). In both cases, they more or less correspond to having one degree of freedom.</p>
<h2 id="data-contamination">
<span class="hash">#</span>
<a href="#data-contamination" class="header-link">Data Contamination</a>
</h2>
<p>The error estimates encountered so far have been <script type="math/tex">\insample</script>, <script type="math/tex">E_{\text {test}}</script>, and <script type="math/tex">\valerror</script>. These can be described as data contaminations, where if the data is used to make choices, it is being contaminated as far as its ability to estimate the real performance. In this sense, <em>contamination</em> is the optimistic (deceptive) bias in estimating <script type="math/tex">\outsample</script>.</p>

<p>The training set is totally contaminated. For example, if it&#39;s put through a neural network with 70 parameters that yielded a very low <script type="math/tex">\insample</script>, we can be assured that <script type="math/tex">\insample</script> is no indication of <script type="math/tex">\outsample</script>, in which case <script type="math/tex">\insample</script> can&#39;t be relied upon as an estimate for <script type="math/tex">\outsample</script>.</p>

<p>The validation set is slightly contaminated because it made a few choices.</p>

<p>The test set, on the other hand, is completely clean, since it wasn&#39;t used in any decisions, so it is unbiased.</p>
<h2 id="cross-validation">
<span class="hash">#</span>
<a href="#cross-validation" class="header-link">Cross-Validation</a>
</h2>
<p>We would like to know the actual <script type="math/tex">\outsample(g)</script>, but we only have <script type="math/tex">\valerror(g^-)</script>, which is supposed to be an estimate for <script type="math/tex">\outsample(g^-)</script>. Therefore it is necessary for <script type="math/tex">K</script> to be small so that <script type="math/tex">g^-</script> is fairly close to <script type="math/tex">g</script>, since the bigger <script type="math/tex">K</script> is, the larger the discrepancy between the training set and the full set, and therefore larger the discrepancy between <script type="math/tex">g^-</script> and <script type="math/tex">g</script>. This creates a <em>dilemma</em> because we would like <script type="math/tex">K</script> to be large, to increase the reliability of <script type="math/tex">\valerror(g^-)</script> as an estimate for <script type="math/tex">\outsample(g^-)</script>.</p>

<p><script type="math/tex; mode=display">\outsample(g) \approx \outsample(g^-) \approx \valerror(g^-)</script></p>

<p>It&#39;s possible to achieve both constraints by using a technique called <em>leave one out</em>. First choose <script type="math/tex">N - 1</script> points for training and <script type="math/tex">1</script> point for validation. The training set <script type="math/tex">\mathcal D_n</script> is denoted by the point that was <script type="math/tex">n</script> that was left out:</p>

<p><script type="math/tex; mode=display">\mathcal D_n = (\feature_1, y_1), \dots, (\feature_{n - 1}, y_{n - 1}), \underbrace {(\feature_n, y_n)}_{\text {validation point}}, (\feature_{n + 1}, y_{n + 1}), \dots, (\feature_N, y_N)</script></p>

<p>The final hypothesis learned from <script type="math/tex">\mathcal D_n</script> is called <script type="math/tex">g_n^-</script>, since it was trained on all examples except <script type="math/tex">n</script>. The validation error is then defined as simply the error on the left-out point using the hypothesis derived from the set that excludes that point. We can be sure that this estimate is an unbiased estimate, albeit a bad estimate:</p>

<p><script type="math/tex; mode=display">\crossentropy_n = \valerror(g_n^-) = \crossentropy(g_n^-(\weight_n), y_n)</script></p>

<p>This can then be repeated for every point in the data set using every <script type="math/tex">\mathcal D_n</script>. What is common between every error measure is that it was computed using the hypothesis derived using a training set of <script type="math/tex">N - 1</script> points, albeit different points.</p>

<p>The <em>cross validation error</em> is then defined as:</p>

<p><script type="math/tex; mode=display">
\def \crossvalidation {E_{\text {ev}}}
\crossvalidation = \frac 1 N \sum_{n = 1}^N \crossentropy_n
</script></p>

<p>The effective size of the validation set represented by the cross validation error is very close to <script type="math/tex">N</script>.</p>

<p>For example, given three points, we train three different sets:</p>

<p><img src="/images/notes/machine-learning/validation/cross-validation.png" class="center"></p>

<p>The cross validation error can then be defined as:</p>

<p><script type="math/tex; mode=display">\crossvalidation = \frac 1 3 (\crossentropy_1 + \crossentropy_2 + \crossentropy_3)</script></p>

<p>This cross validation error can then be interpreted as how well the linear model fits the data <em>out of sample</em>.</p>

<p>Cross-validation can then be used for model selection. Compare the above linear model example with the constant model:</p>

<p><img src="/images/notes/machine-learning/validation/cross-validation-model-selection.png" class="center"></p>

<p>In this case, the constant model is clearly better.</p>

<p>Going back to the case of hand-written digits classification, where we extracted two features: symmetry and intensity. We will sample 500 points for training and the rest for testing the hypothesis. The non-linear transformation is 5th-order:</p>

<p><script type="math/tex; mode=display"> (1, x_1, x_2) \to (1, x_1, x_2, x^2_1, x_1 x_2, x_2^2, x_1^3, x_1^2 x_2, \dots, x_1^5, x_1^4 x_2, x_1^3 x_2^2, x_1^2 x_2^3, x_1 x_2^4, x_2^5) </script></p>

<p><img src="/images/notes/machine-learning/validation/digits-classification.png" class="center"></p>

<p>In this case, we would like to use cross-validation to determine at which point to cut off the non-linear transformation, that is how many terms to use, where each term in the non-linear transformation is considered a separate model, for a total of <script type="math/tex">20</script> different models. Here are the different errors derived based on the number of features used:</p>

<p><img src="/images/notes/machine-learning/validation/digits-classification-2.png" class="center"></p>

<p>From this graph it seems that the best number of features to use is 6. Without validation (left) we can observe overfitting with no in-sample error and <script type="math/tex">2.5\%</script> out-of sample error, whereas with validation (right) provides a smooth surface with non-zero in-sample error but lower out-of-sample error at <script type="math/tex">1.5\%</script>.</p>

<div style="text-align: center; margin-top: 10px">
  <img src="/images/notes/machine-learning/validation/without-validation.png">
  <img src="/images/notes/machine-learning/validation/with-validation.png">
</div>
<h2 id="cross-validation-optimization">
<span class="hash">#</span>
<a href="#cross-validation-optimization" class="header-link">Cross-Validation Optimization</a>
</h2>
<p>When we use leave one out, we have <script type="math/tex">N</script> training sessions on <script type="math/tex">N - 1</script> points each. We would like to use more points for validation. We take the data and break it into different folds, such as 10 folds, making it 10-fold cross-validation. We use one fold for validation and the other 9 for training. In other words, it&#39;s similar to leave one out except instead of leaving one point out we leave out a chunk/fold.</p>

<p><img src="/images/notes/machine-learning/validation/leave-more-than-one-out.png" class="center"></p>

<p>This results in <script type="math/tex">\frac N K</script> training sessions on <script type="math/tex">N - K</script> points each. Specifically, <em>10-fold cross validation</em> is very useful in practice.</p>
<h1 id="support-vector-machines">
<span class="hash">#</span>
<a href="#support-vector-machines" class="header-link">Support Vector Machines</a>
</h1>
<p>Going back to the concept of <a href="#dichotomies">dichotomies</a>, when we wanted to determine the number of ways in which a model could separate data in different configurations. With the linear model, we ignored the actual position and angle of the line that separated the data, only caring for different resultant configurations.</p>

<p>However, if we did care about the position and angle of the line, it could be said that a distinguishing factor of every such line position and angle would be its margin of error, that is, its distance between the data it separates. A higher margin of error makes it more tolerant to out-of-sample data, due to noise for example. We can enforce a lower bound requirement on the margin of error, which naturally would result in less dichotomies, as it places a restriction on the growth function, yielding a smaller VC dimension:</p>

<p><img src="/images/notes/machine-learning/support-vector-machines/margin-dichotomies.png" class="center"></p>

<p>In the scenario in which we would like to enforce a lower bound on the margin of error, the problem of learning then becomes that of finding the <script type="math/tex">\weight</script> that maximizes the margin.</p>

<p>To find the <script type="math/tex">\weight</script> with a large margin we first pick <script type="math/tex">\feature_n</script> that is the nearest data point to the hyperplane <script type="math/tex">\weightT \feature = 0</script>. We would like to determine the distance between the point and this hyperplane, which would represent the margin.</p>

<p>First we normalize <script type="math/tex">\weight</script> with the following constraint, where <script type="math/tex">\feature_n</script> again represents the nearest point to the hyperplane:</p>

<p><script type="math/tex; mode=display">|\weightT \feature_n| = 1</script></p>

<p>Second we have to take out <script type="math/tex">w_0</script>, the threshold/bias weight, from now on referred to as <script type="math/tex">b</script>:</p>

<p><script type="math/tex; mode=display">\weight = (w_1, \dots, w_d)</script></p>

<p>Now the hyperplane is defined as follows, with <script type="math/tex">x_0</script> absent since it was always <script type="math/tex">1</script> and <script type="math/tex">w_0</script> was taken out as <script type="math/tex">b</script>:</p>

<p><script type="math/tex; mode=display">\weightT \feature + b = 0</script></p>

<p>We can now compute the distance between <script type="math/tex">\feature_n</script> and the plane <script type="math/tex">\weightT \feature + b = 0</script> where <script type="math/tex">|\weightT \feature_n + b| = 1</script>. First we must realize that the vector <script type="math/tex">\weight</script> is perpendicular to the plane in <script type="math/tex">\mathcal X</script>-space. For example, pick any <script type="math/tex">\feature'</script> and <script type="math/tex">\feature''</script> on the plane. Since they&#39;re on the plane, they must satisfy the plane equation so that:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\weightT \feature' + b &= 0 \\
\weightT \feature'' + b &= 0 \\
\Longrightarrow \weightT (\feature' - \feature'') &= 0
\end{align}
</script></p>

<p>The conclusion from the above is that <script type="math/tex">\weight</script> is orthogonal to any vector on the plane.</p>

<p><img src="/images/notes/machine-learning/support-vector-machines/plane.png" class="center"></p>

<p>The distance between the nearest point <script type="math/tex">\feature_n</script> and the plane can be computed by taking any point <script type="math/tex">\feature</script> on the plane and computing the projection of the vector formed by <script type="math/tex">\feature_n - \feature</script> on <script type="math/tex">\weight</script>. In order to get this projection, we need the unit vector of <script type="math/tex">\weight</script>:</p>

<p><script type="math/tex; mode=display">\hat \weight = \frac \weight {\lVert \weight \rVert}</script></p>

<p>So that now the distance can be computed as:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\text {distance } &= |\hat \weight \cdot (\feature_n - \feature)| \\
\text {distance } &= \frac 1 {\lVert \weight \rVert}
|\weightT \feature_n - \weightT \feature| \\
\text {distance } &= \frac 1 {\lVert \weight \rVert}
|\weightT \feature_n + b - \weightT \feature - b| \\
\text {distance } &= \frac 1 {\lVert \weight \rVert}
\end{align}
</script></p>

<p>The last component of the penultimate equation disappears because the equation for a point on the plane is defined to be <script type="math/tex">0</script>, and we already restricted the equation for <script type="math/tex">\feature_n</script> to be equal to <script type="math/tex">1</script>.</p>

<p><img src="/images/notes/machine-learning/support-vector-machines/distance.png" class="center"></p>

<p>The resulting optimization problem is to maximize the margin, denoted as <script type="math/tex">\smash {\frac 1 {\lVert \weight \rVert}}</script> subject to the following constraint:</p>

<p><script type="math/tex; mode=display">
\min_{n = 1, 2, \dots, N} |\weightT \feature_n + b| = 1
</script></p>

<p>However, this is not a friendly optimization problem because it has a <script type="math/tex">\min</script> in it. To get rid of the absolute value, notice that:</p>

<p><script type="math/tex; mode=display">|\weightT \feature_n + b| = y_n(\weightT \feature_n + b)</script></p>

<p>Now instead of maximizing <script type="math/tex">\smash {\frac 1 {\lVert \weight \rVert}}</script> we will minimize the following quantity:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\text {minimize }\colon& \frac 1 2 \weightT \weight \\
\text {subject to }\colon& y_n(\weightT \feature_n + b) \geq 1 \quad \text {for } n = 1, 2, \dots, N \\
\end{align}
</script></p>

<p>The domain of this optimization problem is <script type="math/tex">\weight \in \mathbb R^d, b \in \mathbb R</script>. This is therefore a constrained optimization problem, for which we can use Lagrange, but the problem is that these are inequality constraints not equality constraints.</p>

<p>We saw this scenario before with regularization, where we minimized <script type="math/tex">\insample(\weight)</script> under a constraint:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\text {minimize:} &\insample(\weight) = \frac 1 N (\mathbf {Zw - y})^\intercal (\mathbf {Zw - y}) \\[5 pt]
&\text {subject to:} \quad \weightT \weight \leq C
\end{align}
</script></p>

<p>We <a href="#solution-vectors">found</a> that <script type="math/tex">\nabla \insample</script> was normal to the constraint. This presents a conceptual dichotomy between regularization and SVM. In regularization, we optimize the in-sample error <script type="math/tex">\insample</script> under the constraint <script type="math/tex">\weightT \weight</script>. Conversely, with SVM we are optimizing <script type="math/tex">\weightT \weight</script> under the constraint that <script type="math/tex">\insample = 0</script>.</p>

<table>
<thead>
<tr>
<th style="text-align: left">method</th>
<th style="text-align: left">optimize</th>
<th style="text-align: left">constraint</th>
</tr>
</thead>

<tbody>
<tr>
<td style="text-align: left">Regularization</td>
<td style="text-align: left"><script type="math/tex">\insample</script></td>
<td style="text-align: left"><script type="math/tex">\weightT \weight</script></td>
</tr>
<tr>
<td style="text-align: left">SVM</td>
<td style="text-align: left"><script type="math/tex">\weightT \weight</script></td>
<td style="text-align: left"><script type="math/tex">\insample</script></td>
</tr>
</tbody>
</table>
<h2 id="lagrange-formulation">
<span class="hash">#</span>
<a href="#lagrange-formulation" class="header-link">Lagrange Formulation</a>
</h2>
<p>To recap, we are performing the following constrained optimization:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\text {minimize }\colon& \frac 1 2 \weightT \weight \\
\text {subject to }\colon& y_n(\weightT \feature_n + b) \geq 1
\end{align}
</script></p>

<p>We first take the inequality constraint and put it in zero-form, so that:</p>

<p><script type="math/tex; mode=display">\alpha_n (\underbrace {y_n(\weightT \feature_n + b) - 1}_{\text {slack}})</script></p>

<p>This is then combined with the optimization component to form the full optimization problem, which is a Lagrangian <script type="math/tex">\mathcal L</script> dependent on <script type="math/tex">\weight</script>, <script type="math/tex">b</script>, and the Lagrange multipliers <script type="math/tex">\alpha</script>:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\text {minimize}\colon &\mathcal L(\weight, b, \mathbf \alpha) = \frac 1 2 \weightT \weight - \sum_{n = 1}^N \alpha_n (y_n(\weightT \feature_n + b) - 1) \\
&\text {w.r.t. } \weight \text { and } b, \text { and maximize w.r.t. each } \alpha_n \geq 0
\end{align}
</script></p>

<p>Now we can find the gradient of the Lagrangian with respect to the weight, which we want to minimize by setting it to the zero-vector:</p>

<p><script type="math/tex; mode=display">\nabla_\weight \mathcal L = \weight - \sum_{n = 1}^N \alpha_n y_n \feature_n = \mathbf 0</script></p>

<p>Now we find the derivative of the Lagrangian with respect to <script type="math/tex">b</script> in order to minimize it by setting it to zero:</p>

<p><script type="math/tex; mode=display">\frac {\partial \mathcal L} {\partial b} = - \sum_{n = 1}^N \alpha_n y_n</script></p>

<p>We now want to combine these optimization components in order to remove the <script type="math/tex">\weight</script> and <script type="math/tex">b</script> from the optimization problem, so that it&#39;s instead only a function of <script type="math/tex">\alpha</script>. This is referred to as the <em>dual formulation</em> of the problem:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\mathcal L(\mathbf \alpha) = &\sum_{n = 1}^N \alpha_n - \frac 1 N \sum_{n = 1}^N \sum_{m = 1}^N y_n y_m \alpha_n \alpha_m \feature_n^\intercal \feature_m \\ \\
&\text {maximize w.r.t. } \mathbf \alpha \\
&\text {subject to } \alpha_n \geq 0 \text { for } n = 1, \dots, N
\text { and } \smash {\sum_{n = 1}^N} \alpha_n y_n = 0
\end{align}
</script></p>

<p>We still have to get rid of the constraints in order to have a pure optimization problem that we can pass on to some external <a href="http://en.wikipedia.org/wiki/Quadratic_programming">quadratic programming</a> library. First of all, most quadratic programming packages perform minimizations not maximizations, so we have to convert it to a minimization, which is accomplished by simply negating the equation:</p>

<p><script type="math/tex; mode=display">
\max_\alpha \sum_{n = 1}^N \alpha_n - \frac 1 N \sum_{n = 1}^N \sum_{m = 1}^N y_n y_m \alpha_n \alpha_m \feature_n^\intercal \feature_m \\
\min_\alpha \frac 1 N \sum_{n = 1}^N \sum_{m = 1}^N y_n y_m \alpha_n \alpha_m \feature_n^\intercal \feature_m - \sum_{n = 1}^N \alpha_n
</script></p>

<p>Next we want to isolate the coefficients from the <script type="math/tex">\mathbf \alpha</script> values, since the <script type="math/tex">\mathbf \alpha</script> are parameters. What the quadratic programming library receives are the coefficients of our particular problem <script type="math/tex">y</script> and <script type="math/tex">\feature</script> and produces the <script type="math/tex">\mathbf \alpha</script> values that minimize the above equation. This is accomplished <span id="quadratic-coefficients">as follows</span>:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\min_{\mathbf \alpha}\ &\frac 1 2 {\mathbf \alpha}^\intercal
\underbrace {\begin{bmatrix}
y_1 y_1 \feature_1^\intercal \feature_1 & y_1 y_2 \feature_1^\intercal \feature_2 & \dots & y_1 y_N \feature_1^\intercal \feature_N \\
y_2 y_1 \feature_2^\intercal \feature_1 & y_2 y_2 \feature_2^\intercal \feature_2 & \dots & y_2 y_N \feature_2^\intercal \feature_N \\
\dots & \dots & \dots & \dots \\
y_N y_1 \feature_N^\intercal \feature_1 & y_N y_2 \feature_N^\intercal \feature_2 & \dots & y_N y_N \feature_N^\intercal \feature_N \\
\end{bmatrix}}_{\text {quadratic coefficients}}
\mathbf \alpha + \underbrace {(-\mathbf 1^\intercal) \mathbf \alpha}_{\text {linear coefficients}} \\ \\
&\text {subject to}\colon \underbrace {\mathbf y^\intercal \mathbf \alpha = 0}_{\text {linear constraint}}, \quad \underbrace {0 \leq \mathbf \alpha \leq \infty}_{\text {lower and upper bounds}}
\end{align}
</script></p>

<p>The matrix in this equation as well as the linear coefficients are passed to the quadratic programming library. The above equation is equivalent to:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\min_{\mathbf \alpha}\ &\frac 1 2 \mathbf \alpha^\intercal Q \mathbf \alpha - \mathbf 1^\intercal \mathbf \alpha \\ \\
&\text {subject to } \mathbf y^\intercal \mathbf {\alpha} = 0, \quad \mathbf \alpha \geq \mathbf 0
\end{align}
</script></p>

<p>It&#39;s important to note the implications of the number of examples, which clearly yields a larger matrix, in which case there are a variety of heuristics that mitigate this problem.</p>

<p>After being fed the quadratic and linear coefficients, the quadratic programming library produces the vector <script type="math/tex">\mathbf {\alpha} = \alpha_1, \dots, \alpha_N</script>, from which we need to determine the value of <script type="math/tex">\weight</script>, <script type="math/tex">b</script>, the margin, and so on. The weights can be computed as based on the previous constraint that concerned it:</p>

<p><script type="math/tex; mode=display">\weight = \sum_{n = 1}^N \alpha_n y_n \feature_n</script></p>

<p>It might be surprising to realize that the majority of the <script type="math/tex">\mathbf \alpha</script> vector consists of zero elements (interior points). This is due to a <a href="KKT">Karush-Kuhn-Tucker</a> condition that&#39;s relevant here that says that <em>either</em> the slack is zero or the Lagrange multiplier is zero, such that their product is definitely zero:</p>

<p><script type="math/tex; mode=display">
\text {For } n = 1, \dots, N \\
\alpha_n(y_n(\weightT \feature_n + b) - 1) = 0
</script></p>

<p>We saw this before in <a href="#solution-visualization">regularization</a>, where we had a constraint which was to be within the red circle while optimizing a function represented by the blue circle. When the constraint didn&#39;t really constraint us and the absolute optimal was within the blue circle, there was no need for regularization and so <script type="math/tex">\lambda = 0</script>.</p>

<p>The points which define the plane and margin are the ones for which the <script type="math/tex">\alpha</script> values are positive, and all of these values form a <em>support vector</em>:</p>

<p><script type="math/tex; mode=display">\alpha_n > 0 \Longrightarrow \feature_n \text { is a } \textbf {support vector}</script></p>

<p>That is, we had <script type="math/tex">N</script> points which we classified and for which we found the maximum margin, and the margin touched some of the <script type="math/tex">+1</script> and some of the <script type="math/tex">-1</script> points, and it is said that these points &quot;support the plane,&quot; and are called support vectors, while the rest of the points are interior points. In the image below, the support vectors are those points that touch the extents of the margin, those with a circle around them:</p>

<p><img src="/images/notes/machine-learning/support-vector-machines/support-vectors.png" class="center"></p>

<p>Since most <script type="math/tex">\alpha</script> are zero, the weight calculation can be changed to only sum those in the support vector:</p>

<p><script type="math/tex; mode=display">\weight = \sum_{n = 1}^N \alpha_n y_n \feature_n = \sum_{\feature_n \text { is SV}} \alpha_n y_n \feature_n</script></p>

<p>We can also solve for the bias <script type="math/tex">b</script> corresponding to threshold term using <strong><em>any</em></strong> support vector, for which we already know that:</p>

<p><script type="math/tex; mode=display">y_n(\weightT \feature_n + b) = 1</script></p>
<h2 id="svm-non-linear-transformations">
<span class="hash">#</span>
<a href="#svm-non-linear-transformations" class="header-link">SVM Non-Linear Transformations</a>
</h2>
<p>Although all of this so far has only handled situations where the data is linearly separable, it&#39;s possible to attempt to solve it using non-linear transformations in the <script type="math/tex">\mathcal Z</script>-space as we did with perceptrons.</p>

<p><script type="math/tex; mode=display">
\mathcal L(\mathbf \alpha) = \sum_{n = 1}^N \alpha_n - \frac 1 N \sum_{n = 1}^N \sum_{m = 1}^N y_n y_m \alpha_n \alpha_m \feature_n^\intercal \feature_m \\
</script></p>

<p>Observing the equation to maximize, we can see that there are only two components which may need to change in this transition between the <script type="math/tex">\mathcal X</script>-space and the <script type="math/tex">\mathcal Z</script>-space.</p>

<p><script type="math/tex; mode=display">
\mathcal L(\mathbf \alpha) = \sum_{n = 1}^N \alpha_n - \frac 1 N \sum_{n = 1}^N \sum_{m = 1}^N y_n y_m \alpha_n \alpha_m \mathbf {z}_n^\intercal \mathbf {z}_m
</script></p>

<p>The increase in dimensionality resulting from the non-linear transformation has no effect on the complexity of the optimization. The quadratic optimization process results in the <script type="math/tex">\alpha</script> vector, which is interpreted in the original <script type="math/tex">\mathcal X</script>-space.</p>

<p>During a non-linear transformation, the support vectors resulting from the optimization &quot;live&quot; in <script type="math/tex">\mathcal Z</script>-space. In <script type="math/tex">\mathcal X</script>-space, these same points can be thought of as &quot;pre-images&quot; of support vectors, those with a circle around them:</p>

<p><img src="/images/notes/machine-learning/support-vector-machines/x-space-non-linear-svm.png" class="center"></p>

<p>It&#39;s important to remember that the distance between the points in <script type="math/tex">\mathcal X</script>-space and the resulting curve are <em>not</em> the margin, since the margin is maintained only in <script type="math/tex">\mathcal Z</script>-space.</p>

<p>What&#39;s interesting is that even if the non-linear transformation involved a million-dimensional transformation, we ended up with four support vectors, which effectively corresponds to having four parameters, so that generalization applies to the four parameters and not the entire million. This means we&#39;re able to use the flexibility of a higher dimensional representation without sacrificing generalization. In other words, we have a complex hypothesis <script type="math/tex">h</script>, but a simple hypothesis set <script type="math/tex">\mathcal H</script>. Therefore, the generalization result is:</p>

<p><script type="math/tex; mode=display">\mathbb E[\outsample] \leq \frac {\mathbb E[\text {# of SVs}]} {N - 1}</script></p>
<h1 id="kernel-methods">
<span class="hash">#</span>
<a href="#kernel-methods" class="header-link">Kernel Methods</a>
</h1>
<p>We want to go to the <script type="math/tex">\mathcal Z</script>-space without paying the price for it. We saw previously that going to the <script type="math/tex">\mathcal Z</script>-space involved a very simple dot product:</p>

<p><script type="math/tex; mode=display">
\mathcal L(\mathbf \alpha) = \sum_{n = 1}^N \alpha_n - \frac 1 N \sum_{n = 1}^N \sum_{m = 1}^N y_n y_m \alpha_n \alpha_m \mathbf {z}_n^\intercal \mathbf {z}_m
</script></p>

<p>We would like to see if we could do away with this relatively low cost altogether. The constraints for this equation are:</p>

<p><script type="math/tex; mode=display">
\text {constraints: } \alpha_n \geq 0 \text { for } n = 1, \dots, N \text { and } \sum_{n = 1}^N \alpha_n y_n = 0
</script></p>

<p>Notice that <script type="math/tex">\mathbf z</script> isn&#39;t present in the constraints. The hypothesis is essentially:</p>

<p><script type="math/tex; mode=display">
\begin{align}
g(\feature) &= \sign(\weightT \mathbf z + b) \\
\text {where } \weight &= \sum_{\mathbf z_n \text { is SV}} \alpha_n y_n \mathbf z_n \\
\text {and } b &\colon y_n(\weightT \mathbf z_m + b) = 1
\end{align}
</script></p>

<p>We can then substitute the weight&#39;s definition into the hypothesis function, in which case we require the inner products consisting of <script type="math/tex">\mathbf z_n^\intercal \mathbf z</script>. We can do the same exact thing to determine the value of <script type="math/tex">b</script>, in which case we need <script type="math/tex">\mathbf z_n^\intercal \mathbf z_m</script>. In effect, this means that we only deal with <script type="math/tex">\mathbf z</script> as far as the inner product in the hypothesis function is concerned.</p>
<h2 id="kernel-trick">
<span class="hash">#</span>
<a href="#kernel-trick" class="header-link">Kernel Trick</a>
</h2><h3 id="polynomial-kernel">
<span class="hash">#</span>
<a href="#polynomial-kernel" class="header-link">Polynomial Kernel</a>
</h3>
<p>Given two points <script type="math/tex">\feature</script> and <script type="math/tex">\feature' \in \mathcal X</script>, we need <script type="math/tex">\mathbf z^\intercal \mathbf z'</script>. Let <script type="math/tex">\mathbf z^\intercal \mathbf z' = K(\feature, \feature')</script>, where <script type="math/tex">K</script> is referred to as the <em>kernel</em>, which corresponds to some <script type="math/tex">\mathcal Z</script>-space, it is considered the &quot;inner product&quot; of <script type="math/tex">\feature</script> and <script type="math/tex">\feature'</script> after some transormation. For example <script type="math/tex">\feature = (x_1, x_2)</script> undergoes a 2nd-order non-linear transformation:</p>

<p><script type="math/tex; mode=display">\mathbf z = \Phi(\feature) = (1, x_1, x_2, x_1^2, x_2^2, x_1 x_2)</script></p>

<p>So the kernel would be defined as:</p>

<p><script type="math/tex; mode=display">
K(\feature, \feature') = \mathbf {z^\intercal z'} =
1 + x_1 {x'}_1 + x_2 {x'}_2 + x_1^2 {x'}_1^2 + x_2^2 {x'}_2^2 + x_1 {x'}_1 x_2 {x'}_2
</script></p>

<p>Now the question is whether or not we can transform the kernel <script type="math/tex">K(\feature, \feature')</script> <strong>without</strong> transforming <script type="math/tex">\feature</script> and <script type="math/tex">\feature'</script> into the <script type="math/tex">\mathcal Z</script>-space. Consider the following:</p>

<p><script type="math/tex; mode=display">
\begin{align}
K(\feature, \feature') &= (1 + \feature^\intercal \feature')^2 = (1 + x_1 {x'}_1 + x_2 {x'}_2)^2 \\
&= 1 + x_1^2 {x'}_1^2 + x_2^2 {x'}_2^2 + 2 x_1 {x'}_1 + 2 x_2 {x'}_2 + 2 x_1 {x'}_1 x_2 {x'}_2
\end{align}
</script></p>

<p>This looks very similar to an inner product if it weren&#39;t for the twos, in which case it&#39;d be as if we transformed to the 2nd-order polynomial and took the inner product there. However, this <em>is</em> an inner product if we say that the transformation that we are using is:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\Phi(\feature) &= (1, x_1^2, x_2^2, \sqrt {2 x_1}, \sqrt {2 x_2}, \sqrt 2 x_1 x_2) \\
\Phi(\feature') &= (1, {x'}_1^2, {x'}_2^2, \sqrt {2 {x'}_1}, \sqrt {2 {x'}_2}, \sqrt 2 {x'}_1 {x'}_2)
\end{align}
</script></p>

<p>The advantage of the kernel is that we don&#39;t have to take each feature and expand it per the non-linear transformation in order to perform the inner product, instead we directly compute the inner product from the <script type="math/tex">\mathcal X</script>-space feature vector.</p>

<p>More specifically, this is called the <em>polynomial kernel</em>. Generally, given a general d-dimensional space <script type="math/tex">\mathcal X = \mathbb R^d</script> with a transformation of that space into a Qth-order polynomial <script type="math/tex">\Phi\colon \mathcal {X \to Z}</script>, the &quot;equivalent&quot; kernel would be:</p>

<p><script type="math/tex; mode=display">
\begin{align}
K(\feature, \feature') &= (1 + \feature^\intercal \feature')^Q \\
&= (1 + x_1 {x'}_1 + x_2 {x'}_2 + \cdots + x_d {x'}_d)^Q \\
&= (a \feature^\intercal \feature' + b)^Q
\end{align}
</script></p>

<p>A kernel of the above form corresponds to an inner product in a higher space. So now all we have to do is use the <script type="math/tex">\mathcal X</script>-space feature vectors, compute the dot product, and raise that value to some exponent, instead of having to explicitly go into the <script type="math/tex">\mathcal Z</script>-space. This is all guaranteed if the kernel <script type="math/tex">K(\feature, \feature')</script> is an inner product in <em>some</em> space <script type="math/tex">\mathcal Z</script>-space. Consider the following kernel:</p>

<p><script type="math/tex; mode=display">K(\feature, \feature') = \exp\left(-\gamma\ \lVert \feature - \feature' \rVert^2\right)</script></p>

<p>This kernel is clearly a function of <script type="math/tex">\feature</script> and <script type="math/tex">\feature'</script>. It doesn&#39;t have a clear inner product. Does this correspond to an inner product in <script type="math/tex">\mathcal Z</script>-space? Indeed it is, if the <script type="math/tex">\mathcal Z</script>-space is treated as being <em>infinite-dimensional</em>. In this case we would get the full benefit of a non-linear transformation, without worrying about the generalization ramifications of going to an infinite-dimensional space. For example, take a simple case where the kernel is applied to a one-dimensional <script type="math/tex">\mathcal X</script>-space and <script type="math/tex">\gamma = 1</script>:</p>

<p><script type="math/tex; mode=display">
\begin{align}
K(x, x') &= \exp\left(-(x - x')^2\right) \\
&= \exp(-x^2) \exp(-{x'}^2) \exp(2 x x') \\
&= \exp(-x^2) \exp(-{x'}^2) \underbrace {\sum_{k = 0}^\infty \frac {2^k (x)^k (x')^k} {k!}}_{\text {Taylor Series}} \\
\end{align}
</script></p>

<p>The <script type="math/tex">x</script> terms can be grouped together and so can the <script type="math/tex">x'</script> terms, and ultimately this equation can be transformed to look like an inner product in an infinite space, due to the infinite summation. This kernel is called the <em>radial basis function</em> (RBF) kernel.</p>

<p>To see it in action, in the case of a slightly non-separable case in the <script type="math/tex">\mathcal X</script>-space with a slightly curvy target function. We transform this dataset into an <script type="math/tex">\infty</script>-dimensional <script type="math/tex">\mathcal Z</script>-space. The red and blue points are the support vectors (9 total; 4 blue, 5 red), the lime curve is the target function and the black curve is the <script type="math/tex">\mathcal Z</script>-space hyperplane:</p>

<p><img src="/images/notes/machine-learning/kernel-methods/radial-basis-function-example.png" class="center"></p>

<p>Here we were able to visit an infinite-dimensional space and yield only 9 support vectors, which thanks to the error bound we developed earlier means that the out-of-sample error must be less than about <script type="math/tex">10\%</script>. Previously, going to an infinite or even higher order <script type="math/tex">\mathcal Z</script>-space was considered overkill, but that determination is now made based on the number of resultant support vectors.</p>

<p>An important thing to remember is that the distances between the support vectors and the <script type="math/tex">\mathcal X</script>-space hyperplane will possibly not be minimal, because that distance that was solved for was solved in <script type="math/tex">\mathcal Z</script>-space not <script type="math/tex">\mathcal X</script>-space. The support vectors are not support vectors per se, but rather they are &quot;pre-images&quot; of the actual support vectors in the <script type="math/tex">\mathcal Z</script>-space.</p>
<h3 id="kernel-formulation-of-svm">
<span class="hash">#</span>
<a href="#kernel-formulation-of-svm" class="header-link">Kernel Formulation of SVM</a>
</h3>
<p>In solving support vector machines, we would <a href="#quadratic-coefficients">pass the inner products</a> of the feature vectors to the quadratic programming library. All that changes now is that we instead pass the result of the kernel function <script type="math/tex">K</script>; <strong>everything else remains the same</strong>:</p>

<p><script type="math/tex; mode=display">
\underbrace {\begin{bmatrix}
y_1 y_1 K(\feature_1, \feature_1) & y_1 y_2 K(\feature_1, \feature_2) & \dots & y_1 y_N K(\feature_1, \feature_N) \\
y_2 y_1 K(\feature_2, \feature_1) & y_2 y_2 K(\feature_2, \feature_2) & \dots & y_2 y_N K(\feature_2, \feature_N) \\
\dots & \dots & \dots & \dots \\
y_N y_1 K(\feature_N, \feature_1) & y_N y_2 K(\feature_N, \feature_2) & \dots & y_N y_N K(\feature_N, \feature_N) \\
\end{bmatrix}}_{\text {quadratic coefficients}}
</script></p>

<p>When we receive the <script type="math/tex">\alpha</script> and we need to construct the hypothesis in terms of the kernel. In other words, we want to express <script type="math/tex">g(\feature) = \sign(\weightT \mathbf z + b)</script> in terms of <script type="math/tex">K(-, -)</script>. First remember the definition of the weight:</p>

<p><script type="math/tex; mode=display">
\weight = \sum_{\mathbf z_n \text { is SV}} \alpha_n y_n \mathbf z_n
</script></p>

<p>So now we can construct the hypothesis, which is also the general SVM model, which differs based on the kernel function:</p>

<p><script type="math/tex; mode=display">
\begin{align}
g(\feature) &= \sign\left( \sum_{\alpha_n > 0} \alpha_n y_n K(\feature_n, \feature) + b \right) \\
\text {where } b &= y_m - \sum_{\alpha_n > 0} \alpha_n y_n K(\feature_n, \feature_m)
\end{align}
</script></p>
<h3 id="kernel-function-validity">
<span class="hash">#</span>
<a href="#kernel-function-validity" class="header-link">Kernel Function Validity</a>
</h3>
<p>How can we determine that a kernel is valid, i.e. that it corresponds to an inner product in <em>some</em> space, <em>without</em> visiting that space? In other words, how do we know that <script type="math/tex">\mathcal Z</script> exists for a given <script type="math/tex">K(\feature, \feature')</script>?</p>

<p>There are three approaches to come up with a valid kernel. The first method is by construction. The second is by using mathematical properties of the kernel (i.e. Mercer&#39;s condition), which have already been applied to kernel functions other people have developed which can be applied to our own problems. The third approach is to simply not care whether or not <script type="math/tex">\mathcal Z</script> exists, which is an approach taken by a number of people but for which Professor Yaser Abu-Mostafa has reservations, since the guarantees of the method depend on <script type="math/tex">\mathcal Z</script> existing.</p>

<p>To take the second approach for the purposes of designing our own kernel. A kernel <script type="math/tex">K(\feature, \feature')</script> is a valid kernel iff:</p>

<ol>
<li>It is symmetric so that <script type="math/tex">K(\feature, \feature') = K(\feature', \feature)</script></li>
<li>The matrix of all possible pairs of feature vectors applied to the kernel function is <em>positive semi-definite</em>, that is, the matrix should be greater than or equal to 0, for any <script type="math/tex">\feature_1, \dots, \feature_N</script>. This is known as <em>Mercer&#39;s condition</em>:</li>
</ol>

<p><script type="math/tex; mode=display">
\underbrace {\begin{bmatrix}
K(\feature_1, \feature_1) & K(\feature_1, \feature_2) & \dots & K(\feature_1, \feature_N) \\
K(\feature_2, \feature_1) & K(\feature_2, \feature_2) & \dots & K(\feature_2, \feature_N) \\
\dots & \dots & \dots & \dots \\
K(\feature_N, \feature_1) & K(\feature_N, \feature_2) & \dots & K(\feature_N, \feature_N) \\
\end{bmatrix}}_{\text {quadratic coefficients}}
</script></p>
<h2 id="soft-margin-svm">
<span class="hash">#</span>
<a href="#soft-margin-svm" class="header-link">Soft-Margin SVM</a>
</h2>
<p>There are two main kinds of non-separable situations. Slightly non-separable (left) is when it might be beneficial to simply accept that as error using something like the pocket algorithm, as opposed to sacrificing generalization in order to capture what are potentially simply outliers. Seriously non-separable (right) is when it&#39;s not a matter of outliers, but that the data is simply non-linearly separable.</p>

<div style="text-align: center; margin-top: 10px">
  <img src="/images/notes/machine-learning/kernel-methods/slightly-non-separable.png">
  <img src="/images/notes/machine-learning/kernel-methods/seriously-non-separable.png">
</div>

<p>Slightly-separable data can be handled by soft-margin SVM, while seriously non-separable data can be handled by kernel functions. In real world data sets, however, it&#39;s likely that there will be elements of both: built-in non-linearity with outliers. For this reason, we would be combining kernels with soft-margin SVMs in almost all of the problems that we would encounter.</p>
<h3 id="soft-margin-svm-error-measure">
<span class="hash">#</span>
<a href="#soft-margin-svm-error-measure" class="header-link">Soft-Margin SVM Error Measure</a>
</h3>
<p>The error measure will be defined by a so-called margin violation, which could be the point within the margin in the image below. Even though it is correctly classified by the hyperplane, it&#39;s considered a <em>margin violation</em> because it falls within the margin.</p>

<p><img src="/images/notes/machine-learning/kernel-methods/margin-violation.png" class="center"></p>

<p>It can be said that the margin is violated when <script type="math/tex">y_n(\weightT \feature_n + b) \geq 1</script> fails. This failure can be quantified by introducing a &quot;slack&quot; <script type="math/tex">\xi_n \geq 0</script> for every point so that:</p>

<p><script type="math/tex; mode=display">y_n(\weightT \feature_n + b) \geq 1 - \xi_n</script></p>

<p>The total violation made by a derived SVM can be expressed as the sum of the slacks, since those that don&#39;t violate the margin will have <script type="math/tex">\xi = 0</script>:</p>

<p><script type="math/tex; mode=display">\text {Total violation } = \sum_{n = 1}^N \xi_n</script></p>
<h3 id="soft-margin-svm-optimization">
<span class="hash">#</span>
<a href="#soft-margin-svm-optimization" class="header-link">Soft-Margin SVM Optimization</a>
</h3>
<p>The previous optimization that we performed was to minimize the following quantity, which had the effect of maximizing the margin:</p>

<p><script type="math/tex; mode=display">\frac 1 2 \weightT \weight</script></p>

<p>Now we are going to add an error term that corresponds to the violation of the margin:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\text {minimize } &\frac 1 2 \weightT \weight + C \sum_{n = 1}^N \xi_n \\
\text {subject to } &y_n(\weightT \feature_n + b) \geq 1 - \xi_n \quad \text { for } n = 1, \dots, N \\ \\
&\text {and } \xi_n \geq 0 \quad \text { for } n = 1, \dots, N \\
&\weight \in \mathbb R^d, b \in \mathbb R, \xi \in \mathbb R^N
\end{align}
</script></p>

<p>In this new optimization problem, <script type="math/tex">C</script> is a constant that represents the relative importance of the error versus the margin, similar to augmented error. For example, a very high value of <script type="math/tex">C</script> represents that error cannot be afforded, whereas a very low value of <script type="math/tex">C</script> means that we can have a large margin but it would have a higher rate of errors.</p>

<p>The Lagrange formulation for this optimization problem can thus be derived as follows, where the first equation is the original Lagrange formulation:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\mathcal L(\weight, b, \phantom {\xi}, \alpha, \phantom {\beta}) &= \frac 1 2 \weightT \weight \phantom {+ C \sum_{n = 1}^N \xi_n}
- \sum_{n = 1}^N \alpha_n(y_n(\weightT \feature_n + b) - 1 \phantom {+ \xi_n})
\phantom {- \sum_{n = 1}^N \beta_n \xi_n} \\ \\
\mathcal L(\weight, b, \xi, \alpha, \beta) &= \frac 1 2 \weightT \weight + C \sum_{n = 1}^N \xi_n
- \sum_{n = 1}^N \alpha_n(y_n(\weightT \feature_n + b) - 1 + \xi_n)
- \sum_{n = 1}^N \beta_n \xi_n \\ \\
&\text {minimize w.r.t. } \weight, b, \text { and } \xi \\
&\text {maximize w.r.t. each } \alpha_n \geq 0 \text { and } \beta_n \geq 0
\end{align}
</script></p>

<p>The minimization with respect to <script type="math/tex">\weight</script> is the same as before:</p>

<p><script type="math/tex; mode=display">\nabla_\weight \mathcal L = \weight - \sum_{n = 1}^N \alpha_n y_n \feature_n = \mathbf 0</script></p>

<p>The minimization with respect to <script type="math/tex">b</script> is also the same as before:</p>

<p><script type="math/tex; mode=display">\frac {\partial \mathcal L} {\partial b} = - \sum_{n = 1}^N \alpha_n y_n = 0</script></p>

<p>The minimization with respect to <script type="math/tex">\xi</script> can be derived as follows:</p>

<p><script type="math/tex; mode=display">\frac {\partial \mathcal L} {\partial \xi_n} = C - \alpha_n - \beta_n = 0</script></p>

<p>The ramifications of the final equation is that the Lagrange formulation above reduces back down to the original Lagrange formulation, with the only added constraint that <script type="math/tex">\alpha_n</script> be <strong>at most</strong> <script type="math/tex">C</script>:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\text {Maximize } &\mathcal L(\mathbf \alpha) = \sum_{n = 1}^N \alpha_n - \frac 1 N \sum_{n = 1}^N \sum_{m = 1}^N y_n y_m \alpha_n \alpha_m \feature_n^\intercal \feature_m \quad \text {w.r.t } \alpha \\
\text {Subject to } &0 \leq \alpha_n \leq C\ \text { for } n = 1, \dots, N\ \text { and } \sum_{n = 1}^N \alpha_n y_n = 0
\end{align}
</script></p>

<p>The result of this minimization is the weight:</p>

<p><script type="math/tex; mode=display">\weight = \sum_{n = 1}^N \alpha_n y_n \feature_n</script></p>

<p>This particular weight guarantees that we are minimizing the quantity:</p>

<p><script type="math/tex; mode=display">\frac 1 2 \weightT \weight + C \sum_{n = 1}^N \xi_n</script></p>

<p>In practical terms, well we have to do to use a soft-margin is to ensure that <script type="math/tex">0 \leq \alpha_n \leq C</script>.</p>
<h3 id="types-of-support-vectors">
<span class="hash">#</span>
<a href="#types-of-support-vectors" class="header-link">Types of Support Vectors</a>
</h3>
<p>We call the support vectors that fall on the hyperplane boundary <em>margin support vectors</em> (<script type="math/tex">0 \lt \alpha_n \lt C</script>):</p>

<p><script type="math/tex; mode=display">y_n(\weightT \feature_n + b) = 1 \quad (\xi_n = 0)</script></p>

<p>The other vectors that don&#39;t fall on the hyperplane boundary but are still correctly classified are referred to as <em>non-margin support vectors</em> (<script type="math/tex">\alpha_n = C</script>):</p>

<p><script type="math/tex; mode=display">y_n(\weightT \feature_n + b) \lt 1 \quad (\xi_n \gt 0)</script></p>

<p>The <script type="math/tex">C</script> parameter is determined in practice using <a href="#cross-validation">cross-validation</a>.</p>

<p><img src="/images/notes/machine-learning/kernel-methods/types-of-support-vectors.png" class="center"></p>

<p>There are two final observations.</p>

<p>First, with the hard margin, what happens if the data is not linearly separable? The translation from the &quot;primal&quot; form (minimizing <script type="math/tex">\weightT \weight</script>) to the &quot;dual&quot; form (maximizing <script type="math/tex">\mathcal L(\alpha)</script>) begins to break down. This is mathematically valid only if there is a feasible solution. The point is that we don&#39;t have to worry about this, we don&#39;t have to go through the combinatorial process of determining whether a data set is linearly separable. Instead, we can validate the solution.</p>

<p>Second, when we transform to the <script type="math/tex">\mathcal Z</script>-space, some of these transformations had a constant coordinate of <script type="math/tex">1</script>. This used to correspond to <script type="math/tex">w_0</script>, but we made a point to state that in support vectors there are no <script type="math/tex">w_0</script> values, since we took them out as a separate variable <script type="math/tex">b</script>. If the <script type="math/tex">\mathcal Z</script>-space transformation has a <script type="math/tex">w_0 = 1</script>, then we effectively have two variables doing the same thing. We don&#39;t have to worry about this either though because when we get the solution, all of the corresponding weights will go to <script type="math/tex">0</script>, and the bulk of the bias will go to <script type="math/tex">b</script>.</p>
<h1 id="radial-basis-functions">
<span class="hash">#</span>
<a href="#radial-basis-functions" class="header-link">Radial Basis Functions</a>
</h1>
<p>Radial basis functions serve as the glue between many different machine learning topics. The idea is that every point in the data set <script type="math/tex">(\feature_n, y_n) \in \mathcal D</script> influences the value of the hypothesis <script type="math/tex">h(\feature)</script> at every point <script type="math/tex">\feature</script> based on the distance to nearby points <script type="math/tex">\lVert \feature - \feature_n \rVert</script>. For example, imagine in the bottom image that the top of the bump is <script type="math/tex">\feature_n</script> and the surface represents the influence to other points, and the symmetry of the function shows that this influence is a function of the distance.</p>

<p><img src="/images/notes/machine-learning/radial-basis-functions/rbf-example.png" class="center"></p>

<p>The standard form of of a radial basis function embeds the notion that the closer a point is to the feature point, the more of an influence it has on it:</p>

<p><script type="math/tex; mode=display">h(\feature) = \sum_{n = 1}^N w_n \underbrace {\exp\left( -\gamma\ \lVert \feature - \feature_n \rVert^2 \right)}_{\text {basis function}}</script></p>

<p>The equation shows the source of the <em>basis function</em> part of the name, while the <em>radial</em> part of the name comes from the <script type="math/tex">\lVert \feature - \feature_n \rVert</script>.</p>

<p>Now that we have the model, we can move on to the learning algorithm, where we want to find the parameters <script type="math/tex">w_1, \dots, w_n</script>. First it&#39;s useful to note that we have <script type="math/tex">N</script> parameters <script type="math/tex">w_n</script>, so it shouldn&#39;t be difficult to find parameters such that <script type="math/tex">\insample = 0</script>, so that <script type="math/tex">h(\feature_n) = y_n</script> for <script type="math/tex">n = 1, \dots, N</script>. This constraint can be expressed as follows:</p>

<p><script type="math/tex; mode=display">\sum_{m = 1}^N w_m \exp\left( -\gamma\ \lVert \feature_n - \feature_m \rVert^2 \right) = y_n</script></p>

<p>This shows that there are <script type="math/tex">N</script> equations, one for each data point with <script type="math/tex">N</script> unknown <script type="math/tex">w</script> parameters.</p>

<p><script type="math/tex; mode=display">
\newcommand{\basis}[2]{\exp \left(-\gamma\ \lVert #1 - #2 \rVert^2 \right)}
\underbrace {\begin{bmatrix}
\basis {\feature_1} {\feature_1} & \dots & \basis {\feature_1} {\feature_N} \\
\basis {\feature_2} {\feature_1} & \dots & \basis {\feature_2} {\feature_N} \\
\vdots & \vdots & \vdots \\
\basis {\feature_N} {\feature_1} & \dots & \basis {\feature_N} {\feature_N} \\
\end{bmatrix}}_{\mathbf \Phi}
\underbrace {\begin{bmatrix}
w_1 \vphantom {\Big(} \\
w_2 \vphantom {\Big(} \\
\vdots \\
w_N \vphantom {\Big(}
\end{bmatrix}}_{\weight} =
\underbrace {\begin{bmatrix}
y_1 \vphantom {\Big(} \\
y_2 \vphantom {\Big(} \\
\vdots \\
y_N \vphantom {\Big(}
\end{bmatrix}}_{\mathbf y}
</script></p>

<p>If <script type="math/tex">\mathbf \Phi</script> is invertible, then the solution is simply:</p>

<p><script type="math/tex; mode=display">\weight = \mathbf \Phi^{-1} \mathbf y \quad \text {if } \mathbf \Phi \text { is invertible}</script></p>

<p>This solution can be interpreted as being an <em>exact interpolation</em>, because on the points for which we know the value of the hypothesis, we&#39;re getting the exact output <script type="math/tex">y</script> value.</p>
<h2 id="effect-of-script-typemathtexgammascript">
<span class="hash">#</span>
<a href="#effect-of-script-typemathtexgammascript" class="header-link">Effect of <script type="math/tex">\gamma</script></a>
</h2>
<p>The effect of <script type="math/tex">\gamma</script> is such that if it is small, the Gaussian is a wider curve, whereas if it were larger it&#39;d be a steeper curve. Depending on where the points are---specifically how sparse they are---the steepness of the curve makes a difference.</p>

<p>The image on the left shows the effect of choosing a small <script type="math/tex">\gamma</script>. The three training points are on the curve, since that was what the constraint was. The smaller gray curves are the individual contributions from each of the points, that is, <script type="math/tex">w_1, w_2, w_3</script>, such that when they&#39;re plugged into the hypothesis function it yields the blue curve.</p>

<p>The image on the right shows the effect of choosing a larger <script type="math/tex">\gamma</script>. The interpolation between any two points is poor because the individual contributions of each point &quot;dies out&quot; too quickly.</p>

<div style="text-align: center; margin-top: 10px">
  <img src="/images/notes/machine-learning/radial-basis-functions/small-gamma.png">
  <img src="/images/notes/machine-learning/radial-basis-functions/large-gamma.png">
</div>

<p>It&#39;s apparent that <script type="math/tex">\gamma</script> has an effect on the performance of interpolation, and that it seems to depend on the data set in question, specifically, how far apart the points are. We will cover the choice of <script type="math/tex">\gamma</script> later on.</p>
<h2 id="rbf-classification">
<span class="hash">#</span>
<a href="#rbf-classification" class="header-link">RBF Classification</a>
</h2>
<p>The model described above was a regression model, where we considered the output to be real-valued which was matched with the target output which was also real-valued. It&#39;s possible to use RBFs for classification, however, by modifying the hypothesis function to be:</p>

<p><script type="math/tex; mode=display">h(\feature) = \sign\left(\sum_{n = 1}^N w_n \exp\left( -\gamma\ \lVert \feature - \feature_n \rVert^2 \right)\right)</script></p>

<p>The question now is how to determine the <script type="math/tex">w_n</script> values with this <script type="math/tex">\sign</script> function now involved. We&#39;ve done this before when we applied <a href="#linear-regression-for-classification">linear regression for classification</a>. We will focus on the inner component of the <script type="math/tex">\sign</script> function, the signal <script type="math/tex">s</script>:</p>

<p><script type="math/tex; mode=display">s = \sum_{n = 1}^N w_n \exp\left( -\gamma\ \lVert \feature - \feature_n \rVert^2 \right)</script></p>

<p>Where we will try to make the signal itself match the <script type="math/tex">\pm 1</script> target. So that we can minimize <script type="math/tex">(s - y)^2</script> on <script type="math/tex">\mathcal D</script> knowing that <script type="math/tex">y = \pm 1</script>, then we simply return <script type="math/tex">h(\feature) = \sign(s)</script>.</p>

<p>RBFs share a relationship to the nearest-neighbor method, where we classify by looking at the closest point <script type="math/tex">\feature_n</script> within the training set to the point <script type="math/tex">\feature</script> being considered, so that <script type="math/tex">\feature</script> inherits the label <script type="math/tex">y_n</script> corresponding to that closest point <script type="math/tex">\feature_n</script>. For example in the image below, all points in a given red-shaded region inherit the label of the red point within that region because all points within that region are closest to that point rather than any of the other points that define other different regions:</p>

<p><img src="/images/notes/machine-learning/radial-basis-functions/nearest-neighbors.png" class="center"></p>

<p>This can be approximated with RBFs, where we only take influence of nearby points. The basis function would look like a cylinder as in the image below, where it&#39;s a given constant value or zero. With this basis function, the regions would look more like the cylinders.</p>

<p><img src="/images/notes/machine-learning/radial-basis-functions/nearest-neighbors-basis-function.png" class="center"></p>

<p>The nearest-neighbor approach is pretty brittle, as the boundaries between the regions are abrupt. It can be modified into the <script type="math/tex">k</script>-nearest neighbors, where instead of taking the value of the closest point, we look at the <script type="math/tex">k</script> nearest points and adopt the label most represented by those points. This has an effect of smoothing the surface, where the number of fluctuations will decrease.</p>

<p>The RBF can be smoothed similarly by using a Gaussian instead of a cylinder, which does a good job of representing a gradual decrease in influence.</p>

<p>In both cases, the model can be considered as a <em>similarity-based method</em>, where we are classifying points according to how similar they are to points in the training set.</p>
<h3 id="k-centers-rbf">
<span class="hash">#</span>
<a href="#k-centers-rbf" class="header-link">K-Centers RBF</a>
</h3>
<p>We are generally mindful of generalization as a ratio between the number of parameters and data points. However, the RBF model we described has <script type="math/tex">N</script> parameters <script type="math/tex">w_1, \dots, w_N</script> based on <script type="math/tex">N</script> data points, so it may seem hopeless to generalize. We can mitigate this by preventing every point from having its own influence, and instead appoint a number <script type="math/tex">K</script> of important centers for the data and have them influence the points around them.</p>

<p>We do this by taking <script type="math/tex">K \ll N</script> centers <script type="math/tex">\mathbf {\mu_1, \dots, \mu_K}</script> as the centers of the RBFs instead of <script type="math/tex">\feature_1, \dots, \feature_N</script>. The <script type="math/tex">K</script> centers live in the same space as <script type="math/tex">\feature</script>, but they are not necessarily data points---they could be elected data points from the data set, or they may be specially constructed points that we may deem representative of the data set. We then define the hypothesis function as:</p>

<p><script type="math/tex; mode=display">h(\feature) = \sum_{k = 1}^K w_k \exp\left(-\gamma\ \lVert \feature - \mu_k \rVert^2\right)</script></p>

<p>This hypothesis function now reflects the fact that every point <script type="math/tex">\feature_n</script> is being compared against every center <script type="math/tex">\mathbf \mu_k</script>. We now have to determine how to choose the centers <script type="math/tex">\mathbf \mu_k</script> and the weights <script type="math/tex">w_k</script>.</p>
<h3 id="k-means-clustering">
<span class="hash">#</span>
<a href="#k-means-clustering" class="header-link">K-Means Clustering</a>
</h3>
<p>To choose the <script type="math/tex">K</script> centers, we are going to choose the centers as representative of the data inputs, that is, a representative center will exist for every cluster of data points. Such representative centers would be achieved by minimizing the distance between <script type="math/tex">\feature_n</script> and the <strong>closest</strong> center <script type="math/tex">\mathbf \mu_k</script>. This is called <em>K-means clustering</em> because the center of a cluster will end up being the mean of the points in that cluster.</p>

<p>We begin by splitting the data set <script type="math/tex">\feature_1, \dots, \feature_N</script> into clusters <script type="math/tex">S_1, \dots, S_K</script>. Again, a good representative center <script type="math/tex">\mathbf \mu_k</script> of a cluster <script type="math/tex">S_k</script> would minimize the distance between the points in the cluster and itself, represented as the sum of the Euclidean MSE between the candidate center and each of the points in the cluster. The optimal cluster configuration is achieved by then minimizing the sum of each cluster&#39;s error measure.</p>

<p><script type="math/tex; mode=display">\text {Minimize } \quad \sum_{k = 1}^K \sum_{\feature_n \in S_k} \lVert \feature_n - \mathbf \mu_k \rVert^2</script></p>

<p>This is <em>unsupervised learning</em>, since it was performed without any reference to the label <script type="math/tex">y_n</script>. The problem is that this is <em>NP-hard</em> in general; it is intractable to get the absolute minimum. The problem being NP-hard didn&#39;t discourage us before when we realized that finding the absolute minimum error in a neural network was NP-hard as well, in which case we developed a heuristic---gradient descent---which led to back-propagation and thus a decent local-minimum.</p>
<h3 id="lloyd39s-algorithm">
<span class="hash">#</span>
<a href="#lloyd39s-algorithm" class="header-link">LLoyd&#39;s Algorithm</a>
</h3>
<p>We can develop an iterative algorithm to to solve K-means clustering, which works by fixing one of the parameters and attempting to minimize the other. First it fixes the particular membership of the clusters <script type="math/tex">\feature_n \in S_k</script> and it tries to find the optimal centers. After finding these centers, it then fixes these centers and tries to find the optimal clustering for these centers. This is repeated until convergence:</p>

<p><script type="math/tex; mode=display">
\begin{align}
\text {Iteratively Minimize } \quad &\sum_{k = 1}^K \sum_{\feature_n \in S_k}
\lVert \feature_n - \mathbf \mu_k \rVert^2 \quad \text {w.r.t. } \mathbf \mu_k, S_k \\ \\
&\mathbf \mu_k \gets \frac 1 {|S_k|} \sum_{\feature_n \in S_k} \feature_n \\
&S_k \gets \{\feature_n : \lVert \feature_n - \mathbf \mu_k \rVert \leq \text { all } \lVert \feature_n - \mathbf \mu_l \rVert\}
\end{align}
</script></p>

<p>When updating <script type="math/tex">\mathbf \mu_k</script>, we essentially take the mean of the points in the cluster: add them up and divide by the amount of points in the cluster. This would be a good representative if <script type="math/tex">S_k</script> were the real cluster.</p>

<p>The <script type="math/tex">\mathbf \mu_k</script> value is then frozen and we cluster the points based on it. For every point <script type="math/tex">\feature_n</script> in the data set, we measure the distance to the new center and admit it to the cluster <script type="math/tex">S_k</script> if this distance is smaller than the distance between this point <script type="math/tex">\feature_n</script> and any of the other clusters <script type="math/tex">\mathbf \mu_l</script>.</p>

<p>It is apparent that both steps are minimizing the original quantity, and since there are only a finite number of points and a finite number of possible values for <script type="math/tex">\mathbf \mu_k</script>, it is <strong>guaranteed</strong> that we will converge, albeit to a local minimum, which is sensitive to the initial centers or clusters, just like neural networks converged to a local minimum dependent on the initial weights. The general way of choosing decent initial centers or clusters is to perform a number of runs with different centers or clusters and choosing the best one.</p>

<p>LLoyd&#39;s algorithm can be visualized in the image below. The algorithm is fed the only the inputs of the data set, with the centers initialized so some pre-defined values (black dots), then the algorithm iterates and outputs the representative <script type="math/tex">\mathbf \mu_k</script> centers.</p>

<p><img src="/images/notes/machine-learning/radial-basis-functions/lloyds-algorithm.png" class="center"></p>

<p>Notice that since we cluster values without looking at the label <script type="math/tex">y_n</script>, we can have clusters that lie on the boundary so that half of the points are <script type="math/tex">+1</script> and the other half are <script type="math/tex">-1</script>. This is the price paid by unsupervised learning, where we want to find similarity as far as the <strong><em>input</em></strong> is concerned, not as far as the behavior with the target function is concerned:</p>

<p><img src="/images/notes/machine-learning/radial-basis-functions/rbf-centers.png" class="center"></p>
<h3 id="rbf-calculating-weights">
<span class="hash">#</span>
<a href="#rbf-calculating-weights" class="header-link">RBF Calculating Weights</a>
</h3>
<p>Now that we have determined the centers we can determine the weights. Again the hypothesis function was defined as:</p>

<p><script type="math/tex; mode=display">\sum_{k = 1}^K w_k \exp\left( -\gamma\ \lVert \feature_n - \mathbf \mu_n \rVert^2 \right) \approx y_n</script></p>

<p>In this case there are now <script type="math/tex">N</script> equations but only <script type="math/tex">K < N</script> unknowns.</p>

<p><script type="math/tex; mode=display">
\def \center {\mathbf \mu}
\underbrace {\begin{bmatrix}
\basis {\feature_1} {\center_1} & \dots & \basis {\feature_1} {\center_K} \\
\basis {\feature_2} {\center_1} & \dots & \basis {\feature_2} {\center_K} \\
\vdots & \vdots & \vdots \\
\basis {\feature_N} {\center_1} & \dots & \basis {\feature_N} {\center_K} \\
\end{bmatrix}}_{\mathbf \Phi}
\underbrace {\begin{bmatrix}
w_1 \vphantom {\Big(} \\
w_2 \vphantom {\Big(} \\
\vdots \\
w_K \vphantom {\Big(}
\end{bmatrix}}_{\weight} =
\underbrace {\begin{bmatrix}
y_1 \vphantom {\Big(} \\
y_2 \vphantom {\Big(} \\
\vdots \\
y_N \vphantom {\Big(}
\end{bmatrix}}_{\mathbf y}
</script></p>

<p>The weights can be solved for in the same manner as in linear regression by calculating the pseudo-inverse. If <script type="math/tex">\mathbf {\Phi^\intercal \Phi}</script> is invertible, then:</p>

<p><script type="math/tex; mode=display">\weight = \left(\mathbf {\Phi^\intercal \Phi}\right)^{-1} \mathbf {\Phi^\intercal y} \quad \text {if } \mathbf {\Phi^\intercal \Phi} \text { is invertible }</script></p>

<p>In this case we aren&#39;t guaranteed to get the correct value at every data point, but we do have a much higher chance at generalization since we only have <script type="math/tex">K \ll N</script> parameters.</p>
<h2 id="rbf-network">
<span class="hash">#</span>
<a href="#rbf-network" class="header-link">RBF Network</a>
</h2>
<p>We can take the RBF model and design a graphical network in order to relate it to neural networks. In this graphical network, we take the input and compute the radial component, the distance to every <script type="math/tex">\center_k</script>. These radial components are handed to a non-linearity <script type="math/tex">\phi</script>, usually the Gaussian non-linearity. The resultant features are combined with weights <script type="math/tex">w_k</script> in order to compute the output <script type="math/tex">h(\feature)</script>.</p>

<p><img src="/images/notes/machine-learning/radial-basis-functions/rbf-network.png" class="center"></p>

<p>In this case the &quot;features&quot; are <script type="math/tex">\smash {\basis {\feature} {\center_k}}</script>. The non-linear transform features depend on <script type="math/tex">\mathcal D</script>, so it is no longer a linear model. However, since we only used the inputs in order to compute <script type="math/tex">\center</script> it&#39;s <em>almost</em> linear, since we didn&#39;t have to back-propagate because we didn&#39;t like the output, since the inputs were frozen. This is why the <script type="math/tex">w</script> values look like multiplicative values, in which case it&#39;s linear on those values.</p>

<p>A bias term (<script type="math/tex">b</script> or <script type="math/tex">w_0</script>) is often added at the final layer.</p>
<h3 id="rbf-vs.-neural-networks">
<span class="hash">#</span>
<a href="#rbf-vs.-neural-networks" class="header-link">RBF vs. Neural Networks</a>
</h3>
<p>A two-layer neural network can be compared to an RBF network.</p>

<p>Both networks compute features, however in the case of RBF networks, the features depend on the distance to the center, and if that distance is very large then the influence disappears. That is, if a particular value input is huge, we can know that the corresponding feature will have zero contribution. In the case of a neural network, the input always goes through a sigmoid regardless of its size, meaning that it always has a contribution. What RBFs do can be interpreted as looking at local regions of the space without worrying about distant points, such that the basis function will not interfere with those distant points.</p>

<p>The non-linearity is called <script type="math/tex">\phi</script> in RBF networks and <script type="math/tex">\theta</script> in neural networks. In neural networks, the input to the non-linearity consisted of the weights <script type="math/tex">\weight</script> that depended on the labels of the data, so that the error was back-propagated through the network to adjust the weights. This is why in the neural networks the non-linearity outputs are learned features, which definitely makes it a non-linear model. In the case of RBF networks, the <script type="math/tex">\center</script> values are already frozen, so that the <script type="math/tex">\phi</script> is almost linear, which is why we were able to derive the features using the pseudo-inverse.</p>

<p>Any two-layer network with a structure similar to an RBF network lends itself to being a support vector machine, where the first layer handles the kernel and the second one is the linear combination that is built into SVMs. For example, neural networks can be implemented using SVMs with a neural network kernel.</p>

<p><img src="/images/notes/machine-learning/radial-basis-functions/neural-network.png" class="center"></p>
<h2 id="choosing-script-typemathtexgammascript">
<span class="hash">#</span>
<a href="#choosing-script-typemathtexgammascript" class="header-link">Choosing <script type="math/tex">\gamma</script></a>
</h2>
<p>The final parameter to choose was the width of the Gaussian curve <script type="math/tex">\gamma</script>. This is now treated as a genuine parameter to be learned using an iterative approach using an algorithm known as the <em>Expectation Maximization</em> (EM) algorithm, which is used for solving the mixture of Gaussians. The algorithm is as follows:</p>

<ol>
<li>Fix <script type="math/tex">\gamma</script>, solve for <script type="math/tex">w_1, \dots, w_K</script> (using the pseudo-inverse)</li>
<li>Fix <script type="math/tex">w_1, \dots, w_K</script>, minimize error w.r.t. <script type="math/tex">\gamma</script> (using gradient descent)</li>
</ol>

<p>With this simple algorithm, we can have different <script type="math/tex">\gamma_k</script> for each center <script type="math/tex">\center_k</script>, so that for example one center can &quot;reach out further&quot; than another if it has to.</p>
<h2 id="rbf-vs.-svm-kernel">
<span class="hash">#</span>
<a href="#rbf-vs.-svm-kernel" class="header-link">RBF vs. SVM Kernel</a>
</h2>
<p>We previously saw the RBF kernel for SVM that implements the following classifier:</p>

<p><script type="math/tex; mode=display">\sign\left(\sum_{\alpha_n \gt 0} \alpha_n y_n \basis {\feature} {\feature_n} + b\right)</script></p>

<p>We also just saw the straight RBF classifier implementation:</p>

<p><script type="math/tex; mode=display">\sign\left(\sum_{k = 1}^K w_k \basis {\feature} {\center_k} + b\right)</script></p>

<p>The performance of both models can be observed in the image below. It&#39;s apparent from the straight RBF implementation that <script type="math/tex">\insample \ne 0</script> since some points are misclassified. The SVM implementation clearly tracks the target better, with <script type="math/tex">\insample = 0</script>.</p>

<p><img src="/images/notes/machine-learning/radial-basis-functions/rbf-vs-svm.png" class="center"></p>
<h2 id="rbf-regularization">
<span class="hash">#</span>
<a href="#rbf-regularization" class="header-link">RBF Regularization</a>
</h2>
<p>RBFs can be derived based purely on regularization. Take a 1D function with many data points and we want to inter/extra-polate between the points in order to get the whole function. With regularization there are usually two terms: one two minimize <script type="math/tex">\insample</script> and the regularization term.</p>

<p>The in-sample error minimization term can be defined the squared difference between the hypothesis value and the target value:</p>

<p><script type="math/tex; mode=display">\sum_{n = 1}^N (h(x_n) - y_n)^2</script></p>

<p>A &quot;smoothness&quot; constraint can be added, which is a constraint on the derivatives, by taking the kth-derivative of the hypothesis and squaring it, since we are only interested in the magnitude of it, and integrate it from <script type="math/tex">-\infty</script> to <script type="math/tex">\infty</script>. This would be an estimate of the size of the kth-derivative, where if it&#39;s big then it&#39;s bad for smoothness and vice versa. We can then combine the contributions of different derivatives:</p>

<p><script type="math/tex; mode=display">\sum_{k = 0}^\infty a_k \int_{-\infty}^\infty \left( \frac {d^k h} {dx^k} \right)^2 dx</script></p>

<p>These terms can be combined by adding them, but first multiplying the second term by the regularization parameter <script type="math/tex">\lambda</script>. We can then minimize the augmented error <script type="math/tex">\augerror</script> where the bigger <script type="math/tex">\lambda</script> is, the more insistent we are on smoothness versus fitting, as we have seen before on the topic of regularization.</p>

<p><script type="math/tex; mode=display">\sum_{n = 1}^N (h(x_n) - y_n)^2 + \lambda \sum_{k = 0}^\infty a_k \int_{-\infty}^\infty \left( \frac {d^k h} {dx^k} \right)^2 dx</script></p>

<p>If this is solved, we end up with RBFs, which means that with RBFs we are effectively looking for an interpolation, specifically as smooth an interpolation as possible in the sense of the sum of the squares of the derivatives with these coefficients. It isn&#39;t surprising, therefore, that the best interpolation would be Gaussian. This is what gives RBFs credibility as inherently self-regularized.</p>
<h1 id="three-learning-principles">
<span class="hash">#</span>
<a href="#three-learning-principles" class="header-link">Three Learning Principles</a>
</h1><h2 id="occam39s-razor">
<span class="hash">#</span>
<a href="#occam39s-razor" class="header-link">Occam&#39;s Razor</a>
</h2>
<p>Occam&#39;s razor is a symbolic principle set by William of Occam, where we have a razor and we keep trimming the explanation to the bare minimum that is still consistent with the data, which happens to be the best possible explanation. More succinctly, it says that:</p>

<blockquote>
<p>The simplest model that fits the data is also the most plausible.</p>

<p><cite><strong>Occam&#39;s Razor</strong> applied to ML</cite></p>
</blockquote>

<p>It&#39;s therefore important to know when a model is considered &quot;simple,&quot; and for this purpose there are two main types of complexity measures: one being the complexity of the hypothesis <script type="math/tex">h</script> (object), the second being the complexity of the hypothesis set <script type="math/tex">\mathcal H</script> (set of objects).</p>

<p>The complexity of <script type="math/tex">h</script> can be measured using minimum description length (MDL) or the order of the polynomial. MDL concerns specifying an object with as few bits as possible, the fewer the bits, the &quot;simpler&quot; the object. For example, for a million-digit number we want to determine the complexity of individual numbers of that length. If we choose the number <script type="math/tex">2^{1,000,000} - 1</script>, which is a million digits long, all the number <script type="math/tex">9</script>. Despite the fact that this is a very long number, it&#39;s simple because we can simply describe it as <script type="math/tex">2^{1,000,000} - 1</script>, which isn&#39;t a very long description. The order of the polynomial as a measure of complexity is straightforward.</p>

<p>The complexity of <script type="math/tex">\mathcal H</script> can be measured using entropy and the VC dimension. The VC dimension produces a number that describes the diversity of the set, which is considered the complexity.</p>

<p>When we think of &quot;simple&quot;, we think in terms of a single object, <script type="math/tex">h</script> in this case. However, proofs use &quot;simple&quot; in terms of <script type="math/tex">\mathcal H</script>.</p>

<p>The complexity of an object and a set of objects are related, if not almost identical. For example, using the MDL we take <script type="math/tex">l</script> bits to specify the object <script type="math/tex">h</script>, in other words, the complexity of <script type="math/tex">h</script> is <script type="math/tex">l</script> bits. This implies that <script type="math/tex">h</script> is one of <script type="math/tex">2^l</script> elements of a set <script type="math/tex">\mathcal H</script>, in other words, one of <script type="math/tex">2^l</script> is the complexity of set <script type="math/tex">\mathcal H</script>. Put into words, <strong>the link is</strong> that something is complex on its own if it&#39;s one of many, whereas something is simple on its own if it&#39;s one of few.</p>

<p>The link holds with respect to real-valued parameters (as opposed to bits), such as a 17th-order polynomial, which is considered complex because it is one of very many, since there are 17 parameters to tune.</p>

<p>There are exceptions. Support vector machines look complex but is one of few, i.e. when we go into an infinite-dimensional space yet only need a few support vectors to represent the separating hyperplane.</p>
<h3 id="puzzle-1-football-oracle">
<span class="hash">#</span>
<a href="#puzzle-1-football-oracle" class="header-link">Puzzle 1: Football Oracle</a>
</h3>
<p>You get a letter Monday morning predicting the outcome of Monday night&#39;s game. After we watch the game, we realize the letter was right. The same thing happens next Monday, even with the odds stacked against the team it called. This went on for 5 weeks in a row. On the 6th week, the letter asks you if you want to continue getting predictions for a $50 fee. Should you pay?</p>

<p>We shouldn&#39;t. The guy isn&#39;t sending only to us, but to 32 other people. In the first game, to half of those people he said the home team would lose, and the other half that they would win. The home team lost that first week. The second week, he did the same thing with the half of the recipients for which he sent the correct result. He repeated this process for 5 weeks until there was only one person left for him all &quot;predictions&quot; were correct.</p>

<table>
<thead>
<tr>
<th style="text-align: left">Distribution of Predictions</th>
<th style="text-align: left">Result</th>
</tr>
</thead>

<tbody>
<tr>
<td style="text-align: left"><script type="math/tex">\style{color:green}{0000000000000000}\style{color:red}{1111111111111111}</script></td>
<td style="text-align: left"><script type="math/tex">0</script></td>
</tr>
<tr>
<td style="text-align: left"><script type="math/tex">\style{color:green}{00000000}\style{color:red}{11111111}\style{color:gray}{0000000011111111}</script></td>
<td style="text-align: left"><script type="math/tex">1</script></td>
</tr>
<tr>
<td style="text-align: left"><script type="math/tex">\style{color:gray}{00001111}\style{color:green}{0000}\style{color:red}{1111}\style{color:gray}{0000111100001111}</script></td>
<td style="text-align: left"><script type="math/tex">0</script></td>
</tr>
<tr>
<td style="text-align: left"><script type="math/tex">\style{color:gray}{00110011}\style{color:green}{00}\style{color:red}{11}\style{color:gray}{00110011001100110011}</script></td>
<td style="text-align: left"><script type="math/tex">1</script></td>
</tr>
<tr>
<td style="text-align: left"><script type="math/tex">\style{color:gray}{0101010101}\style{color:green}0\style{color:red}1\style{color:gray}{01010101010101010101}</script></td>
<td style="text-align: left"><script type="math/tex">1</script></td>
</tr>
</tbody>
</table>

<p>We thought that the prediction ability was great since we only saw our letters (and we are the last remaining recipient for which all predictions were &quot;correct&quot;). We figured there was one hypothesis, and it predicted perfectly. The problem is that the hypothesis set is actually very complex, so the prediction value is meaningless, we simply didn&#39;t know since we didn&#39;t see the hypothesis set.</p>
<h3 id="why-is-simpler-better">
<span class="hash">#</span>
<a href="#why-is-simpler-better" class="header-link">Why is Simpler Better?</a>
</h3>
<p>Occam&#39;s Razor isn&#39;t making the statement that simpler is more elegant. Instead, Occam&#39;s Razor is making the statement that simpler will have better out-of-sample performance.</p>

<p>The basic argument to back up this claim, which is a formal proof under different idealized conditions, is as follows. There are fewer simpler hypotheses than complex ones, which was captured by the growth function <script type="math/tex">\growthfunc(N)</script>. To recap, the growth function took as parameter the size <script type="math/tex">N</script> of the data set and returned the number of different patterns (dichotomies) that the hypothesis set <script type="math/tex">\mathcal H</script> could generate on those <script type="math/tex">N</script> points. Since there are fewer simpler hypotheses, it is less likely to fit a given data set, specifically, <script type="math/tex">\growthfunc(N)/2^N</script>. Since it is less likely for fitting to occur, then when it does occur it is more significant.</p>

<p>The only difference between someone believing in the scam in Puzzle #1 and someone having the big picture was the fact that the growth function, from our point of view, was <script type="math/tex">\growthfunc(N) = 1</script>---we are only one person, he has one hypothesis, and it was correct, and we gave it a lot of value since that scenario is unlikely to occur. The reality was that the growth function was <script type="math/tex">\growthfunc(N) = 2^N</script>, which means it&#39;s certain to happen, so that when it <em>does</em> happen, it&#39;s meaningless.</p>
<h3 id="meaningless-fit">
<span class="hash">#</span>
<a href="#meaningless-fit" class="header-link">Meaningless Fit</a>
</h3>
<p>Suppose two scientists conduct an experiment to determine if conductivity is linear in the temperature. The first scientist is in a hurry so he simply takes two points from the data set and draws a line to connect them. The second scientist took three points and also drew a line. What evidence does the first or second experiment provide for the hypothesis that conductivity is linear in the temperature?</p>

<div style="text-align: center; margin-top: 10px">
  <img src="/images/notes/machine-learning/three-learning-principles/experiment-a.png">
  <img src="/images/notes/machine-learning/three-learning-principles/experiment-b.png">
</div>

<p>Clearly the experiment that used three points to fit the line provided more evidence than the other. The experiment that used two points provided no evidence at all, because two points can always be connected by a line. This introduces the notion of <em>falsifiability</em>: if your data has no chance of falsifying your assertion, then it doesn&#39;t provide any evidence for the assertion. We have to have a chance to falsify the assertion in order to be able to draw the evidence (axiom of non-falsifiability).</p>

<p>For example, the linear model is <em>too complex</em> for the data set size of <script type="math/tex">N = 2</script> to be able to generalize at all, so there is no evidence. On the other hand, with a data set size of <script type="math/tex">N = 3</script> the assertion could&#39;ve been falsified if one of the three points was not collinear with the other two.</p>

<p><img src="/images/notes/machine-learning/three-learning-principles/falsifiability.png" class="center"></p>
<h2 id="sampling-bias">
<span class="hash">#</span>
<a href="#sampling-bias" class="header-link">Sampling Bias</a>
</h2><h3 id="puzzle-2-presidential-election">
<span class="hash">#</span>
<a href="#puzzle-2-presidential-election" class="header-link">Puzzle 2: Presidential Election</a>
</h3>
<p>In 1948, in the first presidential election after WW2 ended up in a close race between Truman and Dewey. A newspaper ran a phone poll asking people how they voted and concluded that Dewey had won the poll decisively, that is, he won above the error bar. They then printed a newspaper declaring &quot;<em>Dewey Defeats Truman</em>.&quot; However, Truman was the candidate that ended up winning.</p>

<p>This wasn&#39;t <script type="math/tex">\delta</script>&#39;s fault, where <script type="math/tex">\delta</script> was the discrepancy between in-sample (poll) and out-of-sample (general population), where we asked ourselves for the probability that this discrepancy is larger than some quantity <script type="math/tex">\epsilon</script> such that the result was flipped:</p>

<p><script type="math/tex; mode=display">\mathbb P \left[ |\insample - \outsample| \gt \epsilon \right] \leq \delta</script></p>

<p>It was not bad luck, however, because if they had run the poll again with a larger and larger sample size, they would&#39;ve gotten the same result. The problem was that there was a <em>sampling bias</em> in the poll they conducted, and that is that in 1948, phones were expensive, and in that era, rich people favored Dewey more than they favored Truman.</p>

<blockquote>
<p>If the data is sampled in a biased way, then learning will produce a similarly biased outcome.</p>

<p><cite><strong>Sampling Bias Principle</strong></cite></p>
</blockquote>

<p>This presents a problem of making sure that the data is representative of what we want. For example, in financial forecasting, we may want to predict the market by taking periods of the market where the market was normal. If this model is then tested in the real market which contains non-normal conditions, we will have no idea how the model will perform in those conditions.</p>
<h3 id="matching-the-distributions">
<span class="hash">#</span>
<a href="#matching-the-distributions" class="header-link">Matching the Distributions</a>
</h3>
<p>The idea is that we have a distribution on the input space in our mind. Remember that VC, Hoeffding&#39;s, etc. made the assumption that we chose the points for training from the same distribution that we picked for testing. One method, in principle, that might resolve this is to make the training set we have be more representative of the target set. This can be done by resampling from the training set by picking data points that better represent the target set, or by scaling the weights of the training points. This could mean that the effective size of the training set decreases, losing some of the independence of the points.</p>

<p><img src="/images/notes/machine-learning/three-learning-principles/matching-the-distributions.png" class="center"></p>

<p>This method doesn&#39;t work if there is a region in the input space where the probability <script type="math/tex">P = 0</script> for training (nothing will be sampled from this region) but <script type="math/tex">P > 0</script> (it&#39;s being tested anyways). This is similar to the people without phones in Puzzle #2, who had <script type="math/tex">P = 0</script> in the sample (they weren&#39;t in the sample) but not in the general population (there were people without phones who voted).</p>
<h3 id="puzzle-3-credit-approval">
<span class="hash">#</span>
<a href="#puzzle-3-credit-approval" class="header-link">Puzzle 3: Credit Approval</a>
</h3>
<p>A bank wants to approve credit automatically by going through the historical records of previous applicants who were given credit cards with 3-4 years of credit behavior. The input is the information they provided on the credit application, since this is the data that would be available from a new customer:</p>

<table>
<thead>
<tr>
<th style="text-align: left">data</th>
<th style="text-align: left">value</th>
</tr>
</thead>

<tbody>
<tr>
<td style="text-align: left">age</td>
<td style="text-align: left">23 years</td>
</tr>
<tr>
<td style="text-align: left">gender</td>
<td style="text-align: left">male</td>
</tr>
<tr>
<td style="text-align: left">annual salary</td>
<td style="text-align: left">$30,000</td>
</tr>
<tr>
<td style="text-align: left">years in residence</td>
<td style="text-align: left">1 year</td>
</tr>
<tr>
<td style="text-align: left">years in job</td>
<td style="text-align: left">1 year</td>
</tr>
<tr>
<td style="text-align: left">current debt</td>
<td style="text-align: left">$15,000</td>
</tr>
<tr>
<td style="text-align: left">...</td>
<td style="text-align: left">...</td>
</tr>
</tbody>
</table>

<p>The target is whether or not the person was profitable for the bank. For example, someone flirting with disaster who maxed out but paid the debt was profitable as long as they didn&#39;t default.</p>

<p>The sampling bias lies in the fact that we&#39;re only considering the historical data of customers we <em>approved</em>, since they&#39;re the only ones for whom we have credit behavior data on. When we&#39;re done training, we&#39;ll have a system that applies to a new applicant, and we don&#39;t know beforehand whether or not they&#39;ll be approved or not, because they were never part of our training sample. In this case, sampling bias isn&#39;t entirely terrible, because banks tend to be aggressive in providing credit, since borderline credit users are very profitable.</p>
<h2 id="data-snooping">
<span class="hash">#</span>
<a href="#data-snooping" class="header-link">Data Snooping</a>
</h2>
<p>The data snooping principle doesn&#39;t forbid us from doing anything, it simply makes us realize that if we use a particular data set---the whole, subset, or something else---to navigate and decide which model, <script type="math/tex">\lambda</script>, etc., then when we have an outcome from the learning process and we use the same data set that affected the choice of the learning process, the ability to fairly assess the performance of the outcome has been compromised by the fact that it was chosen according to the data set.</p>

<blockquote>
<p>If a data set has affected any step of the learning process, it&#39;s ability to assess the outcome has been compromised.</p>

<p><cite><strong>Data Snooping Principle</strong></cite></p>
</blockquote>

<p>This is the most common trap for practitioners. A possible reason is that, when we data snoop, we end up with better performance---or so we think. Data snooping isn&#39;t only looking at the data, in fact there are many ways to fall into the trap.</p>
<h3 id="looking-at-the-data">
<span class="hash">#</span>
<a href="#looking-at-the-data" class="header-link">Looking at the Data</a>
</h3>
<p>The most common data snooping is &quot;looking at the data.&quot; For example, with non-linear transforms, we use a second-order transform:</p>

<p><script type="math/tex; mode=display">\mathbf z = (1, x_1, x_2, x_1 x_2, x_1^2, x_2^2)</script></p>

<p><img src="/images/notes/machine-learning/three-learning-principles/looking-at-the-data.png" class="center"></p>

<p>We manage to get <script type="math/tex">\insample = 0</script>, so we decide that we fit the problem very well but we don&#39;t like the ratio of parameters-to-data points, with respect to generalization. So we decide that we could&#39;ve done with a simpler transformation:</p>

<p><script type="math/tex; mode=display">\mathbf z = (1, x_1^2, x_2^2) \text { or even } \mathbf z = (1, x_1^2 + x_2^2)</script></p>

<p>The problem is that this process of refining the non-linear transformation is essentially a learning algorithm, and we didn&#39;t take into account the fact that we did some of the learning, and thereby forgot to charge the original VC dimension instead of just the final VC dimension of the non-linear transformation we arrived at.</p>

<p>It&#39;s important to realize that the snooping in this example involves the data set <script type="math/tex">\mathcal D</script>. The danger of looking at the data set is that we are vulnerable to designing the model or choices in learning according to the idiosyncrasies of the data set, so we are working well <em>on that</em> data set, but we don&#39;t know how we&#39;ll be doing in another independently generated data set from the same distribution, which would be the out-of-sample.</p>

<p>On the other hand, we&#39;re completely allowed---encouraged, <em>ordered</em>---to look at all other information related to the target function and input space, <em>except for</em> the realization of the data set that we&#39;re going to use for training, <em>unless</em> we&#39;re going to charge accordingly. For example, we may ask: how many inputs do we have, what is the range of the inputs, how did we measure the inputs, are they physically correlated, do we know of any properties that we can apply. This is all valid and important for us to zoom-in correctly, since we&#39;re not using the data, and are therefore not subject to overfitting the data. Instead we&#39;re using properties of the target function and input space and therefore improving our chances of picking the correct model. <strong>The problem starts</strong> when we look at the data set and <em>not</em> charge accordingly.</p>
<h3 id="puzzle-4-financial-forecasting">
<span class="hash">#</span>
<a href="#puzzle-4-financial-forecasting" class="header-link">Puzzle 4: Financial Forecasting</a>
</h3>
<p>Suppose we want to predict the exchange rate between the US Dollar versus the British Pound. We have 8 years worth of daily trading---about 2,000 data points. Our input-to-output model, where <script type="math/tex">r</script> is the rate, can be expressed as getting the change in rate for the past 20 days, hoping that a particular pattern in the exchange rate will make it more likely that today&#39;s change is positive or negative and by how much:</p>

<p><script type="math/tex; mode=display">\Delta r_{-20}, \Delta r_{-19}, \dots, \Delta r_{-1} \to \Delta r_0</script></p>

<p>We normalize the data to zero-mean and unit-variance, we split randomly into a <script type="math/tex">\trainingset</script> of 1,500 points and <script type="math/tex">\def \testingset {\mathcal D_{\text {test}}}</script> and 500 points. The training set is 1,500 days so that for every day (output) we take the previous 20 days as the input. We completely locked away the <script type="math/tex">\testingset</script> until it was time to test, in an effort to avoid data snooping, resulting in:</p>

<p><img src="/images/notes/machine-learning/three-learning-principles/financial-forecasting.png" class="center"></p>

<p>The plot is of the profit that the testing measure represents, which can be seen to be increasing with more and more days that are trained. When we attempt to try this live on the real market, we end up losing a lot money. The fact is that there was actually data snooping in this learning process: <strong>when we normalized the data</strong>. The problem is that we did this <em>before</em> we separated the data set into <script type="math/tex">\trainingset</script> and <script type="math/tex">\testingset</script>, so we took into consideration the mean and variance <em>of the test set</em>.</p>

<p>If we instead split the data first, then only normalized the <script type="math/tex">\trainingset</script> and then took the <script type="math/tex">\mu</script> and <script type="math/tex">\sigma^2</script> that did the normalization for the training set and applied those to the <script type="math/tex">\testingset</script> so that they live in the same range of values and then reran the test, we would&#39;ve gotten the blue line.</p>

<p>To recap, there&#39;s <strong>nothing wrong with normalization</strong>, as long as the normalization parameters are extracted exclusively from the training set.</p>
<h3 id="data-set-reuse">
<span class="hash">#</span>
<a href="#data-set-reuse" class="header-link">Data Set Reuse</a>
</h3>
<p>If we try one model after another <strong>on the same data set</strong>, we will eventually &quot;succeed.&quot; The problem is that in doing this, we&#39;re increasing the VC dimension without realizing it, since the final model that we end up using in order to learn is <strong>the union of all of the attempted models</strong>. Think of the VC dimension of the <em>total</em> learning model.</p>

<blockquote>
<p>If you torture the data long enough, it will confess.</p>
</blockquote>

<p>This problem could occur not only due to the different models that we have attempted, but <em>also</em> due to what <strong><em>others</em></strong> have attempted. Imagine we decide to try our methods on some data set and we avoid looking at it. We decide to read papers about other people who have used the data set. In the end, we may not have looked at the data ourselves, but we used something that was affected by the data: the papers that others wrote. So that even if we only determined a couple of parameters and yielded great performance, we can&#39;t simply conclude that we have a VC dimension of 2 and 7,000 data points and that we&#39;ll do great out of sample. This is because we don&#39;t <em>only</em> have 2 parameters, but all of the decisions that led up to that model.</p>

<p>The key problem is that we&#39;re matching a <strong><em>particular</em></strong> data set too well; we&#39;re married to the data set, so that a completely new data set generated from the same distribution will look completely foreign to the model we created.</p>

<p>For example, the observation that, in the original data set, whenever a particular pair of points is close, there is always another point on the same line far away---this is clearly an idiosyncrasy of the data set, so that it would be pointless to try to find it in another data set from the same distribution since we may assume that if doesn&#39;t exhibit this same quality, it must be from another distribution. The truth is that the data set is generated from the same distribution, it&#39;s just that we got too used to the data set to the point where we were fitting the noise.</p>
<h3 id="data-snooping-remedies">
<span class="hash">#</span>
<a href="#data-snooping-remedies" class="header-link">Data Snooping Remedies</a>
</h3>
<p>There are two remedies to data snooping: avoiding it or accounting for it. Avoiding it naturally requires very strict discipline. Accounting for data snooping concerns keeping track of how much data contamination has occurred on each data set. The most vulnerable part is looking at the data, because it&#39;s very difficult to model ourselves and say what is the hypothesis set that we explored in order to come up with a particular model by looking at the data.</p>
<h3 id="puzzle-5-bias-via-snooping">
<span class="hash">#</span>
<a href="#puzzle-5-bias-via-snooping" class="header-link">Puzzle 5: Bias via Snooping</a>
</h3>
<p>We&#39;re testing the long-term performance of a famous strategy in trading called &quot;buy and hold&quot; in stocks. We use 50 years worth of data. We want the test to be as broad as possible so we take all currently traded companies in S&amp;P500. We assume that we strictly followed a buy and hold for all of them. We determine that the model will yield very great profits.</p>

<p>There is a sampling bias because we only looked at the <em>currently</em> traded stocks, which obviously excludes those that were around within the 50-year data span but are no longer traded.</p>

<p>People tend to treat this sampling bias not as sampling bias but as data snooping, because we looked at the future to see which stocks would be traded at that point. We consider it a sampling bias caused by &quot;snooping.&quot;</p>
<h1 id="epilogue">
<span class="hash">#</span>
<a href="#epilogue" class="header-link">Epilogue</a>
</h1><h2 id="map-of-machine-learning">
<span class="hash">#</span>
<a href="#map-of-machine-learning" class="header-link">Map of Machine Learning</a>
</h2>
<p>Singular value decomposition would treat the Netflix problem as decomposing the canonical ratings matrix into two matrices (user and movie factors). Graphical Models are models for where the target is a joint probability distribution, they try to find computational efficient ways using graph algorithms to determine the joint probability distribution by means of simplifying the graph. Aggregation is when different solutions are combined.</p>

<p><img src="/images/notes/machine-learning/epilogue/ml-map.png" class="center"></p>
<h2 id="bayesian-learning">
<span class="hash">#</span>
<a href="#bayesian-learning" class="header-link">Bayesian Learning</a>
</h2>
<p>Bayesian learning tries to take a full probabilistic approach to learning. For example, going back to the learning diagram, we can observe that there are many probabilistic components. An inherent probabilistic component is the fact that the target can be noisy, which is why we don&#39;t model the target as a function but rather as a probability distribution. This is apparent in the case of trying to predict heart attacks. Another probability distribution that we had to deal with was the unknown input probability distribution.</p>

<p><img src="/images/notes/machine-learning/epilogue/learning-diagram.png" class="center"></p>

<p>With the Bayesian approach, we want to extend the probabilistic role completely to all components, so that everything is a joint probability distribution. In the case of the heart attack prediction, we were trying to pick the hypothesis by determining the likelihood that that hypothesis would generate the data set in question:</p>

<p><script type="math/tex; mode=display">P(\mathcal D \mid h = f)</script></p>

<p>Bayesian approaches instead try to determine the hypothesis by considering the probability that a given hypothesis is the best one given the data:</p>

<p><script type="math/tex; mode=display">P(h = f \mid \mathcal D)</script></p>

<p>To compute this probability, we still need one more probability distribution:</p>

<p><script type="math/tex; mode=display">
P(h = f \mid \mathcal D) = \frac {P(\mathcal D \mid h = f) P(h = f)} {P(\mathcal D)}
\propto P(\mathcal D \mid h = f) P(h = f)
</script></p>

<p>The probability <script type="math/tex">P(\mathcal D \mid h = f)</script> can be computed the same way as in logistic regression, and the probability <script type="math/tex">P(\mathcal D)</script> can also be computed by simply integrating out what we don&#39;t want from the joint probability distribution in order to end up with the marginal <script type="math/tex">P(\mathcal D)</script>. In fact, since we&#39;re only picking the hypothesis according to the aforementioned criteria, we don&#39;t really care about <script type="math/tex">P(\mathcal D)</script> since it only scales the value up or down, and instead we can only worry about picking the hypothesis that yields the largest numerator.</p>

<p>The quantity that we don&#39;t know is the <em>prior</em>: <script type="math/tex">P(h = f)</script>, that is, the probability that the current hypothesis is the target function. It&#39;s called the prior because it&#39;s our belief of the hypothesis set before we got any data. We can modify this value after we get the data to get the <em>posterior</em>: <script type="math/tex">P(h = f \mid \mathcal D)</script>, which is more informed based on the data.</p>

<p>Given the prior, we have the full distribution. For example, consider a perceptron <script type="math/tex">h</script> determined by <script type="math/tex">\weight = w_0, w_1, \dots, w_d</script>. A possible prior on <script type="math/tex">\weight</script> can take into account the fact that the magnitude of the weights doesn&#39;t matter, so that each weight <script type="math/tex">w_i</script> is independent and uniform over <script type="math/tex">[-1, 1]</script>. This determines the prior over <script type="math/tex">h</script>, <script type="math/tex">P(h = f)</script>. Given <script type="math/tex">\mathcal D</script>, we can compute <script type="math/tex">P(\mathcal D \mid h = f)</script>. Putting them together we get <script type="math/tex">P(h = f \mid \mathcal D) \propto P(h = f)P(\mathcal D \mid h = f)</script>.</p>

<p>This shows that the prior is an assumption. Even the most &quot;neutral&quot; prior for an unknown number <script type="math/tex">x</script> for which we only know that it is between <script type="math/tex">-1</script> and <script type="math/tex">+1</script>. It might seem reasonable to want to model this using a uniform probability distribution from <script type="math/tex">-1</script> to <script type="math/tex">+1</script>. This seems reasonable because it&#39;s as likely to be any value in that range.</p>

<p><img src="/images/notes/machine-learning/epilogue/prior-1.png" class="center"></p>

<p>However, consider that using that model, if we were to take a bunch of <script type="math/tex">x</script>&#39;s and average them, we&#39;d get something close to <script type="math/tex">0</script>. However, in the unknown case, we have no idea what value we&#39;d get from this operation. In fact, we can even say how close it&#39;d be to zero in terms of variance.</p>

<p>In fact, the true equivalent would be the image below, where the probability distribution is a delta function centered on a point <script type="math/tex">a</script> that we don&#39;t know. Choosing to model the situation with the uniform probability distribution results in a huge building based on false premises.</p>

<p><img src="/images/notes/machine-learning/epilogue/prior-2.png" class="center"></p>

<p>If we actually knew the prior then we could compute the posterior <script type="math/tex">P(h = f \mid \mathcal D)</script> for every hypothesis <script type="math/tex">h \in \mathcal H</script>. With this, we could find the most probable hypothesis given the data. In fact, we can derive the expected value of <script type="math/tex">h</script>, <script type="math/tex">\mathbb E(h(\feature))</script> for every <script type="math/tex">\feature</script>. Further still, we can derive the <em>error bar</em>---the chances that we&#39;re wrong. Simply, we can derive everything in a principled way.</p>

<p>Bayesian learning can be justified in two main cases. The first is when the prior is <em>valid</em>, that is, it is indeed the probability that a particular hypothesis is the target function. The second is when the prior is <em>irrelevant</em>, for example, when we place a prior, when get more and more data sets and we look at the posterior we might realize that the posterior is affected largely by the data set and less by the prior; the prior gets factored out as we get more and more data. In this case, we can think of the prior simply as a computational catalyst.</p>

<p>For example, we might choose to use conjugate priors, where we don&#39;t have to recompute the posterior as an entire function, but instead parameterize it and change the parameters when we get new data points. This is valid when we&#39;re going to be doing it enough that by the time we arrive, it doesn&#39;t matter what we started with.</p>
<h2 id="aggregation-methods">
<span class="hash">#</span>
<a href="#aggregation-methods" class="header-link">Aggregation Methods</a>
</h2>
<p>Aggregation is a method that applies to all models, which combines different solutions <script type="math/tex">h_1, h_2, \dots, h_T</script> that were trained on <script type="math/tex">\mathcal D</script>. In the image below, every node corresponds to a resultant system <script type="math/tex">h_i</script> that each person trained.</p>

<p><img src="/images/notes/machine-learning/epilogue/aggregation-1.png" class="center"></p>

<p>For example, when trying to detect a face with computer vision, we can instead detect components related to a face, such as an eye, nose, are the positions of these relative to each other, etc. If we were to try to determine whether or not a face was detected from just one of these features, the error would be large. However, if these are combined correctly, we might perform better. This is important in computer vision because we want to perform at or near real-time, so it helps to base the decision on simple computations.</p>

<p>If the problem is a <em>regression</em>, then we can combine the results by simply taking an average. If the problem is a <em>classification</em>, then we can take a vote.</p>

<p>Aggregation is also known as <em>ensemble learning</em> and <em>boosting</em>.</p>

<p>This is <strong>different</strong> from performing 2-layer learning. For example, in a 2-layer model such as a neural network with one hidden layer, all units learn <em>jointly</em>. In the case of the neural network, each of the nodes are updated jointly via back-propagation. A single node isn&#39;t trying to replicate the function, it&#39;s just trying to contribute positively to the function:</p>

<p><img src="/images/notes/machine-learning/epilogue/aggregation-2.png" class="center"></p>

<p>In the case of aggregation, the units learn <em>independently</em>---as if they were the only unit. In the image below, each node is trying to replicate the function individually. Their outputs are then combined to get the output:</p>

<p><img src="/images/notes/machine-learning/epilogue/aggregation-3.png" class="center"></p>
<h3 id="before-the-fact">
<span class="hash">#</span>
<a href="#before-the-fact" class="header-link">Before the Fact</a>
</h3>
<p>Aggregation before the fact creates the solutions with a view to the fact that they will be combined, such that they will &quot;blend well&quot; together. An example of this is <em>bagging</em>, where we&#39;re given the data set and we resample it so that every solution uses a different bootstrap sample, in order to introduce some independence.</p>

<p>In fact, instead leaving the decorrelation to chance, we can enforce it. We can build each hypothesis by making sure that what it covers is different from what other hypotheses cover. This is done by creating each of the hypotheses <script type="math/tex">h_1, \dots, h_t, \dots</script> sequentially, making each hypothesis <script type="math/tex">h_t</script> decorrelated with previous hypotheses. That is, after the third hypothesis for example, we consider what the best fourth hypothesis would be to add to the mix such that it is sufficiently different from the previous other three. This is accomplished by analyzing how the previous three performed, and based on that performance, providing a data set to the fourth hypothesis so that it develops something fairly independent from the previous three.</p>

<p><img src="/images/notes/machine-learning/epilogue/aggregation-4.png" class="center"></p>

<p>For example, consider that the hypothesis functions that exist so far, combined, can correctly classify 60% of the data points in the set. In order to make the new hypothesis fairly independent from the existing ones, we can emphasize those points on which we performed poorly by giving them larger weights and conversely deemphasize those we got right, such that it looks like 50-50 so that the weighted error is 50%. If we take this distribution and learn on it with the new hypothesis and do better than 50%, then the new hypothesis is adding value to what we had before.</p>

<p>In effect, we choose the weight of <script type="math/tex">h_t</script> based on <script type="math/tex">\insample(h_t)</script>. The most famous algorithm that specifies this behavior is <em>adaptive boosting</em> AKA <em>AdaBoost</em>.</p>
<h3 id="after-the-fact">
<span class="hash">#</span>
<a href="#after-the-fact" class="header-link">After the Fact</a>
</h3>
<p>Aggregation <strong>after the fact</strong> combines existing solutions. This is what happened when the Netflix teams merged, &quot;blending.&quot; For regression problems we might have solutions <script type="math/tex">h_1, h_2, \dots, h_T</script>, and we might combine the solutions using some coefficients <script type="math/tex">\alpha</script> such that:</p>

<p><script type="math/tex; mode=display">g(\feature) = \sum_{t = 1}^T \alpha_t h_t(\feature)</script></p>

<p>To determine the optimal <script type="math/tex">\alpha</script> coefficients, we can minimize the error on a separate &quot;aggregation data set.&quot; The pseudo-inverse can be used for this purpose. It&#39;s important to emphasize that we should use a clean set for the aggregation data set. When this is done, some coefficients may come out negative, but this doesn&#39;t necessarily mean that the solution is bad and is being rid of. It could be that the solution is particularly correlated with another, and so the system is trying to combine them in order to get the signal without the noise.</p>

<p>In that case, how can we determine the most valuable hypothesis in the blend? This can be determined by determining the performance with and without a particular hypothesis, where the difference between these two values corresponds to the contribution of that hypothesis to the blend.</p>
<h1 id="resources">
<span class="hash">#</span>
<a href="#resources" class="header-link">Resources</a>
</h1>
<ul>
<li><a href="http://videolectures.net/course_information_theory_pattern_recognition/">Cambridge Information Theory</a></li>
<li><a href="http://www.youtube.com/user/mathematicalmonk/videos">Mathematical Monk</a></li>
<li><a href="http://www.youtube.com/user/hugolarochelle/videos">Hugo Larochelle</a></li>
<li><a href="http://cm.dce.harvard.edu/2014/01/14328/publicationListing.shtml">Harvard CS 109 --- Data Science</a></li>
<li><a href="http://neuralnetworksanddeeplearning.com/index.html">Neural Networks and Deep Learning</a></li>
<li><a href="https://class.coursera.org/machlearning-001/lecture/index">University of Washington Machine Learning</a> by <a href="https://class.coursera.org/machlearning-001/lecture/preview">Pedro Domingos</a></li>
<li><a href="https://www.coursera.org/course/neuralnets">Neural Networks</a></li>
<li><a href="https://www.coursera.org/course/pgm">Probabilistic Graphical Models</a></li>
<li><a href="https://www.coursera.org/course/compmethods">Computational Methods for Data Analysis</a></li>
<li><a href="https://www.coursera.org/course/compneuro">Computational Neuroscience</a></li>
<li><a href="https://www.coursera.org/course/mathematicalmethods">Mathematical Methods for Quantitative Finance</a></li>
<li><a href="http://www.youtube.com/user/CS188Spring2013/videos?live_view=500&amp;flow=grid&amp;sort=dd&amp;view=0">UC Berkeley CS 188</a></li>
<li><a href="http://www.seas.harvard.edu/courses/cs281/">Harvard CS 281 --- Advanced Machine Learning</a></li>
<li><a href="https://www.coursera.org/course/recsys">Recommender Systems</a></li>
<li><a href="https://www.coursera.org/course/datasci">Introduction to Data Science</a></li>
<li><a href="http://www.metacademy.org/">Metacademy</a></li>
<li><a href="http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/">Probabilistic Programming &amp; Bayesian Methods for Hackers</a></li>
<li><a href="http://homepages.inf.ed.ac.uk/vlavrenk/iaml.html">Basics of Machine Learning</a></li>
<li><a href="http://www.trivedigaurav.com/blog/quoc-les-lectures-on-deep-learning/">Quoc Le&#39;s Lectures on Deep Learning</a></li>
</ul>

<div class="footnotes">
<hr>
<ol>

<li id="fn1">
<p><a href="http://math.stackexchange.com/questions/546388/how-is-the-gradient-derived-here">How is the gradient derived here?</a> --- Math.StackExchange&nbsp;<a href="#fnref1" title="continue reading" rev="footnote"><i class="fa fa-level-up"></i></a></p></li>

</ol>
</div>
</div>
  <div class="meta">
    <div class="meta-component"><i class="fa fa-calendar fa-fw"></i> September 30, 2013</div>
    <div class="meta-component"><i class="fa fa-code-fork fa-fw"></i> <a href="https://github.com/blaenk/site/commits/master/input/notes/machine-learning.markdown">History</a><span class="hash">, <a href="https://github.com/blaenk/site/commit/dc6ff87" title="new table marker syntax; no need for metadata">dc6ff87</a></span></div>
    <div class="meta-component"><i class="fa fa-pencil fa-fw"></i> <a href="..">Notes</a></div>
  </div>
</article>



    
    <footer id="footer">
  <div id="social">
    <a href="https://github.com/blaenk" title="github"><i class="fa fa-github-alt"></i></a>
    &middot;
    <a href="http://stackoverflow.com/users/101090/jorge-israel-pena" title="stackoverflow"><i class="fa fa-stack-overflow"></i></a>
    &middot;
    <a href="https://twitter.com/blaenk" title="twitter"><i class="fa fa-twitter"></i></a>
    &middot;
    <a href="mailto:jorge.israel.p@gmail.com" title="email"><i class="fa fa-envelope"></i></a>
    &middot;
    <a href="/rss.xml" title="feed"><i class="fa fa-rss-square"></i></a>
  </div>
  <!-- <div id="credit">
    Designed by <a href="http://www.blaenkdenum.com">Jorge Israel Peña</a>
  </div> -->
</footer>


<!-- this should instead be something like connectWS("{{{path}}}") -->


<script type="text/javascript">
  jQuery(function (){
    var ws = new WebSocket('ws://' + window.location.hostname + ':9160/notes/machine-learning.markdown');

    ws.onmessage = function (e) {
      var content = jQuery('article .entry-content');
      content.html(e.data);

      window.refresh();

      MathJax.Hub.Queue(["Typeset", MathJax.Hub, jQuery('article .entry-content')[0]]);

      if (window.jumpDown)
        window.scrollDown();
    };
  });
</script>




<!-- google analytics -->
<script async="true" type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-37339861-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>

<!--MathJax CDN-->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none"
  });

  MathJax.Hub.Register.MessageHook('End Process', function() {
    jQuery('#MathJax_Font_Test').empty();
    jQuery('.MathJax_Display').parent().addClass('mathjax');
  });
</script>

  </div>
</body>
</html>
